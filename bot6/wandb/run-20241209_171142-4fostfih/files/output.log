Created new wandb run! 4fostfih
Learner successfully initialized!
Press (p) to pause (c) to checkpoint, (q) to checkpoint and quit (after next iteration)

--------BEGIN ITERATION REPORT--------
Policy Reward: 155.20426
Policy Entropy: 4.49585
Value Function Loss: nan

Mean KL Divergence: 0.00342
SB3 Clip Fraction: 0.04072
Policy Update Magnitude: 2.02433
Value Function Update Magnitude: 2.28154

Collected Steps per Second: 17,244.18883
Overall Steps per Second: 11,367.71998

Timestep Collection Time: 2.90208
Timestep Consumption Time: 1.50021
PPO Batch Consumption Time: 0.18576
Total Iteration Time: 4.40229

Cumulative Model Updates: 4
Cumulative Timesteps: 50,044

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 114.65849
Policy Entropy: 4.48643
Value Function Loss: 176.11125

Mean KL Divergence: 0.00780
SB3 Clip Fraction: 0.09092
Policy Update Magnitude: 2.04897
Value Function Update Magnitude: 1.80686

Collected Steps per Second: 18,280.43868
Overall Steps per Second: 10,544.63890

Timestep Collection Time: 2.73648
Timestep Consumption Time: 2.00755
PPO Batch Consumption Time: 0.15192
Total Iteration Time: 4.74402

Cumulative Model Updates: 12
Cumulative Timesteps: 100,068

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 134.94588
Policy Entropy: 4.47630
Value Function Loss: 83.11769

Mean KL Divergence: 0.01113
SB3 Clip Fraction: 0.11992
Policy Update Magnitude: 1.67877
Value Function Update Magnitude: 1.34580

Collected Steps per Second: 19,180.84912
Overall Steps per Second: 11,139.78524

Timestep Collection Time: 2.60771
Timestep Consumption Time: 1.88233
PPO Batch Consumption Time: 0.15245
Total Iteration Time: 4.49003

Cumulative Model Updates: 20
Cumulative Timesteps: 150,086

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 131.53661
Policy Entropy: 4.48611
Value Function Loss: 19.16096

Mean KL Divergence: 0.00522
SB3 Clip Fraction: 0.06117
Policy Update Magnitude: 1.87771
Value Function Update Magnitude: 1.86771

Collected Steps per Second: 21,630.99932
Overall Steps per Second: 10,287.71098

Timestep Collection Time: 2.31233
Timestep Consumption Time: 2.54959
PPO Batch Consumption Time: 0.15079
Total Iteration Time: 4.86192

Cumulative Model Updates: 32
Cumulative Timesteps: 200,104

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 128.52702
Policy Entropy: 4.47294
Value Function Loss: 13.04619

Mean KL Divergence: 0.00967
SB3 Clip Fraction: 0.11958
Policy Update Magnitude: 1.70499
Value Function Update Magnitude: 1.78094

Collected Steps per Second: 20,239.68284
Overall Steps per Second: 9,854.71040

Timestep Collection Time: 2.47138
Timestep Consumption Time: 2.60436
PPO Batch Consumption Time: 0.15134
Total Iteration Time: 5.07575

Cumulative Model Updates: 44
Cumulative Timesteps: 250,124

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 136.53832
Policy Entropy: 4.47341
Value Function Loss: 4.81309

Mean KL Divergence: 0.00711
SB3 Clip Fraction: 0.07502
Policy Update Magnitude: 1.69060
Value Function Update Magnitude: 0.94648

Collected Steps per Second: 20,105.00145
Overall Steps per Second: 9,973.71682

Timestep Collection Time: 2.48724
Timestep Consumption Time: 2.52654
PPO Batch Consumption Time: 0.15003
Total Iteration Time: 5.01378

Cumulative Model Updates: 56
Cumulative Timesteps: 300,130

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 132.21313
Policy Entropy: 4.46939
Value Function Loss: 5.02274

Mean KL Divergence: 0.00628
SB3 Clip Fraction: 0.07317
Policy Update Magnitude: 1.83539
Value Function Update Magnitude: 0.65144

Collected Steps per Second: 20,009.40443
Overall Steps per Second: 9,913.59005

Timestep Collection Time: 2.50052
Timestep Consumption Time: 2.54649
PPO Batch Consumption Time: 0.15037
Total Iteration Time: 5.04701

Cumulative Model Updates: 68
Cumulative Timesteps: 350,164

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 144.21202
Policy Entropy: 4.46164
Value Function Loss: 4.58974

Mean KL Divergence: 0.00667
SB3 Clip Fraction: 0.08008
Policy Update Magnitude: 2.05716
Value Function Update Magnitude: 0.58687

Collected Steps per Second: 20,938.83730
Overall Steps per Second: 10,148.32668

Timestep Collection Time: 2.38800
Timestep Consumption Time: 2.53912
PPO Batch Consumption Time: 0.15091
Total Iteration Time: 4.92712

Cumulative Model Updates: 80
Cumulative Timesteps: 400,166

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 117.33502
Policy Entropy: 4.45637
Value Function Loss: 3.15583

Mean KL Divergence: 0.00696
SB3 Clip Fraction: 0.08656
Policy Update Magnitude: 2.22140
Value Function Update Magnitude: 0.74667

Collected Steps per Second: 20,405.03085
Overall Steps per Second: 10,003.39119

Timestep Collection Time: 2.45165
Timestep Consumption Time: 2.54925
PPO Batch Consumption Time: 0.15080
Total Iteration Time: 5.00090

Cumulative Model Updates: 92
Cumulative Timesteps: 450,192

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 137.42650
Policy Entropy: 4.45039
Value Function Loss: 2.99322

Mean KL Divergence: 0.00761
SB3 Clip Fraction: 0.09630
Policy Update Magnitude: 2.28162
Value Function Update Magnitude: 0.62996

Collected Steps per Second: 20,577.06683
Overall Steps per Second: 10,091.35470

Timestep Collection Time: 2.43135
Timestep Consumption Time: 2.52636
PPO Batch Consumption Time: 0.15208
Total Iteration Time: 4.95771

Cumulative Model Updates: 104
Cumulative Timesteps: 500,222

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 144.19384
Policy Entropy: 4.44451
Value Function Loss: 2.92371

Mean KL Divergence: 0.00760
SB3 Clip Fraction: 0.09980
Policy Update Magnitude: 2.22743
Value Function Update Magnitude: 0.85887

Collected Steps per Second: 20,621.82310
Overall Steps per Second: 10,097.30792

Timestep Collection Time: 2.42646
Timestep Consumption Time: 2.52912
PPO Batch Consumption Time: 0.15028
Total Iteration Time: 4.95558

Cumulative Model Updates: 116
Cumulative Timesteps: 550,260

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 150.66385
Policy Entropy: 4.43853
Value Function Loss: 3.09773

Mean KL Divergence: 0.00778
SB3 Clip Fraction: 0.10495
Policy Update Magnitude: 2.21732
Value Function Update Magnitude: 0.83087

Collected Steps per Second: 20,826.78387
Overall Steps per Second: 10,341.99573

Timestep Collection Time: 2.40287
Timestep Consumption Time: 2.43604
PPO Batch Consumption Time: 0.15051
Total Iteration Time: 4.83891

Cumulative Model Updates: 128
Cumulative Timesteps: 600,304

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 137.53739
Policy Entropy: 4.43222
Value Function Loss: 3.41712

Mean KL Divergence: 0.00807
SB3 Clip Fraction: 0.10808
Policy Update Magnitude: 2.17121
Value Function Update Magnitude: 0.73918

Collected Steps per Second: 21,168.73947
Overall Steps per Second: 10,141.88752

Timestep Collection Time: 2.36386
Timestep Consumption Time: 2.57013
PPO Batch Consumption Time: 0.15079
Total Iteration Time: 4.93399

Cumulative Model Updates: 140
Cumulative Timesteps: 650,344

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 152.89684
Policy Entropy: 4.42572
Value Function Loss: 3.27354

Mean KL Divergence: 0.00787
SB3 Clip Fraction: 0.10660
Policy Update Magnitude: 2.10760
Value Function Update Magnitude: 0.55204

Collected Steps per Second: 15,508.78999
Overall Steps per Second: 8,508.70949

Timestep Collection Time: 3.22475
Timestep Consumption Time: 2.65299
PPO Batch Consumption Time: 0.15923
Total Iteration Time: 5.87774

Cumulative Model Updates: 152
Cumulative Timesteps: 700,356

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 125.91285
Policy Entropy: 4.42201
Value Function Loss: 2.78306

Mean KL Divergence: 0.00768
SB3 Clip Fraction: 0.10529
Policy Update Magnitude: 2.06779
Value Function Update Magnitude: 0.78827

Collected Steps per Second: 19,862.34989
Overall Steps per Second: 9,812.24837

Timestep Collection Time: 2.51743
Timestep Consumption Time: 2.57845
PPO Batch Consumption Time: 0.15420
Total Iteration Time: 5.09588

Cumulative Model Updates: 164
Cumulative Timesteps: 750,358

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 165.35234
Policy Entropy: 4.41603
Value Function Loss: 2.66601

Mean KL Divergence: 0.00811
SB3 Clip Fraction: 0.11367
Policy Update Magnitude: 2.00564
Value Function Update Magnitude: 0.60997

Collected Steps per Second: 18,756.86592
Overall Steps per Second: 9,674.36207

Timestep Collection Time: 2.66825
Timestep Consumption Time: 2.50501
PPO Batch Consumption Time: 0.15145
Total Iteration Time: 5.17326

Cumulative Model Updates: 176
Cumulative Timesteps: 800,406

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 167.84318
Policy Entropy: 4.40726
Value Function Loss: 2.41838

Mean KL Divergence: 0.00799
SB3 Clip Fraction: 0.11107
Policy Update Magnitude: 1.96058
Value Function Update Magnitude: 0.45565

Collected Steps per Second: 20,691.51960
Overall Steps per Second: 10,259.64766

Timestep Collection Time: 2.41693
Timestep Consumption Time: 2.45750
PPO Batch Consumption Time: 0.15255
Total Iteration Time: 4.87444

Cumulative Model Updates: 188
Cumulative Timesteps: 850,416

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 142.33680
Policy Entropy: 4.40166
Value Function Loss: 2.40654

Mean KL Divergence: 0.00820
SB3 Clip Fraction: 0.11741
Policy Update Magnitude: 1.95660
Value Function Update Magnitude: 0.57727

Collected Steps per Second: 23,927.15511
Overall Steps per Second: 10,984.75745

Timestep Collection Time: 2.08984
Timestep Consumption Time: 2.46228
PPO Batch Consumption Time: 0.14917
Total Iteration Time: 4.55213

Cumulative Model Updates: 200
Cumulative Timesteps: 900,420

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 146.60185
Policy Entropy: 4.40153
Value Function Loss: 2.40271

Mean KL Divergence: 0.00834
SB3 Clip Fraction: 0.11975
Policy Update Magnitude: 1.93402
Value Function Update Magnitude: 0.54199

Collected Steps per Second: 20,689.59330
Overall Steps per Second: 10,326.99247

Timestep Collection Time: 2.41870
Timestep Consumption Time: 2.42704
PPO Batch Consumption Time: 0.15052
Total Iteration Time: 4.84575

Cumulative Model Updates: 212
Cumulative Timesteps: 950,462

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 144.03944
Policy Entropy: 4.39392
Value Function Loss: 2.44555

Mean KL Divergence: 0.00827
SB3 Clip Fraction: 0.11890
Policy Update Magnitude: 1.97706
Value Function Update Magnitude: 0.45705

Collected Steps per Second: 21,780.29561
Overall Steps per Second: 10,229.09062

Timestep Collection Time: 2.29721
Timestep Consumption Time: 2.59413
PPO Batch Consumption Time: 0.15715
Total Iteration Time: 4.89134

Cumulative Model Updates: 224
Cumulative Timesteps: 1,000,496

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 130.72826
Policy Entropy: 4.38895
Value Function Loss: 2.42436

Mean KL Divergence: 0.00829
SB3 Clip Fraction: 0.11890
Policy Update Magnitude: 2.02011
Value Function Update Magnitude: 0.46746

Collected Steps per Second: 20,215.06895
Overall Steps per Second: 9,982.78573

Timestep Collection Time: 2.47588
Timestep Consumption Time: 2.53775
PPO Batch Consumption Time: 0.15407
Total Iteration Time: 5.01363

Cumulative Model Updates: 236
Cumulative Timesteps: 1,050,546

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 165.79396
Policy Entropy: 4.38215
Value Function Loss: 2.51907

Mean KL Divergence: 0.00894
SB3 Clip Fraction: 0.13262
Policy Update Magnitude: 2.02291
Value Function Update Magnitude: 0.54909

Collected Steps per Second: 22,015.39203
Overall Steps per Second: 10,413.72196

Timestep Collection Time: 2.27214
Timestep Consumption Time: 2.53133
PPO Batch Consumption Time: 0.15183
Total Iteration Time: 4.80347

Cumulative Model Updates: 248
Cumulative Timesteps: 1,100,568

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 143.17574
Policy Entropy: 4.37417
Value Function Loss: 2.45337

Mean KL Divergence: 0.00889
SB3 Clip Fraction: 0.13067
Policy Update Magnitude: 2.00821
Value Function Update Magnitude: 0.63573

Collected Steps per Second: 22,273.88118
Overall Steps per Second: 10,534.51045

Timestep Collection Time: 2.24541
Timestep Consumption Time: 2.50222
PPO Batch Consumption Time: 0.15095
Total Iteration Time: 4.74763

Cumulative Model Updates: 260
Cumulative Timesteps: 1,150,582

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 150.85522
Policy Entropy: 4.36138
Value Function Loss: 2.53142

Mean KL Divergence: 0.00901
SB3 Clip Fraction: 0.13142
Policy Update Magnitude: 2.03037
Value Function Update Magnitude: 0.69222

Collected Steps per Second: 21,322.59968
Overall Steps per Second: 10,461.29566

Timestep Collection Time: 2.34587
Timestep Consumption Time: 2.43557
PPO Batch Consumption Time: 0.15106
Total Iteration Time: 4.78143

Cumulative Model Updates: 272
Cumulative Timesteps: 1,200,602

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 174.38576
Policy Entropy: 4.35246
Value Function Loss: 2.44836

Mean KL Divergence: 0.00943
SB3 Clip Fraction: 0.13999
Policy Update Magnitude: 2.03111
Value Function Update Magnitude: 0.71990

Collected Steps per Second: 21,793.70852
Overall Steps per Second: 10,416.59252

Timestep Collection Time: 2.29635
Timestep Consumption Time: 2.50810
PPO Batch Consumption Time: 0.15071
Total Iteration Time: 4.80445

Cumulative Model Updates: 284
Cumulative Timesteps: 1,250,648

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 182.91612
Policy Entropy: 4.34328
Value Function Loss: 2.52867

Mean KL Divergence: 0.00980
SB3 Clip Fraction: 0.14756
Policy Update Magnitude: 2.03308
Value Function Update Magnitude: 0.63091

Collected Steps per Second: 20,839.40171
Overall Steps per Second: 10,334.31185

Timestep Collection Time: 2.40180
Timestep Consumption Time: 2.44149
PPO Batch Consumption Time: 0.14859
Total Iteration Time: 4.84328

Cumulative Model Updates: 296
Cumulative Timesteps: 1,300,700

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 148.75253
Policy Entropy: 4.33590
Value Function Loss: 2.38819

Mean KL Divergence: 0.00975
SB3 Clip Fraction: 0.14635
Policy Update Magnitude: 2.02955
Value Function Update Magnitude: 0.57735

Collected Steps per Second: 22,014.99647
Overall Steps per Second: 10,520.54811

Timestep Collection Time: 2.27136
Timestep Consumption Time: 2.48162
PPO Batch Consumption Time: 0.14994
Total Iteration Time: 4.75298

Cumulative Model Updates: 308
Cumulative Timesteps: 1,350,704

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 133.32705
Policy Entropy: 4.32682
Value Function Loss: 2.43578

Mean KL Divergence: 0.01024
SB3 Clip Fraction: 0.15442
Policy Update Magnitude: 2.00037
Value Function Update Magnitude: 0.56576

Collected Steps per Second: 20,624.17089
Overall Steps per Second: 10,034.16537

Timestep Collection Time: 2.42647
Timestep Consumption Time: 2.56089
PPO Batch Consumption Time: 0.15191
Total Iteration Time: 4.98736

Cumulative Model Updates: 320
Cumulative Timesteps: 1,400,748

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 143.05893
Policy Entropy: 4.31935
Value Function Loss: 2.48315

Mean KL Divergence: 0.01098
SB3 Clip Fraction: 0.16619
Policy Update Magnitude: 1.96335
Value Function Update Magnitude: 0.62392

Collected Steps per Second: 19,389.85990
Overall Steps per Second: 9,868.06582

Timestep Collection Time: 2.57929
Timestep Consumption Time: 2.48878
PPO Batch Consumption Time: 0.15297
Total Iteration Time: 5.06807

Cumulative Model Updates: 332
Cumulative Timesteps: 1,450,760

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 159.01810
Policy Entropy: 4.31171
Value Function Loss: 2.53706

Mean KL Divergence: 0.01118
SB3 Clip Fraction: 0.17272
Policy Update Magnitude: 1.92774
Value Function Update Magnitude: 0.63484

Collected Steps per Second: 21,953.51646
Overall Steps per Second: 9,693.98447

Timestep Collection Time: 2.27991
Timestep Consumption Time: 2.88329
PPO Batch Consumption Time: 0.17520
Total Iteration Time: 5.16320

Cumulative Model Updates: 344
Cumulative Timesteps: 1,500,812

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 143.54445
Policy Entropy: 4.30016
Value Function Loss: 2.46236

Mean KL Divergence: 0.01096
SB3 Clip Fraction: 0.16904
Policy Update Magnitude: 1.90002
Value Function Update Magnitude: 0.59947

Collected Steps per Second: 20,259.81568
Overall Steps per Second: 10,107.00087

Timestep Collection Time: 2.47001
Timestep Consumption Time: 2.48121
PPO Batch Consumption Time: 0.14798
Total Iteration Time: 4.95122

Cumulative Model Updates: 356
Cumulative Timesteps: 1,550,854

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 159.77830
Policy Entropy: 4.29351
Value Function Loss: 2.38620

Mean KL Divergence: 0.01087
SB3 Clip Fraction: 0.16617
Policy Update Magnitude: 1.88092
Value Function Update Magnitude: 0.70660

Collected Steps per Second: 19,059.88680
Overall Steps per Second: 9,380.47347

Timestep Collection Time: 2.62384
Timestep Consumption Time: 2.70745
PPO Batch Consumption Time: 0.16650
Total Iteration Time: 5.33129

Cumulative Model Updates: 368
Cumulative Timesteps: 1,600,864

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 145.30884
Policy Entropy: 4.28359
Value Function Loss: 2.47359

Mean KL Divergence: 0.01109
SB3 Clip Fraction: 0.17166
Policy Update Magnitude: 1.87469
Value Function Update Magnitude: 0.90393

Collected Steps per Second: 20,005.72871
Overall Steps per Second: 10,259.53108

Timestep Collection Time: 2.50178
Timestep Consumption Time: 2.37661
PPO Batch Consumption Time: 0.14405
Total Iteration Time: 4.87839

Cumulative Model Updates: 380
Cumulative Timesteps: 1,650,914

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 142.58691
Policy Entropy: 4.27506
Value Function Loss: 2.42142

Mean KL Divergence: 0.01156
SB3 Clip Fraction: 0.17876
Policy Update Magnitude: 1.81880
Value Function Update Magnitude: 0.62592

Collected Steps per Second: 15,190.33595
Overall Steps per Second: 8,736.55799

Timestep Collection Time: 3.29170
Timestep Consumption Time: 2.43161
PPO Batch Consumption Time: 0.14500
Total Iteration Time: 5.72331

Cumulative Model Updates: 392
Cumulative Timesteps: 1,700,916

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 118.09324
Policy Entropy: 4.26513
Value Function Loss: 2.35952

Mean KL Divergence: 0.01098
SB3 Clip Fraction: 0.16940
Policy Update Magnitude: 1.79218
Value Function Update Magnitude: 0.54566

Collected Steps per Second: 18,009.57734
Overall Steps per Second: 9,687.26108

Timestep Collection Time: 2.78019
Timestep Consumption Time: 2.38846
PPO Batch Consumption Time: 0.14478
Total Iteration Time: 5.16864

Cumulative Model Updates: 404
Cumulative Timesteps: 1,750,986

Timesteps Collected: 50,070
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 176.90091
Policy Entropy: 4.25577
Value Function Loss: 2.44193

Mean KL Divergence: 0.01136
SB3 Clip Fraction: 0.17510
Policy Update Magnitude: 1.79853
Value Function Update Magnitude: 0.51149

Collected Steps per Second: 24,156.67586
Overall Steps per Second: 11,209.84774

Timestep Collection Time: 2.07106
Timestep Consumption Time: 2.39198
PPO Batch Consumption Time: 0.14415
Total Iteration Time: 4.46304

Cumulative Model Updates: 416
Cumulative Timesteps: 1,801,016

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 169.36506
Policy Entropy: 4.25163
Value Function Loss: 2.55814

Mean KL Divergence: 0.01174
SB3 Clip Fraction: 0.18063
Policy Update Magnitude: 1.79281
Value Function Update Magnitude: 0.57373

Collected Steps per Second: 21,474.97483
Overall Steps per Second: 10,565.90139

Timestep Collection Time: 2.32848
Timestep Consumption Time: 2.40410
PPO Batch Consumption Time: 0.14545
Total Iteration Time: 4.73258

Cumulative Model Updates: 428
Cumulative Timesteps: 1,851,020

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 148.64121
Policy Entropy: 4.24210
Value Function Loss: 2.62584

Mean KL Divergence: 0.01158
SB3 Clip Fraction: 0.17908
Policy Update Magnitude: 1.78126
Value Function Update Magnitude: 0.57110

Collected Steps per Second: 22,789.98531
Overall Steps per Second: 10,661.83297

Timestep Collection Time: 2.19421
Timestep Consumption Time: 2.49598
PPO Batch Consumption Time: 0.15429
Total Iteration Time: 4.69019

Cumulative Model Updates: 440
Cumulative Timesteps: 1,901,026

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 139.27799
Policy Entropy: 4.23796
Value Function Loss: 2.61160

Mean KL Divergence: 0.01141
SB3 Clip Fraction: 0.17608
Policy Update Magnitude: 1.75575
Value Function Update Magnitude: 0.55502

Collected Steps per Second: 20,312.05492
Overall Steps per Second: 9,956.27594

Timestep Collection Time: 2.46268
Timestep Consumption Time: 2.56149
PPO Batch Consumption Time: 0.15475
Total Iteration Time: 5.02417

Cumulative Model Updates: 452
Cumulative Timesteps: 1,951,048

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 170.79652
Policy Entropy: 4.22760
Value Function Loss: 2.64955

Mean KL Divergence: 0.01197
SB3 Clip Fraction: 0.18490
Policy Update Magnitude: 1.76583
Value Function Update Magnitude: 0.54100

Collected Steps per Second: 20,304.20498
Overall Steps per Second: 10,366.09155

Timestep Collection Time: 2.46304
Timestep Consumption Time: 2.36135
PPO Batch Consumption Time: 0.14453
Total Iteration Time: 4.82438

Cumulative Model Updates: 464
Cumulative Timesteps: 2,001,058

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 150.20311
Policy Entropy: 4.22307
Value Function Loss: 2.77450

Mean KL Divergence: 0.01213
SB3 Clip Fraction: 0.18631
Policy Update Magnitude: 1.78479
Value Function Update Magnitude: 0.51455

Collected Steps per Second: 22,609.50190
Overall Steps per Second: 9,821.00604

Timestep Collection Time: 2.21358
Timestep Consumption Time: 2.88243
PPO Batch Consumption Time: 0.14889
Total Iteration Time: 5.09602

Cumulative Model Updates: 476
Cumulative Timesteps: 2,051,106

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 130.16782
Policy Entropy: 4.21425
Value Function Loss: 2.88526

Mean KL Divergence: 0.01253
SB3 Clip Fraction: 0.19480
Policy Update Magnitude: 1.74953
Value Function Update Magnitude: 0.55290

Collected Steps per Second: 23,085.35918
Overall Steps per Second: 10,900.47161

Timestep Collection Time: 2.16587
Timestep Consumption Time: 2.42108
PPO Batch Consumption Time: 0.14521
Total Iteration Time: 4.58696

Cumulative Model Updates: 488
Cumulative Timesteps: 2,101,106

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 175.15184
Policy Entropy: 4.20435
Value Function Loss: 2.96418

Mean KL Divergence: 0.01232
SB3 Clip Fraction: 0.19044
Policy Update Magnitude: 1.72850
Value Function Update Magnitude: 0.68135

Collected Steps per Second: 23,148.50802
Overall Steps per Second: 10,822.62905

Timestep Collection Time: 2.16126
Timestep Consumption Time: 2.46146
PPO Batch Consumption Time: 0.14661
Total Iteration Time: 4.62272

Cumulative Model Updates: 500
Cumulative Timesteps: 2,151,136

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 166.32496
Policy Entropy: 4.19369
Value Function Loss: 2.82144

Mean KL Divergence: 0.01248
SB3 Clip Fraction: 0.19244
Policy Update Magnitude: 1.74827
Value Function Update Magnitude: 0.69275

Collected Steps per Second: 19,874.63174
Overall Steps per Second: 10,133.34279

Timestep Collection Time: 2.51647
Timestep Consumption Time: 2.41911
PPO Batch Consumption Time: 0.14386
Total Iteration Time: 4.93559

Cumulative Model Updates: 512
Cumulative Timesteps: 2,201,150

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 152.01820
Policy Entropy: 4.18455
Value Function Loss: 2.82820

Mean KL Divergence: 0.01251
SB3 Clip Fraction: 0.19177
Policy Update Magnitude: 1.75161
Value Function Update Magnitude: 0.66889

Collected Steps per Second: 21,830.26753
Overall Steps per Second: 10,704.02858

Timestep Collection Time: 2.29241
Timestep Consumption Time: 2.38284
PPO Batch Consumption Time: 0.14440
Total Iteration Time: 4.67525

Cumulative Model Updates: 524
Cumulative Timesteps: 2,251,194

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 180.70695
Policy Entropy: 4.17449
Value Function Loss: 2.82674

Mean KL Divergence: 0.01392
SB3 Clip Fraction: 0.21409
Policy Update Magnitude: 1.73264
Value Function Update Magnitude: 0.74831

Collected Steps per Second: 20,955.88138
Overall Steps per Second: 10,392.56089

Timestep Collection Time: 2.38616
Timestep Consumption Time: 2.42536
PPO Batch Consumption Time: 0.14491
Total Iteration Time: 4.81152

Cumulative Model Updates: 536
Cumulative Timesteps: 2,301,198

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 164.82498
Policy Entropy: 4.16102
Value Function Loss: 3.07582

Mean KL Divergence: 0.01431
SB3 Clip Fraction: 0.21855
Policy Update Magnitude: 1.74346
Value Function Update Magnitude: 0.98647

Collected Steps per Second: 22,970.78334
Overall Steps per Second: 10,655.61360

Timestep Collection Time: 2.17894
Timestep Consumption Time: 2.51830
PPO Batch Consumption Time: 0.15151
Total Iteration Time: 4.69724

Cumulative Model Updates: 548
Cumulative Timesteps: 2,351,250

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 143.79366
Policy Entropy: 4.14691
Value Function Loss: 3.00425

Mean KL Divergence: 0.01437
SB3 Clip Fraction: 0.22013
Policy Update Magnitude: 1.70741
Value Function Update Magnitude: 0.93605

Collected Steps per Second: 22,606.41296
Overall Steps per Second: 10,693.15437

Timestep Collection Time: 2.21318
Timestep Consumption Time: 2.46570
PPO Batch Consumption Time: 0.14420
Total Iteration Time: 4.67888

Cumulative Model Updates: 560
Cumulative Timesteps: 2,401,282

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 197.23006
Policy Entropy: 4.13542
Value Function Loss: 2.89869

Mean KL Divergence: 0.01370
SB3 Clip Fraction: 0.21006
Policy Update Magnitude: 1.68739
Value Function Update Magnitude: 1.14674

Collected Steps per Second: 20,756.20167
Overall Steps per Second: 9,704.56731

Timestep Collection Time: 2.40911
Timestep Consumption Time: 2.74351
PPO Batch Consumption Time: 0.16407
Total Iteration Time: 5.15263

Cumulative Model Updates: 572
Cumulative Timesteps: 2,451,286

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 138.29850
Policy Entropy: 4.12569
Value Function Loss: 2.70273

Mean KL Divergence: 0.01342
SB3 Clip Fraction: 0.20639
Policy Update Magnitude: 1.69290
Value Function Update Magnitude: 0.87180

Collected Steps per Second: 19,646.68413
Overall Steps per Second: 9,477.17227

Timestep Collection Time: 2.54587
Timestep Consumption Time: 2.73186
PPO Batch Consumption Time: 0.15692
Total Iteration Time: 5.27773

Cumulative Model Updates: 584
Cumulative Timesteps: 2,501,304

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 160.33054
Policy Entropy: 4.11471
Value Function Loss: 2.77244

Mean KL Divergence: 0.01394
SB3 Clip Fraction: 0.21540
Policy Update Magnitude: 1.68156
Value Function Update Magnitude: 0.63928

Collected Steps per Second: 19,659.47658
Overall Steps per Second: 9,897.98987

Timestep Collection Time: 2.54391
Timestep Consumption Time: 2.50883
PPO Batch Consumption Time: 0.14632
Total Iteration Time: 5.05274

Cumulative Model Updates: 596
Cumulative Timesteps: 2,551,316

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 176.93546
Policy Entropy: 4.10561
Value Function Loss: 2.87055

Mean KL Divergence: 0.01409
SB3 Clip Fraction: 0.21764
Policy Update Magnitude: 1.65849
Value Function Update Magnitude: 0.62620

Collected Steps per Second: 21,817.28333
Overall Steps per Second: 10,599.15927

Timestep Collection Time: 2.29314
Timestep Consumption Time: 2.42705
PPO Batch Consumption Time: 0.15064
Total Iteration Time: 4.72019

Cumulative Model Updates: 608
Cumulative Timesteps: 2,601,346

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 192.77109
Policy Entropy: 4.08563
Value Function Loss: 2.90244

Mean KL Divergence: 0.01429
SB3 Clip Fraction: 0.21914
Policy Update Magnitude: 1.66361
Value Function Update Magnitude: 0.61820

Collected Steps per Second: 23,177.17668
Overall Steps per Second: 10,877.12085

Timestep Collection Time: 2.15816
Timestep Consumption Time: 2.44049
PPO Batch Consumption Time: 0.14576
Total Iteration Time: 4.59864

Cumulative Model Updates: 620
Cumulative Timesteps: 2,651,366

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 190.68998
Policy Entropy: 4.07298
Value Function Loss: 2.91596

Mean KL Divergence: 0.01415
SB3 Clip Fraction: 0.21751
Policy Update Magnitude: 1.66925
Value Function Update Magnitude: 0.56192

Collected Steps per Second: 22,533.69931
Overall Steps per Second: 10,786.05238

Timestep Collection Time: 2.22005
Timestep Consumption Time: 2.41797
PPO Batch Consumption Time: 0.14555
Total Iteration Time: 4.63803

Cumulative Model Updates: 632
Cumulative Timesteps: 2,701,392

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 164.57641
Policy Entropy: 4.06311
Value Function Loss: 2.94196

Mean KL Divergence: 0.01459
SB3 Clip Fraction: 0.22487
Policy Update Magnitude: 1.62715
Value Function Update Magnitude: 0.55674

Collected Steps per Second: 23,921.23159
Overall Steps per Second: 11,086.37500

Timestep Collection Time: 2.09145
Timestep Consumption Time: 2.42130
PPO Batch Consumption Time: 0.14517
Total Iteration Time: 4.51275

Cumulative Model Updates: 644
Cumulative Timesteps: 2,751,422

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 198.35420
Policy Entropy: 4.04966
Value Function Loss: 2.97116

Mean KL Divergence: 0.01433
SB3 Clip Fraction: 0.21901
Policy Update Magnitude: 1.63826
Value Function Update Magnitude: 0.55299

Collected Steps per Second: 23,655.74870
Overall Steps per Second: 10,912.32191

Timestep Collection Time: 2.11433
Timestep Consumption Time: 2.46911
PPO Batch Consumption Time: 0.14522
Total Iteration Time: 4.58344

Cumulative Model Updates: 656
Cumulative Timesteps: 2,801,438

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 190.85738
Policy Entropy: 4.03343
Value Function Loss: 3.05188

Mean KL Divergence: 0.01475
SB3 Clip Fraction: 0.22187
Policy Update Magnitude: 1.63349
Value Function Update Magnitude: 0.54243

Collected Steps per Second: 21,581.37616
Overall Steps per Second: 10,588.20186

Timestep Collection Time: 2.31885
Timestep Consumption Time: 2.40754
PPO Batch Consumption Time: 0.14674
Total Iteration Time: 4.72639

Cumulative Model Updates: 668
Cumulative Timesteps: 2,851,482

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 163.00133
Policy Entropy: 4.02030
Value Function Loss: 3.19185

Mean KL Divergence: 0.01541
SB3 Clip Fraction: 0.23347
Policy Update Magnitude: 1.59966
Value Function Update Magnitude: 0.46917

Collected Steps per Second: 21,338.49224
Overall Steps per Second: 10,472.95353

Timestep Collection Time: 2.34468
Timestep Consumption Time: 2.43257
PPO Batch Consumption Time: 0.14463
Total Iteration Time: 4.77726

Cumulative Model Updates: 680
Cumulative Timesteps: 2,901,514

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 144.40927
Policy Entropy: 4.00301
Value Function Loss: 3.14266

Mean KL Divergence: 0.01551
SB3 Clip Fraction: 0.23355
Policy Update Magnitude: 1.57434
Value Function Update Magnitude: 0.54852

Collected Steps per Second: 24,517.72613
Overall Steps per Second: 11,228.90623

Timestep Collection Time: 2.03975
Timestep Consumption Time: 2.41394
PPO Batch Consumption Time: 0.14605
Total Iteration Time: 4.45368

Cumulative Model Updates: 692
Cumulative Timesteps: 2,951,524

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 182.56180
Policy Entropy: 3.98596
Value Function Loss: 3.16095

Mean KL Divergence: 0.01514
SB3 Clip Fraction: 0.22949
Policy Update Magnitude: 1.58008
Value Function Update Magnitude: 0.46222

Collected Steps per Second: 23,881.18597
Overall Steps per Second: 11,084.30666

Timestep Collection Time: 2.09428
Timestep Consumption Time: 2.41786
PPO Batch Consumption Time: 0.14458
Total Iteration Time: 4.51215

Cumulative Model Updates: 704
Cumulative Timesteps: 3,001,538

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 170.14691
Policy Entropy: 3.96717
Value Function Loss: 3.07718

Mean KL Divergence: 0.01494
SB3 Clip Fraction: 0.22555
Policy Update Magnitude: 1.57401
Value Function Update Magnitude: 0.46389

Collected Steps per Second: 23,215.37004
Overall Steps per Second: 10,916.60737

Timestep Collection Time: 2.15555
Timestep Consumption Time: 2.42847
PPO Batch Consumption Time: 0.14475
Total Iteration Time: 4.58402

Cumulative Model Updates: 716
Cumulative Timesteps: 3,051,580

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.28281
Policy Entropy: 3.94953
Value Function Loss: 3.29279

Mean KL Divergence: 0.01549
SB3 Clip Fraction: 0.23294
Policy Update Magnitude: 1.57365
Value Function Update Magnitude: 0.44945

Collected Steps per Second: 23,366.13468
Overall Steps per Second: 11,105.39769

Timestep Collection Time: 2.14130
Timestep Consumption Time: 2.36407
PPO Batch Consumption Time: 0.14588
Total Iteration Time: 4.50538

Cumulative Model Updates: 728
Cumulative Timesteps: 3,101,614

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 176.97409
Policy Entropy: 3.92950
Value Function Loss: 3.37210

Mean KL Divergence: 0.01596
SB3 Clip Fraction: 0.24043
Policy Update Magnitude: 1.59542
Value Function Update Magnitude: 0.44973

Collected Steps per Second: 23,266.14040
Overall Steps per Second: 10,844.22949

Timestep Collection Time: 2.14982
Timestep Consumption Time: 2.46259
PPO Batch Consumption Time: 0.14493
Total Iteration Time: 4.61241

Cumulative Model Updates: 740
Cumulative Timesteps: 3,151,632

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 162.48317
Policy Entropy: 3.91061
Value Function Loss: 3.46685

Mean KL Divergence: 0.01606
SB3 Clip Fraction: 0.24048
Policy Update Magnitude: 1.59439
Value Function Update Magnitude: 0.45506

Collected Steps per Second: 17,262.81617
Overall Steps per Second: 9,372.60001

Timestep Collection Time: 2.90034
Timestep Consumption Time: 2.44162
PPO Batch Consumption Time: 0.14531
Total Iteration Time: 5.34195

Cumulative Model Updates: 752
Cumulative Timesteps: 3,201,700

Timesteps Collected: 50,068
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 182.37184
Policy Entropy: 3.90363
Value Function Loss: 3.34412

Mean KL Divergence: 0.01586
SB3 Clip Fraction: 0.23749
Policy Update Magnitude: 1.60052
Value Function Update Magnitude: 0.44973

Collected Steps per Second: 19,237.03407
Overall Steps per Second: 9,954.30078

Timestep Collection Time: 2.59967
Timestep Consumption Time: 2.42429
PPO Batch Consumption Time: 0.14449
Total Iteration Time: 5.02396

Cumulative Model Updates: 764
Cumulative Timesteps: 3,251,710

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 156.33604
Policy Entropy: 3.87782
Value Function Loss: 3.29145

Mean KL Divergence: 0.01745
SB3 Clip Fraction: 0.26028
Policy Update Magnitude: 1.53295
Value Function Update Magnitude: 0.45193

Collected Steps per Second: 23,496.19093
Overall Steps per Second: 10,911.32775

Timestep Collection Time: 2.12886
Timestep Consumption Time: 2.45537
PPO Batch Consumption Time: 0.14449
Total Iteration Time: 4.58423

Cumulative Model Updates: 776
Cumulative Timesteps: 3,301,730

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 157.39993
Policy Entropy: 3.85878
Value Function Loss: 3.37037

Mean KL Divergence: 0.01741
SB3 Clip Fraction: 0.25607
Policy Update Magnitude: 1.55934
Value Function Update Magnitude: 0.45932

Collected Steps per Second: 22,741.53291
Overall Steps per Second: 10,494.35521

Timestep Collection Time: 2.20082
Timestep Consumption Time: 2.56841
PPO Batch Consumption Time: 0.15467
Total Iteration Time: 4.76923

Cumulative Model Updates: 788
Cumulative Timesteps: 3,351,780

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 190.59601
Policy Entropy: 3.83403
Value Function Loss: 3.36963

Mean KL Divergence: 0.01688
SB3 Clip Fraction: 0.25206
Policy Update Magnitude: 1.51620
Value Function Update Magnitude: 0.45360

Collected Steps per Second: 20,766.03734
Overall Steps per Second: 10,312.60020

Timestep Collection Time: 2.40845
Timestep Consumption Time: 2.44134
PPO Batch Consumption Time: 0.14433
Total Iteration Time: 4.84980

Cumulative Model Updates: 800
Cumulative Timesteps: 3,401,794

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 200.26934
Policy Entropy: 3.81348
Value Function Loss: 3.50558

Mean KL Divergence: 0.01705
SB3 Clip Fraction: 0.25265
Policy Update Magnitude: 1.55255
Value Function Update Magnitude: 0.47982

Collected Steps per Second: 20,264.06214
Overall Steps per Second: 10,059.42778

Timestep Collection Time: 2.46930
Timestep Consumption Time: 2.50494
PPO Batch Consumption Time: 0.14863
Total Iteration Time: 4.97424

Cumulative Model Updates: 812
Cumulative Timesteps: 3,451,832

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 179.72581
Policy Entropy: 3.79862
Value Function Loss: 3.53051

Mean KL Divergence: 0.01693
SB3 Clip Fraction: 0.25462
Policy Update Magnitude: 1.52796
Value Function Update Magnitude: 0.48178

Collected Steps per Second: 23,337.24246
Overall Steps per Second: 10,776.11774

Timestep Collection Time: 2.14473
Timestep Consumption Time: 2.49999
PPO Batch Consumption Time: 0.14476
Total Iteration Time: 4.64472

Cumulative Model Updates: 824
Cumulative Timesteps: 3,501,884

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 192.37064
Policy Entropy: 3.78660
Value Function Loss: 3.74338

Mean KL Divergence: 0.01770
SB3 Clip Fraction: 0.26304
Policy Update Magnitude: 1.53354
Value Function Update Magnitude: 0.44814

Collected Steps per Second: 22,394.76757
Overall Steps per Second: 10,774.16829

Timestep Collection Time: 2.23284
Timestep Consumption Time: 2.40826
PPO Batch Consumption Time: 0.14582
Total Iteration Time: 4.64110

Cumulative Model Updates: 836
Cumulative Timesteps: 3,551,888

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 181.90660
Policy Entropy: 3.77196
Value Function Loss: 3.68646

Mean KL Divergence: 0.01740
SB3 Clip Fraction: 0.25986
Policy Update Magnitude: 1.53518
Value Function Update Magnitude: 0.45748

Collected Steps per Second: 25,065.25657
Overall Steps per Second: 11,311.01540

Timestep Collection Time: 1.99639
Timestep Consumption Time: 2.42762
PPO Batch Consumption Time: 0.14528
Total Iteration Time: 4.42401

Cumulative Model Updates: 848
Cumulative Timesteps: 3,601,928

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 164.83531
Policy Entropy: 3.76176
Value Function Loss: 3.82819

Mean KL Divergence: 0.01718
SB3 Clip Fraction: 0.25618
Policy Update Magnitude: 1.59141
Value Function Update Magnitude: 0.46235

Collected Steps per Second: 20,125.99761
Overall Steps per Second: 10,112.36391

Timestep Collection Time: 2.48663
Timestep Consumption Time: 2.46236
PPO Batch Consumption Time: 0.14644
Total Iteration Time: 4.94899

Cumulative Model Updates: 860
Cumulative Timesteps: 3,651,974

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 223.52573
Policy Entropy: 3.74718
Value Function Loss: 3.83970

Mean KL Divergence: 0.01718
SB3 Clip Fraction: 0.25727
Policy Update Magnitude: 1.61566
Value Function Update Magnitude: 0.46259

Collected Steps per Second: 22,700.67418
Overall Steps per Second: 10,932.80101

Timestep Collection Time: 2.20381
Timestep Consumption Time: 2.37214
PPO Batch Consumption Time: 0.14643
Total Iteration Time: 4.57595

Cumulative Model Updates: 872
Cumulative Timesteps: 3,702,002

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.91572
Policy Entropy: 3.73290
Value Function Loss: 4.09139

Mean KL Divergence: 0.01790
SB3 Clip Fraction: 0.26288
Policy Update Magnitude: 1.62891
Value Function Update Magnitude: 0.47318

Collected Steps per Second: 23,388.66844
Overall Steps per Second: 10,915.06243

Timestep Collection Time: 2.13950
Timestep Consumption Time: 2.44499
PPO Batch Consumption Time: 0.14655
Total Iteration Time: 4.58449

Cumulative Model Updates: 884
Cumulative Timesteps: 3,752,042

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.04086
Policy Entropy: 3.71586
Value Function Loss: 4.00025

Mean KL Divergence: 0.01954
SB3 Clip Fraction: 0.28157
Policy Update Magnitude: 1.45819
Value Function Update Magnitude: 0.47716

Collected Steps per Second: 22,694.14428
Overall Steps per Second: 10,969.97208

Timestep Collection Time: 2.20409
Timestep Consumption Time: 2.35563
PPO Batch Consumption Time: 0.14452
Total Iteration Time: 4.55972

Cumulative Model Updates: 896
Cumulative Timesteps: 3,802,062

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 153.17337
Policy Entropy: 3.69921
Value Function Loss: 4.03756

Mean KL Divergence: 0.01851
SB3 Clip Fraction: 0.27248
Policy Update Magnitude: 1.43467
Value Function Update Magnitude: 0.47134

Collected Steps per Second: 20,173.54735
Overall Steps per Second: 9,848.46843

Timestep Collection Time: 2.47998
Timestep Consumption Time: 2.60000
PPO Batch Consumption Time: 0.16016
Total Iteration Time: 5.07998

Cumulative Model Updates: 908
Cumulative Timesteps: 3,852,092

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 201.34860
Policy Entropy: 3.67850
Value Function Loss: 3.87243

Mean KL Divergence: 0.01735
SB3 Clip Fraction: 0.25722
Policy Update Magnitude: 1.53299
Value Function Update Magnitude: 0.47745

Collected Steps per Second: 19,991.05857
Overall Steps per Second: 9,721.57686

Timestep Collection Time: 2.50452
Timestep Consumption Time: 2.64567
PPO Batch Consumption Time: 0.16212
Total Iteration Time: 5.15019

Cumulative Model Updates: 920
Cumulative Timesteps: 3,902,160

Timesteps Collected: 50,068
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 194.36928
Policy Entropy: 3.66350
Value Function Loss: 3.95319

Mean KL Divergence: 0.01759
SB3 Clip Fraction: 0.25923
Policy Update Magnitude: 1.50910
Value Function Update Magnitude: 0.50595

Collected Steps per Second: 20,171.78393
Overall Steps per Second: 9,955.25706

Timestep Collection Time: 2.48020
Timestep Consumption Time: 2.54529
PPO Batch Consumption Time: 0.15079
Total Iteration Time: 5.02549

Cumulative Model Updates: 932
Cumulative Timesteps: 3,952,190

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 182.41076
Policy Entropy: 3.63356
Value Function Loss: 4.03546

Mean KL Divergence: 0.01893
SB3 Clip Fraction: 0.27562
Policy Update Magnitude: 1.46808
Value Function Update Magnitude: 0.52814

Collected Steps per Second: 20,485.71430
Overall Steps per Second: 10,099.24706

Timestep Collection Time: 2.44092
Timestep Consumption Time: 2.51034
PPO Batch Consumption Time: 0.14626
Total Iteration Time: 4.95126

Cumulative Model Updates: 944
Cumulative Timesteps: 4,002,194

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.96618
Policy Entropy: 3.61900
Value Function Loss: 4.01208

Mean KL Divergence: 0.01947
SB3 Clip Fraction: 0.28361
Policy Update Magnitude: 1.32961
Value Function Update Magnitude: 0.53416

Collected Steps per Second: 20,796.02109
Overall Steps per Second: 10,490.10147

Timestep Collection Time: 2.40633
Timestep Consumption Time: 2.36408
PPO Batch Consumption Time: 0.14465
Total Iteration Time: 4.77040

Cumulative Model Updates: 956
Cumulative Timesteps: 4,052,236

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 197.80132
Policy Entropy: 3.59444
Value Function Loss: 4.10117

Mean KL Divergence: 0.01719
SB3 Clip Fraction: 0.25639
Policy Update Magnitude: 1.43706
Value Function Update Magnitude: 0.55192

Collected Steps per Second: 22,554.75304
Overall Steps per Second: 10,804.65939

Timestep Collection Time: 2.21763
Timestep Consumption Time: 2.41167
PPO Batch Consumption Time: 0.14530
Total Iteration Time: 4.62930

Cumulative Model Updates: 968
Cumulative Timesteps: 4,102,254

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.03125
Policy Entropy: 3.57925
Value Function Loss: 4.08275

Mean KL Divergence: 0.01761
SB3 Clip Fraction: 0.25732
Policy Update Magnitude: 1.51148
Value Function Update Magnitude: 0.56184

Collected Steps per Second: 20,728.80792
Overall Steps per Second: 10,297.53872

Timestep Collection Time: 2.41480
Timestep Consumption Time: 2.44616
PPO Batch Consumption Time: 0.14708
Total Iteration Time: 4.86097

Cumulative Model Updates: 980
Cumulative Timesteps: 4,152,310

Timesteps Collected: 50,056
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 197.59336
Policy Entropy: 3.55895
Value Function Loss: 4.23253

Mean KL Divergence: 0.01827
SB3 Clip Fraction: 0.26990
Policy Update Magnitude: 1.43289
Value Function Update Magnitude: 0.57526

Collected Steps per Second: 20,879.80141
Overall Steps per Second: 10,158.86455

Timestep Collection Time: 2.39600
Timestep Consumption Time: 2.52857
PPO Batch Consumption Time: 0.15010
Total Iteration Time: 4.92457

Cumulative Model Updates: 992
Cumulative Timesteps: 4,202,338

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.88863
Policy Entropy: 3.53426
Value Function Loss: 4.15115

Mean KL Divergence: 0.01894
SB3 Clip Fraction: 0.27481
Policy Update Magnitude: 1.38056
Value Function Update Magnitude: 0.56267

Collected Steps per Second: 19,113.96331
Overall Steps per Second: 9,856.68580

Timestep Collection Time: 2.61746
Timestep Consumption Time: 2.45828
PPO Batch Consumption Time: 0.14759
Total Iteration Time: 5.07574

Cumulative Model Updates: 1,004
Cumulative Timesteps: 4,252,368

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 201.12717
Policy Entropy: 3.52430
Value Function Loss: 3.94197

Mean KL Divergence: 0.01775
SB3 Clip Fraction: 0.26372
Policy Update Magnitude: 1.32058
Value Function Update Magnitude: 0.68521

Collected Steps per Second: 19,522.39765
Overall Steps per Second: 9,923.11432

Timestep Collection Time: 2.56178
Timestep Consumption Time: 2.47817
PPO Batch Consumption Time: 0.14822
Total Iteration Time: 5.03995

Cumulative Model Updates: 1,016
Cumulative Timesteps: 4,302,380

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 194.74516
Policy Entropy: 3.50111
Value Function Loss: 3.92309

Mean KL Divergence: 0.01697
SB3 Clip Fraction: 0.25367
Policy Update Magnitude: 1.46132
Value Function Update Magnitude: 0.81591

Collected Steps per Second: 19,822.35408
Overall Steps per Second: 10,053.33650

Timestep Collection Time: 2.52362
Timestep Consumption Time: 2.45225
PPO Batch Consumption Time: 0.14658
Total Iteration Time: 4.97586

Cumulative Model Updates: 1,028
Cumulative Timesteps: 4,352,404

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.90988
Policy Entropy: 3.48445
Value Function Loss: 3.98822

Mean KL Divergence: 0.01834
SB3 Clip Fraction: 0.26466
Policy Update Magnitude: 1.45694
Value Function Update Magnitude: 0.87756

Collected Steps per Second: 18,523.84359
Overall Steps per Second: 9,497.31970

Timestep Collection Time: 2.70106
Timestep Consumption Time: 2.56716
PPO Batch Consumption Time: 0.15500
Total Iteration Time: 5.26822

Cumulative Model Updates: 1,040
Cumulative Timesteps: 4,402,438

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.35895
Policy Entropy: 3.46339
Value Function Loss: 4.13435

Mean KL Divergence: 0.01880
SB3 Clip Fraction: 0.27272
Policy Update Magnitude: 1.36118
Value Function Update Magnitude: 0.90473

Collected Steps per Second: 18,655.66190
Overall Steps per Second: 9,646.68263

Timestep Collection Time: 2.68037
Timestep Consumption Time: 2.50318
PPO Batch Consumption Time: 0.14886
Total Iteration Time: 5.18354

Cumulative Model Updates: 1,052
Cumulative Timesteps: 4,452,442

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.31344
Policy Entropy: 3.45075
Value Function Loss: 4.18331

Mean KL Divergence: 0.01893
SB3 Clip Fraction: 0.27483
Policy Update Magnitude: 1.32788
Value Function Update Magnitude: 0.88736

Collected Steps per Second: 21,461.57433
Overall Steps per Second: 10,325.21879

Timestep Collection Time: 2.32984
Timestep Consumption Time: 2.51287
PPO Batch Consumption Time: 0.15349
Total Iteration Time: 4.84271

Cumulative Model Updates: 1,064
Cumulative Timesteps: 4,502,444

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 178.62547
Policy Entropy: 3.43515
Value Function Loss: 4.17087

Mean KL Divergence: 0.01860
SB3 Clip Fraction: 0.27321
Policy Update Magnitude: 1.29803
Value Function Update Magnitude: 0.86576

Collected Steps per Second: 21,474.25901
Overall Steps per Second: 10,128.15540

Timestep Collection Time: 2.32958
Timestep Consumption Time: 2.60972
PPO Batch Consumption Time: 0.15957
Total Iteration Time: 4.93930

Cumulative Model Updates: 1,076
Cumulative Timesteps: 4,552,470

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 218.91926
Policy Entropy: 3.42155
Value Function Loss: 4.34341

Mean KL Divergence: 0.01823
SB3 Clip Fraction: 0.27050
Policy Update Magnitude: 1.32076
Value Function Update Magnitude: 0.86793

Collected Steps per Second: 19,022.01201
Overall Steps per Second: 9,866.14169

Timestep Collection Time: 2.63001
Timestep Consumption Time: 2.44067
PPO Batch Consumption Time: 0.14460
Total Iteration Time: 5.07068

Cumulative Model Updates: 1,088
Cumulative Timesteps: 4,602,498

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 189.83229
Policy Entropy: 3.41309
Value Function Loss: 4.33875

Mean KL Divergence: 0.01779
SB3 Clip Fraction: 0.25925
Policy Update Magnitude: 1.41485
Value Function Update Magnitude: 0.83933

Collected Steps per Second: 21,131.34881
Overall Steps per Second: 10,101.29934

Timestep Collection Time: 2.36719
Timestep Consumption Time: 2.58484
PPO Batch Consumption Time: 0.14570
Total Iteration Time: 4.95204

Cumulative Model Updates: 1,100
Cumulative Timesteps: 4,652,520

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.33152
Policy Entropy: 3.40889
Value Function Loss: 4.27001

Mean KL Divergence: 0.01764
SB3 Clip Fraction: 0.25841
Policy Update Magnitude: 1.40586
Value Function Update Magnitude: 0.85382

Collected Steps per Second: 21,774.00253
Overall Steps per Second: 10,507.74581

Timestep Collection Time: 2.29834
Timestep Consumption Time: 2.46424
PPO Batch Consumption Time: 0.14824
Total Iteration Time: 4.76258

Cumulative Model Updates: 1,112
Cumulative Timesteps: 4,702,564

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.03613
Policy Entropy: 3.39753
Value Function Loss: 4.03556

Mean KL Divergence: 0.01919
SB3 Clip Fraction: 0.27854
Policy Update Magnitude: 1.34232
Value Function Update Magnitude: 0.80862

Collected Steps per Second: 20,055.43971
Overall Steps per Second: 10,312.05141

Timestep Collection Time: 2.49608
Timestep Consumption Time: 2.35843
PPO Batch Consumption Time: 0.14427
Total Iteration Time: 4.85451

Cumulative Model Updates: 1,124
Cumulative Timesteps: 4,752,624

Timesteps Collected: 50,060
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 201.16371
Policy Entropy: 3.38344
Value Function Loss: 3.81229

Mean KL Divergence: 0.01886
SB3 Clip Fraction: 0.27305
Policy Update Magnitude: 1.30926
Value Function Update Magnitude: 0.72916

Collected Steps per Second: 19,232.08658
Overall Steps per Second: 9,904.00609

Timestep Collection Time: 2.59993
Timestep Consumption Time: 2.44874
PPO Batch Consumption Time: 0.14520
Total Iteration Time: 5.04866

Cumulative Model Updates: 1,136
Cumulative Timesteps: 4,802,626

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 242.12745
Policy Entropy: 3.37162
Value Function Loss: 3.82859

Mean KL Divergence: 0.01767
SB3 Clip Fraction: 0.25978
Policy Update Magnitude: 1.33861
Value Function Update Magnitude: 0.67366

Collected Steps per Second: 19,929.66404
Overall Steps per Second: 10,260.21426

Timestep Collection Time: 2.51143
Timestep Consumption Time: 2.36683
PPO Batch Consumption Time: 0.14616
Total Iteration Time: 4.87826

Cumulative Model Updates: 1,148
Cumulative Timesteps: 4,852,678

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 219.54460
Policy Entropy: 3.36411
Value Function Loss: 3.94225

Mean KL Divergence: 0.01985
SB3 Clip Fraction: 0.28407
Policy Update Magnitude: 1.35165
Value Function Update Magnitude: 0.69863

Collected Steps per Second: 20,649.79121
Overall Steps per Second: 10,137.06348

Timestep Collection Time: 2.42317
Timestep Consumption Time: 2.51297
PPO Batch Consumption Time: 0.14991
Total Iteration Time: 4.93614

Cumulative Model Updates: 1,160
Cumulative Timesteps: 4,902,716

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.35525
Policy Entropy: 3.36757
Value Function Loss: 3.97744

Mean KL Divergence: 0.02071
SB3 Clip Fraction: 0.29134
Policy Update Magnitude: 1.25761
Value Function Update Magnitude: 0.79674

Collected Steps per Second: 20,578.85764
Overall Steps per Second: 9,776.00739

Timestep Collection Time: 2.42978
Timestep Consumption Time: 2.68499
PPO Batch Consumption Time: 0.16049
Total Iteration Time: 5.11477

Cumulative Model Updates: 1,172
Cumulative Timesteps: 4,952,718

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.07819
Policy Entropy: 3.36104
Value Function Loss: 4.09233

Mean KL Divergence: 0.01942
SB3 Clip Fraction: 0.27942
Policy Update Magnitude: 1.35299
Value Function Update Magnitude: 0.76685

Collected Steps per Second: 21,671.17092
Overall Steps per Second: 10,449.65570

Timestep Collection Time: 2.30934
Timestep Consumption Time: 2.47991
PPO Batch Consumption Time: 0.14714
Total Iteration Time: 4.78925

Cumulative Model Updates: 1,184
Cumulative Timesteps: 5,002,764

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 230.36194
Policy Entropy: 3.34511
Value Function Loss: 4.04081

Mean KL Divergence: 0.02129
SB3 Clip Fraction: 0.30250
Policy Update Magnitude: 1.24985
Value Function Update Magnitude: 0.62992

Collected Steps per Second: 19,948.73004
Overall Steps per Second: 9,802.00072

Timestep Collection Time: 2.50763
Timestep Consumption Time: 2.59582
PPO Batch Consumption Time: 0.15736
Total Iteration Time: 5.10345

Cumulative Model Updates: 1,196
Cumulative Timesteps: 5,052,788

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.96096
Policy Entropy: 3.32924
Value Function Loss: 4.01770

Mean KL Divergence: 0.01890
SB3 Clip Fraction: 0.27334
Policy Update Magnitude: 1.26693
Value Function Update Magnitude: 0.63675

Collected Steps per Second: 19,432.14513
Overall Steps per Second: 9,951.58036

Timestep Collection Time: 2.57388
Timestep Consumption Time: 2.45206
PPO Batch Consumption Time: 0.14835
Total Iteration Time: 5.02594

Cumulative Model Updates: 1,208
Cumulative Timesteps: 5,102,804

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 243.86573
Policy Entropy: 3.31440
Value Function Loss: 3.97343

Mean KL Divergence: 0.01944
SB3 Clip Fraction: 0.28298
Policy Update Magnitude: 1.25188
Value Function Update Magnitude: 0.68018

Collected Steps per Second: 21,437.09744
Overall Steps per Second: 10,520.38346

Timestep Collection Time: 2.33278
Timestep Consumption Time: 2.42066
PPO Batch Consumption Time: 0.14421
Total Iteration Time: 4.75344

Cumulative Model Updates: 1,220
Cumulative Timesteps: 5,152,812

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 200.72600
Policy Entropy: 3.30130
Value Function Loss: 4.01112

Mean KL Divergence: 0.01864
SB3 Clip Fraction: 0.27094
Policy Update Magnitude: 1.26683
Value Function Update Magnitude: 0.82743

Collected Steps per Second: 22,313.54764
Overall Steps per Second: 10,733.02141

Timestep Collection Time: 2.24151
Timestep Consumption Time: 2.41850
PPO Batch Consumption Time: 0.14593
Total Iteration Time: 4.66001

Cumulative Model Updates: 1,232
Cumulative Timesteps: 5,202,828

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.70363
Policy Entropy: 3.29183
Value Function Loss: 4.13372

Mean KL Divergence: 0.01821
SB3 Clip Fraction: 0.26559
Policy Update Magnitude: 1.35146
Value Function Update Magnitude: 0.76393

Collected Steps per Second: 23,282.61195
Overall Steps per Second: 10,807.53511

Timestep Collection Time: 2.14950
Timestep Consumption Time: 2.48116
PPO Batch Consumption Time: 0.14457
Total Iteration Time: 4.63066

Cumulative Model Updates: 1,244
Cumulative Timesteps: 5,252,874

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 188.79935
Policy Entropy: 3.29330
Value Function Loss: 4.11076

Mean KL Divergence: 0.01895
SB3 Clip Fraction: 0.27419
Policy Update Magnitude: 1.31707
Value Function Update Magnitude: 0.72196

Collected Steps per Second: 20,705.47517
Overall Steps per Second: 10,363.54088

Timestep Collection Time: 2.41637
Timestep Consumption Time: 2.41133
PPO Batch Consumption Time: 0.14468
Total Iteration Time: 4.82769

Cumulative Model Updates: 1,256
Cumulative Timesteps: 5,302,906

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.73461
Policy Entropy: 3.28711
Value Function Loss: 4.14722

Mean KL Divergence: 0.01913
SB3 Clip Fraction: 0.27421
Policy Update Magnitude: 1.32311
Value Function Update Magnitude: 0.69754

Collected Steps per Second: 21,335.63353
Overall Steps per Second: 10,605.52109

Timestep Collection Time: 2.34368
Timestep Consumption Time: 2.37122
PPO Batch Consumption Time: 0.14434
Total Iteration Time: 4.71490

Cumulative Model Updates: 1,268
Cumulative Timesteps: 5,352,910

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 251.69470
Policy Entropy: 3.27698
Value Function Loss: 3.98847

Mean KL Divergence: 0.01908
SB3 Clip Fraction: 0.27521
Policy Update Magnitude: 1.30064
Value Function Update Magnitude: 0.68406

Collected Steps per Second: 18,620.24069
Overall Steps per Second: 9,244.67748

Timestep Collection Time: 2.68708
Timestep Consumption Time: 2.72512
PPO Batch Consumption Time: 0.16621
Total Iteration Time: 5.41220

Cumulative Model Updates: 1,280
Cumulative Timesteps: 5,402,944

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 166.85022
Policy Entropy: 3.26635
Value Function Loss: 3.90344

Mean KL Divergence: 0.01869
SB3 Clip Fraction: 0.27020
Policy Update Magnitude: 1.29652
Value Function Update Magnitude: 0.75458

Collected Steps per Second: 21,757.91982
Overall Steps per Second: 10,268.60936

Timestep Collection Time: 2.29801
Timestep Consumption Time: 2.57119
PPO Batch Consumption Time: 0.15421
Total Iteration Time: 4.86921

Cumulative Model Updates: 1,292
Cumulative Timesteps: 5,452,944

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.28180
Policy Entropy: 3.25114
Value Function Loss: 3.83521

Mean KL Divergence: 0.01874
SB3 Clip Fraction: 0.27021
Policy Update Magnitude: 1.30677
Value Function Update Magnitude: 0.81865

Collected Steps per Second: 21,473.10739
Overall Steps per Second: 10,514.92848

Timestep Collection Time: 2.32868
Timestep Consumption Time: 2.42684
PPO Batch Consumption Time: 0.14704
Total Iteration Time: 4.75552

Cumulative Model Updates: 1,304
Cumulative Timesteps: 5,502,948

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 234.04313
Policy Entropy: 3.24046
Value Function Loss: 3.84032

Mean KL Divergence: 0.01939
SB3 Clip Fraction: 0.27705
Policy Update Magnitude: 1.26570
Value Function Update Magnitude: 0.72602

Collected Steps per Second: 18,070.52771
Overall Steps per Second: 9,677.07107

Timestep Collection Time: 2.76915
Timestep Consumption Time: 2.40184
PPO Batch Consumption Time: 0.14431
Total Iteration Time: 5.17099

Cumulative Model Updates: 1,316
Cumulative Timesteps: 5,552,988

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 233.30788
Policy Entropy: 3.23080
Value Function Loss: 3.90354

Mean KL Divergence: 0.01934
SB3 Clip Fraction: 0.27835
Policy Update Magnitude: 1.18550
Value Function Update Magnitude: 0.71732

Collected Steps per Second: 23,143.00307
Overall Steps per Second: 10,746.89825

Timestep Collection Time: 2.16091
Timestep Consumption Time: 2.49252
PPO Batch Consumption Time: 0.15675
Total Iteration Time: 4.65344

Cumulative Model Updates: 1,328
Cumulative Timesteps: 5,602,998

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 221.80169
Policy Entropy: 3.22257
Value Function Loss: 3.90934

Mean KL Divergence: 0.02063
SB3 Clip Fraction: 0.29491
Policy Update Magnitude: 1.14666
Value Function Update Magnitude: 0.82121

Collected Steps per Second: 16,402.15972
Overall Steps per Second: 8,757.77887

Timestep Collection Time: 3.05204
Timestep Consumption Time: 2.66402
PPO Batch Consumption Time: 0.15447
Total Iteration Time: 5.71606

Cumulative Model Updates: 1,340
Cumulative Timesteps: 5,653,058

Timesteps Collected: 50,060
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.58088
Policy Entropy: 3.21269
Value Function Loss: 3.99887

Mean KL Divergence: 0.02192
SB3 Clip Fraction: 0.29988
Policy Update Magnitude: 1.08516
Value Function Update Magnitude: 0.78865

Collected Steps per Second: 19,904.62160
Overall Steps per Second: 10,004.60029

Timestep Collection Time: 2.51308
Timestep Consumption Time: 2.48682
PPO Batch Consumption Time: 0.14772
Total Iteration Time: 4.99990

Cumulative Model Updates: 1,352
Cumulative Timesteps: 5,703,080

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 215.24349
Policy Entropy: 3.19276
Value Function Loss: 3.90228

Mean KL Divergence: 0.01947
SB3 Clip Fraction: 0.27853
Policy Update Magnitude: 1.12939
Value Function Update Magnitude: 0.77953

Collected Steps per Second: 20,556.52829
Overall Steps per Second: 10,109.63929

Timestep Collection Time: 2.43271
Timestep Consumption Time: 2.51386
PPO Batch Consumption Time: 0.14830
Total Iteration Time: 4.94657

Cumulative Model Updates: 1,364
Cumulative Timesteps: 5,753,088

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 230.81646
Policy Entropy: 3.18071
Value Function Loss: 3.92682

Mean KL Divergence: 0.01767
SB3 Clip Fraction: 0.25634
Policy Update Magnitude: 1.29006
Value Function Update Magnitude: 0.81887

Collected Steps per Second: 19,347.76970
Overall Steps per Second: 9,626.58865

Timestep Collection Time: 2.58676
Timestep Consumption Time: 2.61218
PPO Batch Consumption Time: 0.14919
Total Iteration Time: 5.19893

Cumulative Model Updates: 1,376
Cumulative Timesteps: 5,803,136

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 223.51410
Policy Entropy: 3.16139
Value Function Loss: 4.07152

Mean KL Divergence: 0.01941
SB3 Clip Fraction: 0.27480
Policy Update Magnitude: 1.38228
Value Function Update Magnitude: 0.80304

Collected Steps per Second: 18,473.84584
Overall Steps per Second: 9,448.00723

Timestep Collection Time: 2.70772
Timestep Consumption Time: 2.58673
PPO Batch Consumption Time: 0.15262
Total Iteration Time: 5.29445

Cumulative Model Updates: 1,388
Cumulative Timesteps: 5,853,158

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 226.68713
Policy Entropy: 3.15140
Value Function Loss: 3.95902

Mean KL Divergence: 0.02097
SB3 Clip Fraction: 0.29105
Policy Update Magnitude: 1.17271
Value Function Update Magnitude: 0.77162

Collected Steps per Second: 18,073.97864
Overall Steps per Second: 9,292.45246

Timestep Collection Time: 2.76729
Timestep Consumption Time: 2.61514
PPO Batch Consumption Time: 0.15691
Total Iteration Time: 5.38243

Cumulative Model Updates: 1,400
Cumulative Timesteps: 5,903,174

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.39532
Policy Entropy: 3.14823
Value Function Loss: 3.82246

Mean KL Divergence: 0.02053
SB3 Clip Fraction: 0.28642
Policy Update Magnitude: 1.13042
Value Function Update Magnitude: 0.69568

Collected Steps per Second: 18,871.55638
Overall Steps per Second: 9,565.44556

Timestep Collection Time: 2.65055
Timestep Consumption Time: 2.57869
PPO Batch Consumption Time: 0.14995
Total Iteration Time: 5.22924

Cumulative Model Updates: 1,412
Cumulative Timesteps: 5,953,194

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 241.43138
Policy Entropy: 3.14427
Value Function Loss: 3.66216

Mean KL Divergence: 0.02015
SB3 Clip Fraction: 0.28575
Policy Update Magnitude: 1.12573
Value Function Update Magnitude: 0.71856

Collected Steps per Second: 19,478.42179
Overall Steps per Second: 9,808.40781

Timestep Collection Time: 2.56961
Timestep Consumption Time: 2.53336
PPO Batch Consumption Time: 0.14917
Total Iteration Time: 5.10297

Cumulative Model Updates: 1,424
Cumulative Timesteps: 6,003,246

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 246.48328
Policy Entropy: 3.14364
Value Function Loss: 3.80513

Mean KL Divergence: 0.01990
SB3 Clip Fraction: 0.28020
Policy Update Magnitude: 1.16065
Value Function Update Magnitude: 0.74777

Collected Steps per Second: 20,627.77696
Overall Steps per Second: 10,164.15063

Timestep Collection Time: 2.42586
Timestep Consumption Time: 2.49733
PPO Batch Consumption Time: 0.14767
Total Iteration Time: 4.92319

Cumulative Model Updates: 1,436
Cumulative Timesteps: 6,053,286

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 192.26361
Policy Entropy: 3.13539
Value Function Loss: 3.92163

Mean KL Divergence: 0.01896
SB3 Clip Fraction: 0.26990
Policy Update Magnitude: 1.27277
Value Function Update Magnitude: 0.86564

Collected Steps per Second: 18,857.47847
Overall Steps per Second: 9,409.86416

Timestep Collection Time: 2.65497
Timestep Consumption Time: 2.66562
PPO Batch Consumption Time: 0.16115
Total Iteration Time: 5.32059

Cumulative Model Updates: 1,448
Cumulative Timesteps: 6,103,352

Timesteps Collected: 50,066
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 181.63060
Policy Entropy: 3.12831
Value Function Loss: 3.84790

Mean KL Divergence: 0.02091
SB3 Clip Fraction: 0.29267
Policy Update Magnitude: 1.24220
Value Function Update Magnitude: 0.83745

Collected Steps per Second: 20,062.61040
Overall Steps per Second: 9,718.87186

Timestep Collection Time: 2.49240
Timestep Consumption Time: 2.65264
PPO Batch Consumption Time: 0.15733
Total Iteration Time: 5.14504

Cumulative Model Updates: 1,460
Cumulative Timesteps: 6,153,356

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 240.31044
Policy Entropy: 3.11756
Value Function Loss: 3.80974

Mean KL Divergence: 0.01963
SB3 Clip Fraction: 0.27553
Policy Update Magnitude: 1.24405
Value Function Update Magnitude: 0.77311

Collected Steps per Second: 17,456.38137
Overall Steps per Second: 9,207.34791

Timestep Collection Time: 2.86692
Timestep Consumption Time: 2.56852
PPO Batch Consumption Time: 0.15423
Total Iteration Time: 5.43544

Cumulative Model Updates: 1,472
Cumulative Timesteps: 6,203,402

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 221.91758
Policy Entropy: 3.11024
Value Function Loss: 3.77190

Mean KL Divergence: 0.01960
SB3 Clip Fraction: 0.28131
Policy Update Magnitude: 1.24832
Value Function Update Magnitude: 0.74005

Collected Steps per Second: 17,984.93322
Overall Steps per Second: 9,478.66383

Timestep Collection Time: 2.78155
Timestep Consumption Time: 2.49620
PPO Batch Consumption Time: 0.14707
Total Iteration Time: 5.27775

Cumulative Model Updates: 1,484
Cumulative Timesteps: 6,253,428

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 239.07295
Policy Entropy: 3.10075
Value Function Loss: 3.83644

Mean KL Divergence: 0.02153
SB3 Clip Fraction: 0.29637
Policy Update Magnitude: 1.14989
Value Function Update Magnitude: 0.74259

Collected Steps per Second: 18,768.26725
Overall Steps per Second: 9,635.67551

Timestep Collection Time: 2.66460
Timestep Consumption Time: 2.52548
PPO Batch Consumption Time: 0.14757
Total Iteration Time: 5.19009

Cumulative Model Updates: 1,496
Cumulative Timesteps: 6,303,438

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.79638
Policy Entropy: 3.08680
Value Function Loss: 3.86409

Mean KL Divergence: 0.02031
SB3 Clip Fraction: 0.28337
Policy Update Magnitude: 1.06920
Value Function Update Magnitude: 0.81045

Collected Steps per Second: 18,215.81834
Overall Steps per Second: 9,505.24613

Timestep Collection Time: 2.74640
Timestep Consumption Time: 2.51679
PPO Batch Consumption Time: 0.15150
Total Iteration Time: 5.26320

Cumulative Model Updates: 1,508
Cumulative Timesteps: 6,353,466

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 241.63460
Policy Entropy: 3.07216
Value Function Loss: 3.84295

Mean KL Divergence: 0.01973
SB3 Clip Fraction: 0.27616
Policy Update Magnitude: 1.14601
Value Function Update Magnitude: 0.73040

Collected Steps per Second: 17,810.75349
Overall Steps per Second: 8,803.01933

Timestep Collection Time: 2.81134
Timestep Consumption Time: 2.87671
PPO Batch Consumption Time: 0.15723
Total Iteration Time: 5.68805

Cumulative Model Updates: 1,520
Cumulative Timesteps: 6,403,538

Timesteps Collected: 50,072
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 215.01578
Policy Entropy: 3.05562
Value Function Loss: 3.79761

Mean KL Divergence: 0.01920
SB3 Clip Fraction: 0.27641
Policy Update Magnitude: 1.07352
Value Function Update Magnitude: 0.76777

Collected Steps per Second: 17,742.55932
Overall Steps per Second: 9,277.61987

Timestep Collection Time: 2.81842
Timestep Consumption Time: 2.57154
PPO Batch Consumption Time: 0.15440
Total Iteration Time: 5.38996

Cumulative Model Updates: 1,532
Cumulative Timesteps: 6,453,544

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 210.20603
Policy Entropy: 3.03871
Value Function Loss: 3.74465

Mean KL Divergence: 0.01966
SB3 Clip Fraction: 0.27846
Policy Update Magnitude: 1.02135
Value Function Update Magnitude: 0.80634

Collected Steps per Second: 16,854.37078
Overall Steps per Second: 9,109.22270

Timestep Collection Time: 2.96671
Timestep Consumption Time: 2.52245
PPO Batch Consumption Time: 0.14748
Total Iteration Time: 5.48916

Cumulative Model Updates: 1,544
Cumulative Timesteps: 6,503,546

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 204.98820
Policy Entropy: 3.02512
Value Function Loss: 3.74021

Mean KL Divergence: 0.01904
SB3 Clip Fraction: 0.27171
Policy Update Magnitude: 1.03295
Value Function Update Magnitude: 0.74874

Collected Steps per Second: 18,853.37838
Overall Steps per Second: 9,676.01881

Timestep Collection Time: 2.65544
Timestep Consumption Time: 2.51859
PPO Batch Consumption Time: 0.15342
Total Iteration Time: 5.17403

Cumulative Model Updates: 1,556
Cumulative Timesteps: 6,553,610

Timesteps Collected: 50,064
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.51570
Policy Entropy: 3.01318
Value Function Loss: 3.70067

Mean KL Divergence: 0.01890
SB3 Clip Fraction: 0.27096
Policy Update Magnitude: 1.04823
Value Function Update Magnitude: 0.73389

Collected Steps per Second: 22,214.13492
Overall Steps per Second: 10,694.24157

Timestep Collection Time: 2.25154
Timestep Consumption Time: 2.42537
PPO Batch Consumption Time: 0.14731
Total Iteration Time: 4.67691

Cumulative Model Updates: 1,568
Cumulative Timesteps: 6,603,626

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.47150
Policy Entropy: 3.00031
Value Function Loss: 3.53197

Mean KL Divergence: 0.01865
SB3 Clip Fraction: 0.26699
Policy Update Magnitude: 0.98510
Value Function Update Magnitude: 0.73345

Collected Steps per Second: 20,449.31835
Overall Steps per Second: 9,919.03645

Timestep Collection Time: 2.44595
Timestep Consumption Time: 2.59668
PPO Batch Consumption Time: 0.15402
Total Iteration Time: 5.04263

Cumulative Model Updates: 1,580
Cumulative Timesteps: 6,653,644

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.63045
Policy Entropy: 2.98220
Value Function Loss: 3.42766

Mean KL Divergence: 0.01715
SB3 Clip Fraction: 0.24898
Policy Update Magnitude: 1.03038
Value Function Update Magnitude: 0.73055

Collected Steps per Second: 19,454.30836
Overall Steps per Second: 9,391.42979

Timestep Collection Time: 2.57054
Timestep Consumption Time: 2.75432
PPO Batch Consumption Time: 0.16927
Total Iteration Time: 5.32485

Cumulative Model Updates: 1,592
Cumulative Timesteps: 6,703,652

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 198.52597
Policy Entropy: 2.96293
Value Function Loss: 3.49427

Mean KL Divergence: 0.01802
SB3 Clip Fraction: 0.25767
Policy Update Magnitude: 0.97614
Value Function Update Magnitude: 0.69571

Collected Steps per Second: 18,915.04795
Overall Steps per Second: 9,727.62873

Timestep Collection Time: 2.64456
Timestep Consumption Time: 2.49770
PPO Batch Consumption Time: 0.14876
Total Iteration Time: 5.14226

Cumulative Model Updates: 1,604
Cumulative Timesteps: 6,753,674

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.29114
Policy Entropy: 2.93941
Value Function Loss: 3.51644

Mean KL Divergence: 0.01728
SB3 Clip Fraction: 0.24958
Policy Update Magnitude: 0.99345
Value Function Update Magnitude: 0.72042

Collected Steps per Second: 17,823.48548
Overall Steps per Second: 9,275.05547

Timestep Collection Time: 2.80585
Timestep Consumption Time: 2.58603
PPO Batch Consumption Time: 0.16098
Total Iteration Time: 5.39188

Cumulative Model Updates: 1,616
Cumulative Timesteps: 6,803,684

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 219.48344
Policy Entropy: 2.90076
Value Function Loss: 3.52776

Mean KL Divergence: 0.01765
SB3 Clip Fraction: 0.25439
Policy Update Magnitude: 0.93554
Value Function Update Magnitude: 0.70260

Collected Steps per Second: 18,818.06136
Overall Steps per Second: 9,521.43643

Timestep Collection Time: 2.65723
Timestep Consumption Time: 2.59449
PPO Batch Consumption Time: 0.15138
Total Iteration Time: 5.25173

Cumulative Model Updates: 1,628
Cumulative Timesteps: 6,853,688

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 204.15801
Policy Entropy: 2.88141
Value Function Loss: 3.56067

Mean KL Divergence: 0.01763
SB3 Clip Fraction: 0.25534
Policy Update Magnitude: 0.93057
Value Function Update Magnitude: 0.73234

Collected Steps per Second: 19,254.67595
Overall Steps per Second: 9,864.17382

Timestep Collection Time: 2.59864
Timestep Consumption Time: 2.47386
PPO Batch Consumption Time: 0.15219
Total Iteration Time: 5.07250

Cumulative Model Updates: 1,640
Cumulative Timesteps: 6,903,724

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 174.16175
Policy Entropy: 2.84702
Value Function Loss: 3.83406

Mean KL Divergence: 0.01708
SB3 Clip Fraction: 0.25043
Policy Update Magnitude: 0.93442
Value Function Update Magnitude: 0.70300

Collected Steps per Second: 19,795.93758
Overall Steps per Second: 9,962.77995

Timestep Collection Time: 2.52759
Timestep Consumption Time: 2.49470
PPO Batch Consumption Time: 0.15071
Total Iteration Time: 5.02229

Cumulative Model Updates: 1,652
Cumulative Timesteps: 6,953,760

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 227.45042
Policy Entropy: 2.83084
Value Function Loss: 3.83054

Mean KL Divergence: 0.01800
SB3 Clip Fraction: 0.25891
Policy Update Magnitude: 0.83974
Value Function Update Magnitude: 0.72270

Collected Steps per Second: 19,142.74343
Overall Steps per Second: 9,786.50198

Timestep Collection Time: 2.61394
Timestep Consumption Time: 2.49902
PPO Batch Consumption Time: 0.15092
Total Iteration Time: 5.11296

Cumulative Model Updates: 1,664
Cumulative Timesteps: 7,003,798

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 228.77713
Policy Entropy: 2.79963
Value Function Loss: 3.65505

Mean KL Divergence: 0.01668
SB3 Clip Fraction: 0.24022
Policy Update Magnitude: 0.82134
Value Function Update Magnitude: 0.72927

Collected Steps per Second: 19,460.87893
Overall Steps per Second: 9,188.51579

Timestep Collection Time: 2.56936
Timestep Consumption Time: 2.87243
PPO Batch Consumption Time: 0.16115
Total Iteration Time: 5.44179

Cumulative Model Updates: 1,676
Cumulative Timesteps: 7,053,800

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.89222
Policy Entropy: 2.74681
Value Function Loss: 3.53828

Mean KL Divergence: 0.01685
SB3 Clip Fraction: 0.24372
Policy Update Magnitude: 0.86266
Value Function Update Magnitude: 0.81621

Collected Steps per Second: 19,388.55307
Overall Steps per Second: 9,630.54634

Timestep Collection Time: 2.58090
Timestep Consumption Time: 2.61506
PPO Batch Consumption Time: 0.15631
Total Iteration Time: 5.19597

Cumulative Model Updates: 1,688
Cumulative Timesteps: 7,103,840

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.95651
Policy Entropy: 2.75186
Value Function Loss: 3.54271

Mean KL Divergence: 0.01882
SB3 Clip Fraction: 0.27142
Policy Update Magnitude: 0.83626
Value Function Update Magnitude: 0.90213

Collected Steps per Second: 19,411.87987
Overall Steps per Second: 9,833.47285

Timestep Collection Time: 2.57749
Timestep Consumption Time: 2.51064
PPO Batch Consumption Time: 0.15602
Total Iteration Time: 5.08813

Cumulative Model Updates: 1,700
Cumulative Timesteps: 7,153,874

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 175.08917
Policy Entropy: 2.75776
Value Function Loss: 3.68382

Mean KL Divergence: 0.01829
SB3 Clip Fraction: 0.26217
Policy Update Magnitude: 0.79848
Value Function Update Magnitude: 0.86462

Collected Steps per Second: 18,103.83760
Overall Steps per Second: 9,436.62511

Timestep Collection Time: 2.76339
Timestep Consumption Time: 2.53808
PPO Batch Consumption Time: 0.15375
Total Iteration Time: 5.30147

Cumulative Model Updates: 1,712
Cumulative Timesteps: 7,203,902

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.06729
Policy Entropy: 2.76671
Value Function Loss: 3.64024

Mean KL Divergence: 0.01765
SB3 Clip Fraction: 0.25468
Policy Update Magnitude: 0.85153
Value Function Update Magnitude: 0.81444

Collected Steps per Second: 21,581.45165
Overall Steps per Second: 10,424.32911

Timestep Collection Time: 2.31847
Timestep Consumption Time: 2.48145
PPO Batch Consumption Time: 0.14795
Total Iteration Time: 4.79993

Cumulative Model Updates: 1,724
Cumulative Timesteps: 7,253,938

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.03103
Policy Entropy: 2.76743
Value Function Loss: 3.73813

Mean KL Divergence: 0.01749
SB3 Clip Fraction: 0.25217
Policy Update Magnitude: 0.88721
Value Function Update Magnitude: 0.80285

Collected Steps per Second: 21,445.87265
Overall Steps per Second: 10,415.65986

Timestep Collection Time: 2.33154
Timestep Consumption Time: 2.46911
PPO Batch Consumption Time: 0.14792
Total Iteration Time: 4.80066

Cumulative Model Updates: 1,736
Cumulative Timesteps: 7,303,940

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.74149
Policy Entropy: 2.77148
Value Function Loss: 3.58940

Mean KL Divergence: 0.01814
SB3 Clip Fraction: 0.26103
Policy Update Magnitude: 0.94672
Value Function Update Magnitude: 0.91536

Collected Steps per Second: 19,547.72414
Overall Steps per Second: 9,825.39179

Timestep Collection Time: 2.55917
Timestep Consumption Time: 2.53233
PPO Batch Consumption Time: 0.14850
Total Iteration Time: 5.09150

Cumulative Model Updates: 1,748
Cumulative Timesteps: 7,353,966

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.71018
Policy Entropy: 2.78612
Value Function Loss: 3.50305

Mean KL Divergence: 0.01780
SB3 Clip Fraction: 0.25658
Policy Update Magnitude: 1.02108
Value Function Update Magnitude: 0.89989

Collected Steps per Second: 20,289.02899
Overall Steps per Second: 10,209.13121

Timestep Collection Time: 2.46458
Timestep Consumption Time: 2.43339
PPO Batch Consumption Time: 0.14853
Total Iteration Time: 4.89797

Cumulative Model Updates: 1,760
Cumulative Timesteps: 7,403,970

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 260.51171
Policy Entropy: 2.77291
Value Function Loss: 3.40398

Mean KL Divergence: 0.01912
SB3 Clip Fraction: 0.27191
Policy Update Magnitude: 0.90293
Value Function Update Magnitude: 0.77099

Collected Steps per Second: 20,707.61366
Overall Steps per Second: 10,080.06337

Timestep Collection Time: 2.41476
Timestep Consumption Time: 2.54592
PPO Batch Consumption Time: 0.15067
Total Iteration Time: 4.96068

Cumulative Model Updates: 1,772
Cumulative Timesteps: 7,453,974

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 227.47972
Policy Entropy: 2.74690
Value Function Loss: 3.41519

Mean KL Divergence: 0.01891
SB3 Clip Fraction: 0.26418
Policy Update Magnitude: 0.95043
Value Function Update Magnitude: 0.74737

Collected Steps per Second: 19,873.45538
Overall Steps per Second: 9,674.09184

Timestep Collection Time: 2.51592
Timestep Consumption Time: 2.65253
PPO Batch Consumption Time: 0.16257
Total Iteration Time: 5.16844

Cumulative Model Updates: 1,784
Cumulative Timesteps: 7,503,974

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 236.04460
Policy Entropy: 2.70463
Value Function Loss: 3.46829

Mean KL Divergence: 0.01736
SB3 Clip Fraction: 0.25105
Policy Update Magnitude: 0.92810
Value Function Update Magnitude: 0.80094

Collected Steps per Second: 19,624.88132
Overall Steps per Second: 9,969.40074

Timestep Collection Time: 2.54860
Timestep Consumption Time: 2.46835
PPO Batch Consumption Time: 0.14800
Total Iteration Time: 5.01695

Cumulative Model Updates: 1,796
Cumulative Timesteps: 7,553,990

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 217.40238
Policy Entropy: 2.67735
Value Function Loss: 3.47557

Mean KL Divergence: 0.01697
SB3 Clip Fraction: 0.24695
Policy Update Magnitude: 0.82846
Value Function Update Magnitude: 0.87944

Collected Steps per Second: 23,185.02643
Overall Steps per Second: 10,833.05523

Timestep Collection Time: 2.15700
Timestep Consumption Time: 2.45943
PPO Batch Consumption Time: 0.14638
Total Iteration Time: 4.61643

Cumulative Model Updates: 1,808
Cumulative Timesteps: 7,604,000

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 226.12155
Policy Entropy: 2.65402
Value Function Loss: 3.54747

Mean KL Divergence: 0.01630
SB3 Clip Fraction: 0.23608
Policy Update Magnitude: 0.84797
Value Function Update Magnitude: 0.87882

Collected Steps per Second: 21,128.65842
Overall Steps per Second: 10,417.31097

Timestep Collection Time: 2.36664
Timestep Consumption Time: 2.43344
PPO Batch Consumption Time: 0.14972
Total Iteration Time: 4.80009

Cumulative Model Updates: 1,820
Cumulative Timesteps: 7,654,004

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.24171
Policy Entropy: 2.62591
Value Function Loss: 3.55643

Mean KL Divergence: 0.01563
SB3 Clip Fraction: 0.23064
Policy Update Magnitude: 0.85084
Value Function Update Magnitude: 0.82345

Collected Steps per Second: 19,660.42127
Overall Steps per Second: 9,853.71432

Timestep Collection Time: 2.54318
Timestep Consumption Time: 2.53105
PPO Batch Consumption Time: 0.14982
Total Iteration Time: 5.07423

Cumulative Model Updates: 1,832
Cumulative Timesteps: 7,704,004

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 250.90754
Policy Entropy: 2.59594
Value Function Loss: 3.61380

Mean KL Divergence: 0.01610
SB3 Clip Fraction: 0.23494
Policy Update Magnitude: 0.86345
Value Function Update Magnitude: 0.84604

Collected Steps per Second: 20,751.81504
Overall Steps per Second: 10,357.50215

Timestep Collection Time: 2.41068
Timestep Consumption Time: 2.41925
PPO Batch Consumption Time: 0.14970
Total Iteration Time: 4.82993

Cumulative Model Updates: 1,844
Cumulative Timesteps: 7,754,030

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.51642
Policy Entropy: 2.56663
Value Function Loss: 3.62105

Mean KL Divergence: 0.01646
SB3 Clip Fraction: 0.23765
Policy Update Magnitude: 0.81158
Value Function Update Magnitude: 0.90415

Collected Steps per Second: 20,037.82521
Overall Steps per Second: 9,989.64536

Timestep Collection Time: 2.49818
Timestep Consumption Time: 2.51281
PPO Batch Consumption Time: 0.14964
Total Iteration Time: 5.01099

Cumulative Model Updates: 1,856
Cumulative Timesteps: 7,804,088

Timesteps Collected: 50,058
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.91190
Policy Entropy: 2.53371
Value Function Loss: 3.46129

Mean KL Divergence: 0.01638
SB3 Clip Fraction: 0.23539
Policy Update Magnitude: 0.84259
Value Function Update Magnitude: 0.96745

Collected Steps per Second: 18,633.07655
Overall Steps per Second: 9,684.03033

Timestep Collection Time: 2.68361
Timestep Consumption Time: 2.47994
PPO Batch Consumption Time: 0.14807
Total Iteration Time: 5.16355

Cumulative Model Updates: 1,868
Cumulative Timesteps: 7,854,092

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 241.64349
Policy Entropy: 2.51030
Value Function Loss: 3.30685

Mean KL Divergence: 0.01592
SB3 Clip Fraction: 0.23183
Policy Update Magnitude: 0.78223
Value Function Update Magnitude: 0.93467

Collected Steps per Second: 19,198.94532
Overall Steps per Second: 9,853.45783

Timestep Collection Time: 2.60629
Timestep Consumption Time: 2.47193
PPO Batch Consumption Time: 0.14502
Total Iteration Time: 5.07822

Cumulative Model Updates: 1,880
Cumulative Timesteps: 7,904,130

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.89268
Policy Entropy: 2.48215
Value Function Loss: 3.23364

Mean KL Divergence: 0.01550
SB3 Clip Fraction: 0.22462
Policy Update Magnitude: 0.79351
Value Function Update Magnitude: 0.83569

Collected Steps per Second: 18,438.39457
Overall Steps per Second: 9,275.68017

Timestep Collection Time: 2.71434
Timestep Consumption Time: 2.68128
PPO Batch Consumption Time: 0.16202
Total Iteration Time: 5.39562

Cumulative Model Updates: 1,892
Cumulative Timesteps: 7,954,178

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 194.63536
Policy Entropy: 2.46383
Value Function Loss: 3.24886

Mean KL Divergence: 0.01493
SB3 Clip Fraction: 0.21866
Policy Update Magnitude: 0.80315
Value Function Update Magnitude: 0.85487

Collected Steps per Second: 19,062.47854
Overall Steps per Second: 9,165.32586

Timestep Collection Time: 2.62369
Timestep Consumption Time: 2.83318
PPO Batch Consumption Time: 0.16536
Total Iteration Time: 5.45687

Cumulative Model Updates: 1,904
Cumulative Timesteps: 8,004,192

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.15320
Policy Entropy: 2.43537
Value Function Loss: 3.19248

Mean KL Divergence: 0.01540
SB3 Clip Fraction: 0.22386
Policy Update Magnitude: 0.78695
Value Function Update Magnitude: 0.85018

Collected Steps per Second: 16,713.84392
Overall Steps per Second: 8,691.12710

Timestep Collection Time: 2.99297
Timestep Consumption Time: 2.76279
PPO Batch Consumption Time: 0.16128
Total Iteration Time: 5.75576

Cumulative Model Updates: 1,916
Cumulative Timesteps: 8,054,216

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.68314
Policy Entropy: 2.40498
Value Function Loss: 3.36016

Mean KL Divergence: 0.01513
SB3 Clip Fraction: 0.22110
Policy Update Magnitude: 0.76179
Value Function Update Magnitude: 0.82367

Collected Steps per Second: 18,440.93079
Overall Steps per Second: 9,437.39302

Timestep Collection Time: 2.71418
Timestep Consumption Time: 2.58940
PPO Batch Consumption Time: 0.15442
Total Iteration Time: 5.30358

Cumulative Model Updates: 1,928
Cumulative Timesteps: 8,104,268

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.99501
Policy Entropy: 2.36101
Value Function Loss: 3.45161

Mean KL Divergence: 0.01473
SB3 Clip Fraction: 0.21381
Policy Update Magnitude: 0.74635
Value Function Update Magnitude: 0.79958

Collected Steps per Second: 21,284.51418
Overall Steps per Second: 10,394.25426

Timestep Collection Time: 2.35110
Timestep Consumption Time: 2.46329
PPO Batch Consumption Time: 0.14870
Total Iteration Time: 4.81439

Cumulative Model Updates: 1,940
Cumulative Timesteps: 8,154,310

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 200.85426
Policy Entropy: 2.32599
Value Function Loss: 3.47352

Mean KL Divergence: 0.01511
SB3 Clip Fraction: 0.21839
Policy Update Magnitude: 0.74370
Value Function Update Magnitude: 0.79240

Collected Steps per Second: 16,775.49615
Overall Steps per Second: 8,529.04636

Timestep Collection Time: 2.98233
Timestep Consumption Time: 2.88351
PPO Batch Consumption Time: 0.15369
Total Iteration Time: 5.86584

Cumulative Model Updates: 1,952
Cumulative Timesteps: 8,204,340

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.48129
Policy Entropy: 2.28894
Value Function Loss: 3.34613

Mean KL Divergence: 0.01517
SB3 Clip Fraction: 0.21646
Policy Update Magnitude: 0.76720
Value Function Update Magnitude: 0.86175

Collected Steps per Second: 17,266.76864
Overall Steps per Second: 9,388.87570

Timestep Collection Time: 2.89852
Timestep Consumption Time: 2.43205
PPO Batch Consumption Time: 0.14754
Total Iteration Time: 5.33056

Cumulative Model Updates: 1,964
Cumulative Timesteps: 8,254,388

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 226.50036
Policy Entropy: 2.25417
Value Function Loss: 3.25276

Mean KL Divergence: 0.01526
SB3 Clip Fraction: 0.21676
Policy Update Magnitude: 0.74265
Value Function Update Magnitude: 0.98546

Collected Steps per Second: 18,842.21861
Overall Steps per Second: 9,260.15032

Timestep Collection Time: 2.65404
Timestep Consumption Time: 2.74630
PPO Batch Consumption Time: 0.16319
Total Iteration Time: 5.40034

Cumulative Model Updates: 1,976
Cumulative Timesteps: 8,304,396

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 200.08580
Policy Entropy: 2.22543
Value Function Loss: 3.25332

Mean KL Divergence: 0.01484
SB3 Clip Fraction: 0.21161
Policy Update Magnitude: 0.72835
Value Function Update Magnitude: 1.01277

Collected Steps per Second: 11,729.94649
Overall Steps per Second: 6,697.29980

Timestep Collection Time: 4.26532
Timestep Consumption Time: 3.20515
PPO Batch Consumption Time: 0.15324
Total Iteration Time: 7.47047

Cumulative Model Updates: 1,988
Cumulative Timesteps: 8,354,428

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.79495
Policy Entropy: 2.19548
Value Function Loss: 3.26048

Mean KL Divergence: 0.01462
SB3 Clip Fraction: 0.20828
Policy Update Magnitude: 0.70936
Value Function Update Magnitude: 0.89102

Collected Steps per Second: 17,097.25846
Overall Steps per Second: 8,850.55473

Timestep Collection Time: 2.92585
Timestep Consumption Time: 2.72623
PPO Batch Consumption Time: 0.15076
Total Iteration Time: 5.65208

Cumulative Model Updates: 2,000
Cumulative Timesteps: 8,404,452

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 236.33975
Policy Entropy: 2.16653
Value Function Loss: 3.22521

Mean KL Divergence: 0.01480
SB3 Clip Fraction: 0.20871
Policy Update Magnitude: 0.74933
Value Function Update Magnitude: 0.87409

Collected Steps per Second: 17,050.66056
Overall Steps per Second: 8,785.63276

Timestep Collection Time: 2.93291
Timestep Consumption Time: 2.75911
PPO Batch Consumption Time: 0.15404
Total Iteration Time: 5.69202

Cumulative Model Updates: 2,012
Cumulative Timesteps: 8,454,460

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.91401
Policy Entropy: 2.12327
Value Function Loss: 3.24323

Mean KL Divergence: 0.01634
SB3 Clip Fraction: 0.22349
Policy Update Magnitude: 0.76675
Value Function Update Magnitude: 0.81568

Collected Steps per Second: 18,413.35632
Overall Steps per Second: 9,504.33876

Timestep Collection Time: 2.71596
Timestep Consumption Time: 2.54584
PPO Batch Consumption Time: 0.14675
Total Iteration Time: 5.26181

Cumulative Model Updates: 2,024
Cumulative Timesteps: 8,504,470

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.24199
Policy Entropy: 2.08810
Value Function Loss: 3.17948

Mean KL Divergence: 0.01710
SB3 Clip Fraction: 0.23362
Policy Update Magnitude: 0.71408
Value Function Update Magnitude: 0.84806

Collected Steps per Second: 19,111.69926
Overall Steps per Second: 9,796.49633

Timestep Collection Time: 2.61651
Timestep Consumption Time: 2.48797
PPO Batch Consumption Time: 0.14790
Total Iteration Time: 5.10448

Cumulative Model Updates: 2,036
Cumulative Timesteps: 8,554,476

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 255.32157
Policy Entropy: 2.02964
Value Function Loss: 3.25583

Mean KL Divergence: 0.01513
SB3 Clip Fraction: 0.20687
Policy Update Magnitude: 0.68441
Value Function Update Magnitude: 0.96184

Collected Steps per Second: 18,761.80486
Overall Steps per Second: 9,700.22893

Timestep Collection Time: 2.66733
Timestep Consumption Time: 2.49172
PPO Batch Consumption Time: 0.14403
Total Iteration Time: 5.15905

Cumulative Model Updates: 2,048
Cumulative Timesteps: 8,604,520

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.07323
Policy Entropy: 1.99707
Value Function Loss: 3.17062

Mean KL Divergence: 0.01409
SB3 Clip Fraction: 0.19432
Policy Update Magnitude: 0.73472
Value Function Update Magnitude: 1.02906

Collected Steps per Second: 19,653.56798
Overall Steps per Second: 9,950.57965

Timestep Collection Time: 2.54712
Timestep Consumption Time: 2.48374
PPO Batch Consumption Time: 0.14586
Total Iteration Time: 5.03086

Cumulative Model Updates: 2,060
Cumulative Timesteps: 8,654,580

Timesteps Collected: 50,060
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.62186
Policy Entropy: 1.96327
Value Function Loss: 3.27799

Mean KL Divergence: 0.01476
SB3 Clip Fraction: 0.19969
Policy Update Magnitude: 0.74145
Value Function Update Magnitude: 0.92575

Collected Steps per Second: 20,808.56272
Overall Steps per Second: 10,436.31755

Timestep Collection Time: 2.40305
Timestep Consumption Time: 2.38830
PPO Batch Consumption Time: 0.14604
Total Iteration Time: 4.79135

Cumulative Model Updates: 2,072
Cumulative Timesteps: 8,704,584

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.40524
Policy Entropy: 1.95008
Value Function Loss: 3.29280

Mean KL Divergence: 0.01544
SB3 Clip Fraction: 0.20664
Policy Update Magnitude: 0.75115
Value Function Update Magnitude: 0.95311

Collected Steps per Second: 18,570.24421
Overall Steps per Second: 9,717.79751

Timestep Collection Time: 2.69259
Timestep Consumption Time: 2.45282
PPO Batch Consumption Time: 0.14502
Total Iteration Time: 5.14540

Cumulative Model Updates: 2,084
Cumulative Timesteps: 8,754,586

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 215.84853
Policy Entropy: 1.90843
Value Function Loss: 3.42082

Mean KL Divergence: 0.01610
SB3 Clip Fraction: 0.21222
Policy Update Magnitude: 0.72187
Value Function Update Magnitude: 0.93723

Collected Steps per Second: 19,997.12121
Overall Steps per Second: 10,208.49566

Timestep Collection Time: 2.50036
Timestep Consumption Time: 2.39752
PPO Batch Consumption Time: 0.14376
Total Iteration Time: 4.89788

Cumulative Model Updates: 2,096
Cumulative Timesteps: 8,804,586

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 222.30067
Policy Entropy: 1.86951
Value Function Loss: 3.40329

Mean KL Divergence: 0.01399
SB3 Clip Fraction: 0.18900
Policy Update Magnitude: 0.67057
Value Function Update Magnitude: 0.94901

Collected Steps per Second: 19,735.41979
Overall Steps per Second: 9,750.02148

Timestep Collection Time: 2.53524
Timestep Consumption Time: 2.59644
PPO Batch Consumption Time: 0.15125
Total Iteration Time: 5.13168

Cumulative Model Updates: 2,108
Cumulative Timesteps: 8,854,620

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.68862
Policy Entropy: 1.82444
Value Function Loss: 3.34393

Mean KL Divergence: 0.01295
SB3 Clip Fraction: 0.17769
Policy Update Magnitude: 0.68212
Value Function Update Magnitude: 0.97196

Collected Steps per Second: 18,840.03818
Overall Steps per Second: 9,785.07672

Timestep Collection Time: 2.65647
Timestep Consumption Time: 2.45826
PPO Batch Consumption Time: 0.14604
Total Iteration Time: 5.11473

Cumulative Model Updates: 2,120
Cumulative Timesteps: 8,904,668

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.13693
Policy Entropy: 1.78199
Value Function Loss: 3.24997

Mean KL Divergence: 0.01221
SB3 Clip Fraction: 0.16935
Policy Update Magnitude: 0.67666
Value Function Update Magnitude: 1.17059

Collected Steps per Second: 19,102.69704
Overall Steps per Second: 9,793.99798

Timestep Collection Time: 2.61932
Timestep Consumption Time: 2.48953
PPO Batch Consumption Time: 0.14767
Total Iteration Time: 5.10884

Cumulative Model Updates: 2,132
Cumulative Timesteps: 8,954,704

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.39293
Policy Entropy: 1.73850
Value Function Loss: 3.21128

Mean KL Divergence: 0.01208
SB3 Clip Fraction: 0.16491
Policy Update Magnitude: 0.71828
Value Function Update Magnitude: 1.18929

Collected Steps per Second: 19,303.42292
Overall Steps per Second: 9,858.20972

Timestep Collection Time: 2.59032
Timestep Consumption Time: 2.48180
PPO Batch Consumption Time: 0.14550
Total Iteration Time: 5.07212

Cumulative Model Updates: 2,144
Cumulative Timesteps: 9,004,706

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 190.07202
Policy Entropy: 1.69565
Value Function Loss: 3.26133

Mean KL Divergence: 0.01210
SB3 Clip Fraction: 0.16316
Policy Update Magnitude: 0.70875
Value Function Update Magnitude: 1.16247

Collected Steps per Second: 19,344.30088
Overall Steps per Second: 9,859.82245

Timestep Collection Time: 2.58681
Timestep Consumption Time: 2.48833
PPO Batch Consumption Time: 0.14660
Total Iteration Time: 5.07514

Cumulative Model Updates: 2,156
Cumulative Timesteps: 9,054,746

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.12687
Policy Entropy: 1.65365
Value Function Loss: 3.21168

Mean KL Divergence: 0.01223
SB3 Clip Fraction: 0.16311
Policy Update Magnitude: 0.68817
Value Function Update Magnitude: 1.01525

Collected Steps per Second: 19,075.50986
Overall Steps per Second: 9,780.80963

Timestep Collection Time: 2.62315
Timestep Consumption Time: 2.49278
PPO Batch Consumption Time: 0.14573
Total Iteration Time: 5.11594

Cumulative Model Updates: 2,168
Cumulative Timesteps: 9,104,784

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 204.35031
Policy Entropy: 1.61444
Value Function Loss: 3.08568

Mean KL Divergence: 0.01159
SB3 Clip Fraction: 0.15600
Policy Update Magnitude: 0.68508
Value Function Update Magnitude: 0.98058

Collected Steps per Second: 19,898.87840
Overall Steps per Second: 10,008.28201

Timestep Collection Time: 2.51351
Timestep Consumption Time: 2.48395
PPO Batch Consumption Time: 0.14836
Total Iteration Time: 4.99746

Cumulative Model Updates: 2,180
Cumulative Timesteps: 9,154,800

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 221.80435
Policy Entropy: 1.57423
Value Function Loss: 3.03722

Mean KL Divergence: 0.01138
SB3 Clip Fraction: 0.15164
Policy Update Magnitude: 0.69029
Value Function Update Magnitude: 0.96100

Collected Steps per Second: 22,249.55050
Overall Steps per Second: 10,600.62273

Timestep Collection Time: 2.24903
Timestep Consumption Time: 2.47144
PPO Batch Consumption Time: 0.15223
Total Iteration Time: 4.72048

Cumulative Model Updates: 2,192
Cumulative Timesteps: 9,204,840

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.30102
Policy Entropy: 1.53722
Value Function Loss: 2.92485

Mean KL Divergence: 0.01214
SB3 Clip Fraction: 0.15853
Policy Update Magnitude: 0.70281
Value Function Update Magnitude: 0.92639

Collected Steps per Second: 20,345.68074
Overall Steps per Second: 10,014.93360

Timestep Collection Time: 2.46126
Timestep Consumption Time: 2.53887
PPO Batch Consumption Time: 0.15217
Total Iteration Time: 5.00013

Cumulative Model Updates: 2,204
Cumulative Timesteps: 9,254,916

Timesteps Collected: 50,076
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.15322
Policy Entropy: 1.47774
Value Function Loss: 3.10473

Mean KL Divergence: 0.01131
SB3 Clip Fraction: 0.14772
Policy Update Magnitude: 0.68606
Value Function Update Magnitude: 0.99993

Collected Steps per Second: 19,110.55986
Overall Steps per Second: 9,912.65873

Timestep Collection Time: 2.61918
Timestep Consumption Time: 2.43032
PPO Batch Consumption Time: 0.14661
Total Iteration Time: 5.04950

Cumulative Model Updates: 2,216
Cumulative Timesteps: 9,304,970

Timesteps Collected: 50,054
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.07985
Policy Entropy: 1.43469
Value Function Loss: 3.03652

Mean KL Divergence: 0.01152
SB3 Clip Fraction: 0.14860
Policy Update Magnitude: 0.64310
Value Function Update Magnitude: 1.21743

Collected Steps per Second: 21,487.27665
Overall Steps per Second: 10,472.58991

Timestep Collection Time: 2.32761
Timestep Consumption Time: 2.44809
PPO Batch Consumption Time: 0.14535
Total Iteration Time: 4.77571

Cumulative Model Updates: 2,228
Cumulative Timesteps: 9,354,984

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.25618
Policy Entropy: 1.38023
Value Function Loss: 3.22531

Mean KL Divergence: 0.01078
SB3 Clip Fraction: 0.13896
Policy Update Magnitude: 0.65528
Value Function Update Magnitude: 1.01666

Collected Steps per Second: 20,799.99009
Overall Steps per Second: 10,370.18402

Timestep Collection Time: 2.40510
Timestep Consumption Time: 2.41893
PPO Batch Consumption Time: 0.14441
Total Iteration Time: 4.82402

Cumulative Model Updates: 2,240
Cumulative Timesteps: 9,405,010

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 184.44819
Policy Entropy: 1.35481
Value Function Loss: 3.02073

Mean KL Divergence: 0.01112
SB3 Clip Fraction: 0.14189
Policy Update Magnitude: 0.64569
Value Function Update Magnitude: 0.92999

Collected Steps per Second: 21,352.69338
Overall Steps per Second: 10,447.78639

Timestep Collection Time: 2.34181
Timestep Consumption Time: 2.44427
PPO Batch Consumption Time: 0.14965
Total Iteration Time: 4.78609

Cumulative Model Updates: 2,252
Cumulative Timesteps: 9,455,014

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 158.77188
Policy Entropy: 1.29108
Value Function Loss: 2.95265

Mean KL Divergence: 0.01052
SB3 Clip Fraction: 0.13414
Policy Update Magnitude: 0.62950
Value Function Update Magnitude: 0.91212

Collected Steps per Second: 19,378.55404
Overall Steps per Second: 9,902.91647

Timestep Collection Time: 2.58213
Timestep Consumption Time: 2.47072
PPO Batch Consumption Time: 0.14579
Total Iteration Time: 5.05285

Cumulative Model Updates: 2,264
Cumulative Timesteps: 9,505,052

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 176.98873
Policy Entropy: 1.24315
Value Function Loss: 2.93996

Mean KL Divergence: 0.00995
SB3 Clip Fraction: 0.12694
Policy Update Magnitude: 0.64625
Value Function Update Magnitude: 0.80927

Collected Steps per Second: 17,461.09126
Overall Steps per Second: 9,313.56101

Timestep Collection Time: 2.86362
Timestep Consumption Time: 2.50511
PPO Batch Consumption Time: 0.15302
Total Iteration Time: 5.36873

Cumulative Model Updates: 2,276
Cumulative Timesteps: 9,555,054

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 210.92878
Policy Entropy: 1.19634
Value Function Loss: 2.98024

Mean KL Divergence: 0.00968
SB3 Clip Fraction: 0.12273
Policy Update Magnitude: 0.61706
Value Function Update Magnitude: 0.81335

Collected Steps per Second: 23,461.17673
Overall Steps per Second: 10,899.27593

Timestep Collection Time: 2.13118
Timestep Consumption Time: 2.45628
PPO Batch Consumption Time: 0.14706
Total Iteration Time: 4.58746

Cumulative Model Updates: 2,288
Cumulative Timesteps: 9,605,054

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 233.56701
Policy Entropy: 1.15339
Value Function Loss: 2.94151

Mean KL Divergence: 0.00971
SB3 Clip Fraction: 0.12293
Policy Update Magnitude: 0.61651
Value Function Update Magnitude: 0.87761

Collected Steps per Second: 20,421.39214
Overall Steps per Second: 10,174.57558

Timestep Collection Time: 2.45243
Timestep Consumption Time: 2.46984
PPO Batch Consumption Time: 0.14603
Total Iteration Time: 4.92227

Cumulative Model Updates: 2,300
Cumulative Timesteps: 9,655,136

Timesteps Collected: 50,082
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 195.91177
Policy Entropy: 1.10179
Value Function Loss: 2.79098

Mean KL Divergence: 0.00967
SB3 Clip Fraction: 0.11790
Policy Update Magnitude: 0.63296
Value Function Update Magnitude: 0.94781

Collected Steps per Second: 20,534.51280
Overall Steps per Second: 10,285.01595

Timestep Collection Time: 2.43717
Timestep Consumption Time: 2.42875
PPO Batch Consumption Time: 0.14785
Total Iteration Time: 4.86591

Cumulative Model Updates: 2,312
Cumulative Timesteps: 9,705,182

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.71036
Policy Entropy: 1.07842
Value Function Loss: 2.69347

Mean KL Divergence: 0.00995
SB3 Clip Fraction: 0.11632
Policy Update Magnitude: 0.62906
Value Function Update Magnitude: 0.96424

Collected Steps per Second: 18,866.27849
Overall Steps per Second: 9,804.10174

Timestep Collection Time: 2.65246
Timestep Consumption Time: 2.45173
PPO Batch Consumption Time: 0.14774
Total Iteration Time: 5.10419

Cumulative Model Updates: 2,324
Cumulative Timesteps: 9,755,224

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.01606
Policy Entropy: 1.01585
Value Function Loss: 2.61446

Mean KL Divergence: 0.00969
SB3 Clip Fraction: 0.11362
Policy Update Magnitude: 0.60845
Value Function Update Magnitude: 0.94725

Collected Steps per Second: 20,720.75202
Overall Steps per Second: 10,153.66690

Timestep Collection Time: 2.41468
Timestep Consumption Time: 2.51300
PPO Batch Consumption Time: 0.15092
Total Iteration Time: 4.92768

Cumulative Model Updates: 2,336
Cumulative Timesteps: 9,805,258

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 204.52553
Policy Entropy: 0.97436
Value Function Loss: 2.65726

Mean KL Divergence: 0.00915
SB3 Clip Fraction: 0.10754
Policy Update Magnitude: 0.61223
Value Function Update Magnitude: 0.87486

Collected Steps per Second: 18,953.55191
Overall Steps per Second: 9,635.75239

Timestep Collection Time: 2.64119
Timestep Consumption Time: 2.55404
PPO Batch Consumption Time: 0.14900
Total Iteration Time: 5.19524

Cumulative Model Updates: 2,348
Cumulative Timesteps: 9,855,318

Timesteps Collected: 50,060
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 195.54074
Policy Entropy: 0.92930
Value Function Loss: 2.74816

Mean KL Divergence: 0.00941
SB3 Clip Fraction: 0.10638
Policy Update Magnitude: 0.62469
Value Function Update Magnitude: 0.85298

Collected Steps per Second: 18,298.34933
Overall Steps per Second: 9,644.30473

Timestep Collection Time: 2.73434
Timestep Consumption Time: 2.45359
PPO Batch Consumption Time: 0.14671
Total Iteration Time: 5.18793

Cumulative Model Updates: 2,360
Cumulative Timesteps: 9,905,352

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.73619
Policy Entropy: 0.89641
Value Function Loss: 2.79187

Mean KL Divergence: 0.00916
SB3 Clip Fraction: 0.10267
Policy Update Magnitude: 0.61964
Value Function Update Magnitude: 0.86901

Collected Steps per Second: 18,035.48764
Overall Steps per Second: 9,454.38949

Timestep Collection Time: 2.77475
Timestep Consumption Time: 2.51845
PPO Batch Consumption Time: 0.14592
Total Iteration Time: 5.29320

Cumulative Model Updates: 2,372
Cumulative Timesteps: 9,955,396

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 226.90305
Policy Entropy: 0.85304
Value Function Loss: 2.79050

Mean KL Divergence: 0.00870
SB3 Clip Fraction: 0.09763
Policy Update Magnitude: 0.64495
Value Function Update Magnitude: 0.86615

Collected Steps per Second: 20,606.25373
Overall Steps per Second: 10,190.67580

Timestep Collection Time: 2.42771
Timestep Consumption Time: 2.48129
PPO Batch Consumption Time: 0.15056
Total Iteration Time: 4.90900

Cumulative Model Updates: 2,384
Cumulative Timesteps: 10,005,422

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.45330
Policy Entropy: 0.83760
Value Function Loss: 2.75062

Mean KL Divergence: 0.00895
SB3 Clip Fraction: 0.09878
Policy Update Magnitude: 0.63326
Value Function Update Magnitude: 0.92494

Collected Steps per Second: 19,459.20675
Overall Steps per Second: 9,849.62324

Timestep Collection Time: 2.57102
Timestep Consumption Time: 2.50836
PPO Batch Consumption Time: 0.14989
Total Iteration Time: 5.07938

Cumulative Model Updates: 2,396
Cumulative Timesteps: 10,055,452

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.25549
Policy Entropy: 0.79478
Value Function Loss: 2.76578

Mean KL Divergence: 0.00908
SB3 Clip Fraction: 0.09678
Policy Update Magnitude: 0.60929
Value Function Update Magnitude: 0.88894

Collected Steps per Second: 20,846.00394
Overall Steps per Second: 10,122.89693

Timestep Collection Time: 2.40008
Timestep Consumption Time: 2.54238
PPO Batch Consumption Time: 0.15208
Total Iteration Time: 4.94246

Cumulative Model Updates: 2,408
Cumulative Timesteps: 10,105,484

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.32913
Policy Entropy: 0.73848
Value Function Loss: 2.90904

Mean KL Divergence: 0.00808
SB3 Clip Fraction: 0.08690
Policy Update Magnitude: 0.64213
Value Function Update Magnitude: 0.92530

Collected Steps per Second: 18,863.75004
Overall Steps per Second: 9,497.88168

Timestep Collection Time: 2.65313
Timestep Consumption Time: 2.61625
PPO Batch Consumption Time: 0.14727
Total Iteration Time: 5.26939

Cumulative Model Updates: 2,420
Cumulative Timesteps: 10,155,532

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.65903
Policy Entropy: 0.69118
Value Function Loss: 3.00184

Mean KL Divergence: 0.00794
SB3 Clip Fraction: 0.08200
Policy Update Magnitude: 0.65616
Value Function Update Magnitude: 0.98282

Collected Steps per Second: 18,235.99356
Overall Steps per Second: 9,165.65490

Timestep Collection Time: 2.74435
Timestep Consumption Time: 2.71581
PPO Batch Consumption Time: 0.15048
Total Iteration Time: 5.46017

Cumulative Model Updates: 2,432
Cumulative Timesteps: 10,205,578

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 195.19823
Policy Entropy: 0.66061
Value Function Loss: 3.04867

Mean KL Divergence: 0.00902
SB3 Clip Fraction: 0.08867
Policy Update Magnitude: 0.67723
Value Function Update Magnitude: 1.12305

Collected Steps per Second: 18,927.80563
Overall Steps per Second: 9,944.33735

Timestep Collection Time: 2.64373
Timestep Consumption Time: 2.38828
PPO Batch Consumption Time: 0.14862
Total Iteration Time: 5.03201

Cumulative Model Updates: 2,444
Cumulative Timesteps: 10,255,618

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.87248
Policy Entropy: 0.63168
Value Function Loss: 2.91542

Mean KL Divergence: 0.00905
SB3 Clip Fraction: 0.08332
Policy Update Magnitude: 0.68682
Value Function Update Magnitude: 1.13190

Collected Steps per Second: 18,610.11661
Overall Steps per Second: 9,608.70452

Timestep Collection Time: 2.68886
Timestep Consumption Time: 2.51892
PPO Batch Consumption Time: 0.15376
Total Iteration Time: 5.20778

Cumulative Model Updates: 2,456
Cumulative Timesteps: 10,305,658

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.44502
Policy Entropy: 0.60983
Value Function Loss: 2.80463

Mean KL Divergence: 0.00864
SB3 Clip Fraction: 0.07682
Policy Update Magnitude: 0.67992
Value Function Update Magnitude: 1.08275

Collected Steps per Second: 20,538.56555
Overall Steps per Second: 10,331.53221

Timestep Collection Time: 2.43620
Timestep Consumption Time: 2.40684
PPO Batch Consumption Time: 0.14391
Total Iteration Time: 4.84304

Cumulative Model Updates: 2,468
Cumulative Timesteps: 10,355,694

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.15477
Policy Entropy: 0.59550
Value Function Loss: 2.74354

Mean KL Divergence: 0.00932
SB3 Clip Fraction: 0.07962
Policy Update Magnitude: 0.69107
Value Function Update Magnitude: 0.98409

Collected Steps per Second: 20,372.36698
Overall Steps per Second: 10,192.55168

Timestep Collection Time: 2.45499
Timestep Consumption Time: 2.45192
PPO Batch Consumption Time: 0.14523
Total Iteration Time: 4.90692

Cumulative Model Updates: 2,480
Cumulative Timesteps: 10,405,708

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.15396
Policy Entropy: 0.56880
Value Function Loss: 2.80053

Mean KL Divergence: 0.00850
SB3 Clip Fraction: 0.07253
Policy Update Magnitude: 0.77421
Value Function Update Magnitude: 0.93664

Collected Steps per Second: 23,952.22197
Overall Steps per Second: 10,861.29019

Timestep Collection Time: 2.08841
Timestep Consumption Time: 2.51712
PPO Batch Consumption Time: 0.15161
Total Iteration Time: 4.60553

Cumulative Model Updates: 2,492
Cumulative Timesteps: 10,455,730

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.31137
Policy Entropy: 0.57568
Value Function Loss: 2.95557

Mean KL Divergence: 0.00856
SB3 Clip Fraction: 0.07583
Policy Update Magnitude: 0.82556
Value Function Update Magnitude: 1.02151

Collected Steps per Second: 21,742.88502
Overall Steps per Second: 10,775.02794

Timestep Collection Time: 2.30153
Timestep Consumption Time: 2.34272
PPO Batch Consumption Time: 0.14426
Total Iteration Time: 4.64426

Cumulative Model Updates: 2,504
Cumulative Timesteps: 10,505,772

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 235.30342
Policy Entropy: 0.55916
Value Function Loss: 2.85490

Mean KL Divergence: 0.00839
SB3 Clip Fraction: 0.07469
Policy Update Magnitude: 0.84130
Value Function Update Magnitude: 1.00059

Collected Steps per Second: 24,154.43537
Overall Steps per Second: 11,047.52299

Timestep Collection Time: 2.07043
Timestep Consumption Time: 2.45638
PPO Batch Consumption Time: 0.14728
Total Iteration Time: 4.52681

Cumulative Model Updates: 2,516
Cumulative Timesteps: 10,555,782

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 198.35086
Policy Entropy: 0.56011
Value Function Loss: 2.82821

Mean KL Divergence: 0.00857
SB3 Clip Fraction: 0.07557
Policy Update Magnitude: 0.83578
Value Function Update Magnitude: 0.96818

Collected Steps per Second: 19,090.16083
Overall Steps per Second: 9,714.50260

Timestep Collection Time: 2.62062
Timestep Consumption Time: 2.52921
PPO Batch Consumption Time: 0.14923
Total Iteration Time: 5.14983

Cumulative Model Updates: 2,528
Cumulative Timesteps: 10,605,810

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 197.21732
Policy Entropy: 0.55061
Value Function Loss: 2.72321

Mean KL Divergence: 0.00834
SB3 Clip Fraction: 0.07343
Policy Update Magnitude: 0.80849
Value Function Update Magnitude: 0.98737

Collected Steps per Second: 18,113.82536
Overall Steps per Second: 9,544.60537

Timestep Collection Time: 2.76275
Timestep Consumption Time: 2.48042
PPO Batch Consumption Time: 0.14641
Total Iteration Time: 5.24317

Cumulative Model Updates: 2,540
Cumulative Timesteps: 10,655,854

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.41626
Policy Entropy: 0.55179
Value Function Loss: 2.81518

Mean KL Divergence: 0.01073
SB3 Clip Fraction: 0.08948
Policy Update Magnitude: 0.69224
Value Function Update Magnitude: 0.92504

Collected Steps per Second: 20,211.75333
Overall Steps per Second: 10,134.99667

Timestep Collection Time: 2.47618
Timestep Consumption Time: 2.46195
PPO Batch Consumption Time: 0.14524
Total Iteration Time: 4.93814

Cumulative Model Updates: 2,552
Cumulative Timesteps: 10,705,902

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.79966
Policy Entropy: 0.52100
Value Function Loss: 2.85135

Mean KL Divergence: 0.01084
SB3 Clip Fraction: 0.08761
Policy Update Magnitude: 0.67209
Value Function Update Magnitude: 0.93086

Collected Steps per Second: 18,758.03243
Overall Steps per Second: 9,763.41093

Timestep Collection Time: 2.66926
Timestep Consumption Time: 2.45907
PPO Batch Consumption Time: 0.14657
Total Iteration Time: 5.12833

Cumulative Model Updates: 2,564
Cumulative Timesteps: 10,755,972

Timesteps Collected: 50,070
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.35172
Policy Entropy: 0.48064
Value Function Loss: 2.79118

Mean KL Divergence: 0.00788
SB3 Clip Fraction: 0.06966
Policy Update Magnitude: 0.73611
Value Function Update Magnitude: 0.99799

Collected Steps per Second: 17,022.99732
Overall Steps per Second: 9,274.75589

Timestep Collection Time: 2.93791
Timestep Consumption Time: 2.45436
PPO Batch Consumption Time: 0.14569
Total Iteration Time: 5.39227

Cumulative Model Updates: 2,576
Cumulative Timesteps: 10,805,984

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.85249
Policy Entropy: 0.46411
Value Function Loss: 2.76415

Mean KL Divergence: 0.00835
SB3 Clip Fraction: 0.07024
Policy Update Magnitude: 0.77511
Value Function Update Magnitude: 0.98059

Collected Steps per Second: 22,431.40698
Overall Steps per Second: 10,823.41544

Timestep Collection Time: 2.22937
Timestep Consumption Time: 2.39098
PPO Batch Consumption Time: 0.14502
Total Iteration Time: 4.62035

Cumulative Model Updates: 2,588
Cumulative Timesteps: 10,855,992

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.98267
Policy Entropy: 0.47848
Value Function Loss: 2.70105

Mean KL Divergence: 0.00840
SB3 Clip Fraction: 0.07063
Policy Update Magnitude: 0.78170
Value Function Update Magnitude: 0.92622

Collected Steps per Second: 21,241.69920
Overall Steps per Second: 10,467.19450

Timestep Collection Time: 2.35480
Timestep Consumption Time: 2.42394
PPO Batch Consumption Time: 0.14538
Total Iteration Time: 4.77874

Cumulative Model Updates: 2,600
Cumulative Timesteps: 10,906,012

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 185.07527
Policy Entropy: 0.46847
Value Function Loss: 2.86588

Mean KL Divergence: 0.00962
SB3 Clip Fraction: 0.07703
Policy Update Magnitude: 0.76124
Value Function Update Magnitude: 0.91067

Collected Steps per Second: 21,262.61389
Overall Steps per Second: 10,395.18123

Timestep Collection Time: 2.35305
Timestep Consumption Time: 2.45995
PPO Batch Consumption Time: 0.14992
Total Iteration Time: 4.81300

Cumulative Model Updates: 2,612
Cumulative Timesteps: 10,956,044

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.19446
Policy Entropy: 0.46379
Value Function Loss: 2.86911

Mean KL Divergence: 0.01044
SB3 Clip Fraction: 0.08086
Policy Update Magnitude: 0.75141
Value Function Update Magnitude: 0.97417

Collected Steps per Second: 22,429.32010
Overall Steps per Second: 10,715.32538

Timestep Collection Time: 2.23110
Timestep Consumption Time: 2.43904
PPO Batch Consumption Time: 0.14608
Total Iteration Time: 4.67013

Cumulative Model Updates: 2,624
Cumulative Timesteps: 11,006,086

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.29483
Policy Entropy: 0.48497
Value Function Loss: 2.86404

Mean KL Divergence: 0.01084
SB3 Clip Fraction: 0.08433
Policy Update Magnitude: 0.71974
Value Function Update Magnitude: 1.02223

Collected Steps per Second: 21,210.92271
Overall Steps per Second: 10,359.90681

Timestep Collection Time: 2.35775
Timestep Consumption Time: 2.46952
PPO Batch Consumption Time: 0.14447
Total Iteration Time: 4.82726

Cumulative Model Updates: 2,636
Cumulative Timesteps: 11,056,096

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.24568
Policy Entropy: 0.49489
Value Function Loss: 2.82157

Mean KL Divergence: 0.00886
SB3 Clip Fraction: 0.07611
Policy Update Magnitude: 0.84868
Value Function Update Magnitude: 1.04546

Collected Steps per Second: 17,780.36545
Overall Steps per Second: 9,629.32509

Timestep Collection Time: 2.81265
Timestep Consumption Time: 2.38086
PPO Batch Consumption Time: 0.14400
Total Iteration Time: 5.19351

Cumulative Model Updates: 2,648
Cumulative Timesteps: 11,106,106

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.17174
Policy Entropy: 0.50140
Value Function Loss: 2.91745

Mean KL Divergence: 0.00917
SB3 Clip Fraction: 0.08033
Policy Update Magnitude: 0.86678
Value Function Update Magnitude: 1.10239

Collected Steps per Second: 20,117.16877
Overall Steps per Second: 10,216.75317

Timestep Collection Time: 2.48653
Timestep Consumption Time: 2.40954
PPO Batch Consumption Time: 0.14335
Total Iteration Time: 4.89608

Cumulative Model Updates: 2,660
Cumulative Timesteps: 11,156,128

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 187.41232
Policy Entropy: 0.48501
Value Function Loss: 2.83516

Mean KL Divergence: 0.00940
SB3 Clip Fraction: 0.08017
Policy Update Magnitude: 0.85468
Value Function Update Magnitude: 1.13721

Collected Steps per Second: 21,709.80867
Overall Steps per Second: 10,565.46463

Timestep Collection Time: 2.30412
Timestep Consumption Time: 2.43036
PPO Batch Consumption Time: 0.14490
Total Iteration Time: 4.73448

Cumulative Model Updates: 2,672
Cumulative Timesteps: 11,206,150

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 207.42442
Policy Entropy: 0.44958
Value Function Loss: 2.80607

Mean KL Divergence: 0.00869
SB3 Clip Fraction: 0.07492
Policy Update Magnitude: 0.82286
Value Function Update Magnitude: 1.00839

Collected Steps per Second: 19,830.26678
Overall Steps per Second: 10,096.07929

Timestep Collection Time: 2.52221
Timestep Consumption Time: 2.43180
PPO Batch Consumption Time: 0.14462
Total Iteration Time: 4.95400

Cumulative Model Updates: 2,684
Cumulative Timesteps: 11,256,166

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.94423
Policy Entropy: 0.43587
Value Function Loss: 2.65270

Mean KL Divergence: 0.00897
SB3 Clip Fraction: 0.07246
Policy Update Magnitude: 0.80909
Value Function Update Magnitude: 0.94562

Collected Steps per Second: 19,434.97632
Overall Steps per Second: 9,727.81438

Timestep Collection Time: 2.57278
Timestep Consumption Time: 2.56732
PPO Batch Consumption Time: 0.15209
Total Iteration Time: 5.14011

Cumulative Model Updates: 2,696
Cumulative Timesteps: 11,306,168

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 228.64755
Policy Entropy: 0.43572
Value Function Loss: 2.84518

Mean KL Divergence: 0.00926
SB3 Clip Fraction: 0.07341
Policy Update Magnitude: 0.83689
Value Function Update Magnitude: 0.89753

Collected Steps per Second: 18,724.21109
Overall Steps per Second: 9,582.19923

Timestep Collection Time: 2.67290
Timestep Consumption Time: 2.55012
PPO Batch Consumption Time: 0.15106
Total Iteration Time: 5.22302

Cumulative Model Updates: 2,708
Cumulative Timesteps: 11,356,216

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 252.24586
Policy Entropy: 0.44445
Value Function Loss: 2.81938

Mean KL Divergence: 0.01161
SB3 Clip Fraction: 0.08541
Policy Update Magnitude: 0.79570
Value Function Update Magnitude: 0.93786

Collected Steps per Second: 19,044.80349
Overall Steps per Second: 9,744.87145

Timestep Collection Time: 2.62623
Timestep Consumption Time: 2.50632
PPO Batch Consumption Time: 0.14806
Total Iteration Time: 5.13255

Cumulative Model Updates: 2,720
Cumulative Timesteps: 11,406,232

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 252.14787
Policy Entropy: 0.44963
Value Function Loss: 2.95585

Mean KL Divergence: 0.01352
SB3 Clip Fraction: 0.09610
Policy Update Magnitude: 0.73015
Value Function Update Magnitude: 0.98771

Collected Steps per Second: 17,755.76289
Overall Steps per Second: 9,363.06692

Timestep Collection Time: 2.81644
Timestep Consumption Time: 2.52455
PPO Batch Consumption Time: 0.15860
Total Iteration Time: 5.34099

Cumulative Model Updates: 2,732
Cumulative Timesteps: 11,456,240

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.87587
Policy Entropy: 0.46033
Value Function Loss: 2.88639

Mean KL Divergence: 0.01226
SB3 Clip Fraction: 0.09133
Policy Update Magnitude: 0.81718
Value Function Update Magnitude: 0.97758

Collected Steps per Second: 19,951.71420
Overall Steps per Second: 9,456.60757

Timestep Collection Time: 2.50645
Timestep Consumption Time: 2.78170
PPO Batch Consumption Time: 0.15283
Total Iteration Time: 5.28815

Cumulative Model Updates: 2,744
Cumulative Timesteps: 11,506,248

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.13425
Policy Entropy: 0.46086
Value Function Loss: 3.01148

Mean KL Divergence: 0.01158
SB3 Clip Fraction: 0.08977
Policy Update Magnitude: 0.87046
Value Function Update Magnitude: 0.94706

Collected Steps per Second: 18,557.48257
Overall Steps per Second: 9,617.77372

Timestep Collection Time: 2.69703
Timestep Consumption Time: 2.50688
PPO Batch Consumption Time: 0.15036
Total Iteration Time: 5.20391

Cumulative Model Updates: 2,756
Cumulative Timesteps: 11,556,298

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 228.63970
Policy Entropy: 0.45236
Value Function Loss: 3.05030

Mean KL Divergence: 0.01112
SB3 Clip Fraction: 0.08601
Policy Update Magnitude: 0.86827
Value Function Update Magnitude: 0.97696

Collected Steps per Second: 20,622.91441
Overall Steps per Second: 10,315.76472

Timestep Collection Time: 2.42585
Timestep Consumption Time: 2.42382
PPO Batch Consumption Time: 0.14453
Total Iteration Time: 4.84966

Cumulative Model Updates: 2,768
Cumulative Timesteps: 11,606,326

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 255.64807
Policy Entropy: 0.44592
Value Function Loss: 3.11701

Mean KL Divergence: 0.01113
SB3 Clip Fraction: 0.08651
Policy Update Magnitude: 0.81339
Value Function Update Magnitude: 0.97712

Collected Steps per Second: 20,635.76773
Overall Steps per Second: 9,931.81593

Timestep Collection Time: 2.42472
Timestep Consumption Time: 2.61323
PPO Batch Consumption Time: 0.14520
Total Iteration Time: 5.03795

Cumulative Model Updates: 2,780
Cumulative Timesteps: 11,656,362

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 227.95106
Policy Entropy: 0.45571
Value Function Loss: 3.07287

Mean KL Divergence: 0.01386
SB3 Clip Fraction: 0.09988
Policy Update Magnitude: 0.74983
Value Function Update Magnitude: 1.00189

Collected Steps per Second: 20,919.83381
Overall Steps per Second: 10,445.96356

Timestep Collection Time: 2.39170
Timestep Consumption Time: 2.39809
PPO Batch Consumption Time: 0.14591
Total Iteration Time: 4.78979

Cumulative Model Updates: 2,792
Cumulative Timesteps: 11,706,396

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 226.98591
Policy Entropy: 0.46300
Value Function Loss: 3.07834

Mean KL Divergence: 0.01502
SB3 Clip Fraction: 0.10447
Policy Update Magnitude: 0.73346
Value Function Update Magnitude: 0.97965

Collected Steps per Second: 20,835.29098
Overall Steps per Second: 9,965.24064

Timestep Collection Time: 2.39977
Timestep Consumption Time: 2.61767
PPO Batch Consumption Time: 0.15563
Total Iteration Time: 5.01744

Cumulative Model Updates: 2,804
Cumulative Timesteps: 11,756,396

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 239.05136
Policy Entropy: 0.49180
Value Function Loss: 3.02176

Mean KL Divergence: 0.01690
SB3 Clip Fraction: 0.11551
Policy Update Magnitude: 0.72081
Value Function Update Magnitude: 0.94677

Collected Steps per Second: 19,457.66487
Overall Steps per Second: 10,112.64811

Timestep Collection Time: 2.56989
Timestep Consumption Time: 2.37481
PPO Batch Consumption Time: 0.14548
Total Iteration Time: 4.94470

Cumulative Model Updates: 2,816
Cumulative Timesteps: 11,806,400

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 258.71651
Policy Entropy: 0.49298
Value Function Loss: 3.13931

Mean KL Divergence: 0.01650
SB3 Clip Fraction: 0.11570
Policy Update Magnitude: 0.72363
Value Function Update Magnitude: 0.93991

Collected Steps per Second: 19,565.92174
Overall Steps per Second: 9,639.32608

Timestep Collection Time: 2.55679
Timestep Consumption Time: 2.63299
PPO Batch Consumption Time: 0.15282
Total Iteration Time: 5.18978

Cumulative Model Updates: 2,828
Cumulative Timesteps: 11,856,426

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.52734
Policy Entropy: 0.49289
Value Function Loss: 3.16310

Mean KL Divergence: 0.01554
SB3 Clip Fraction: 0.11201
Policy Update Magnitude: 0.74030
Value Function Update Magnitude: 0.93744

Collected Steps per Second: 18,865.89029
Overall Steps per Second: 9,672.44333

Timestep Collection Time: 2.65177
Timestep Consumption Time: 2.52045
PPO Batch Consumption Time: 0.15183
Total Iteration Time: 5.17222

Cumulative Model Updates: 2,840
Cumulative Timesteps: 11,906,454

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 219.79427
Policy Entropy: 0.51396
Value Function Loss: 3.29422

Mean KL Divergence: 0.01614
SB3 Clip Fraction: 0.11692
Policy Update Magnitude: 0.68769
Value Function Update Magnitude: 0.97606

Collected Steps per Second: 20,146.39823
Overall Steps per Second: 10,145.06324

Timestep Collection Time: 2.48183
Timestep Consumption Time: 2.44667
PPO Batch Consumption Time: 0.14780
Total Iteration Time: 4.92851

Cumulative Model Updates: 2,852
Cumulative Timesteps: 11,956,454

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 228.69047
Policy Entropy: 0.53250
Value Function Loss: 3.18168

Mean KL Divergence: 0.01516
SB3 Clip Fraction: 0.11642
Policy Update Magnitude: 0.72260
Value Function Update Magnitude: 0.98967

Collected Steps per Second: 19,876.45762
Overall Steps per Second: 9,943.50809

Timestep Collection Time: 2.51795
Timestep Consumption Time: 2.51528
PPO Batch Consumption Time: 0.15047
Total Iteration Time: 5.03323

Cumulative Model Updates: 2,864
Cumulative Timesteps: 12,006,502

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 251.25520
Policy Entropy: 0.55224
Value Function Loss: 3.13877

Mean KL Divergence: 0.01610
SB3 Clip Fraction: 0.12452
Policy Update Magnitude: 0.75934
Value Function Update Magnitude: 0.96486

Collected Steps per Second: 20,200.13303
Overall Steps per Second: 10,276.74022

Timestep Collection Time: 2.47711
Timestep Consumption Time: 2.39194
PPO Batch Consumption Time: 0.14809
Total Iteration Time: 4.86905

Cumulative Model Updates: 2,876
Cumulative Timesteps: 12,056,540

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 255.14965
Policy Entropy: 0.55615
Value Function Loss: 3.12162

Mean KL Divergence: 0.01857
SB3 Clip Fraction: 0.13449
Policy Update Magnitude: 0.85501
Value Function Update Magnitude: 1.02536

Collected Steps per Second: 21,947.35653
Overall Steps per Second: 10,514.70611

Timestep Collection Time: 2.27955
Timestep Consumption Time: 2.47855
PPO Batch Consumption Time: 0.14640
Total Iteration Time: 4.75810

Cumulative Model Updates: 2,888
Cumulative Timesteps: 12,106,570

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 227.30775
Policy Entropy: 0.53406
Value Function Loss: 3.01411

Mean KL Divergence: 0.02018
SB3 Clip Fraction: 0.13498
Policy Update Magnitude: 0.92920
Value Function Update Magnitude: 1.05788

Collected Steps per Second: 22,032.74057
Overall Steps per Second: 10,646.44750

Timestep Collection Time: 2.27008
Timestep Consumption Time: 2.42783
PPO Batch Consumption Time: 0.14674
Total Iteration Time: 4.69791

Cumulative Model Updates: 2,900
Cumulative Timesteps: 12,156,586

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 225.92931
Policy Entropy: 0.54868
Value Function Loss: 3.05607

Mean KL Divergence: 0.02105
SB3 Clip Fraction: 0.13988
Policy Update Magnitude: 0.74909
Value Function Update Magnitude: 1.00960

Collected Steps per Second: 21,794.43425
Overall Steps per Second: 10,490.36022

Timestep Collection Time: 2.29692
Timestep Consumption Time: 2.47508
PPO Batch Consumption Time: 0.14502
Total Iteration Time: 4.77200

Cumulative Model Updates: 2,912
Cumulative Timesteps: 12,206,646

Timesteps Collected: 50,060
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.30067
Policy Entropy: 0.56932
Value Function Loss: 3.02473

Mean KL Divergence: 0.02261
SB3 Clip Fraction: 0.14489
Policy Update Magnitude: 0.75182
Value Function Update Magnitude: 1.00151

Collected Steps per Second: 20,936.34158
Overall Steps per Second: 10,423.68990

Timestep Collection Time: 2.38876
Timestep Consumption Time: 2.40915
PPO Batch Consumption Time: 0.14644
Total Iteration Time: 4.79792

Cumulative Model Updates: 2,924
Cumulative Timesteps: 12,256,658

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 243.75087
Policy Entropy: 0.55262
Value Function Loss: 3.07959

Mean KL Divergence: 0.01876
SB3 Clip Fraction: 0.13660
Policy Update Magnitude: 0.81010
Value Function Update Magnitude: 1.07181

Collected Steps per Second: 20,580.40648
Overall Steps per Second: 10,486.68925

Timestep Collection Time: 2.42979
Timestep Consumption Time: 2.33873
PPO Batch Consumption Time: 0.14448
Total Iteration Time: 4.76852

Cumulative Model Updates: 2,936
Cumulative Timesteps: 12,306,664

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.06228
Policy Entropy: 0.54388
Value Function Loss: 3.08274

Mean KL Divergence: 0.01774
SB3 Clip Fraction: 0.13055
Policy Update Magnitude: 1.00053
Value Function Update Magnitude: 1.01991

Collected Steps per Second: 21,383.93237
Overall Steps per Second: 10,293.14075

Timestep Collection Time: 2.33961
Timestep Consumption Time: 2.52091
PPO Batch Consumption Time: 0.15458
Total Iteration Time: 4.86052

Cumulative Model Updates: 2,948
Cumulative Timesteps: 12,356,694

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.20495
Policy Entropy: 0.54998
Value Function Loss: 3.12053

Mean KL Divergence: 0.02214
SB3 Clip Fraction: 0.14177
Policy Update Magnitude: 0.87852
Value Function Update Magnitude: 0.98362

Collected Steps per Second: 20,345.05132
Overall Steps per Second: 9,956.75272

Timestep Collection Time: 2.45780
Timestep Consumption Time: 2.56432
PPO Batch Consumption Time: 0.14546
Total Iteration Time: 5.02212

Cumulative Model Updates: 2,960
Cumulative Timesteps: 12,406,698

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.89309
Policy Entropy: 0.56061
Value Function Loss: 3.06590

Mean KL Divergence: 0.02315
SB3 Clip Fraction: 0.14955
Policy Update Magnitude: 0.90923
Value Function Update Magnitude: 0.98323

Collected Steps per Second: 22,501.71003
Overall Steps per Second: 10,473.21989

Timestep Collection Time: 2.22276
Timestep Consumption Time: 2.55284
PPO Batch Consumption Time: 0.15377
Total Iteration Time: 4.77561

Cumulative Model Updates: 2,972
Cumulative Timesteps: 12,456,714

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 245.59452
Policy Entropy: 0.58933
Value Function Loss: 3.16819

Mean KL Divergence: 0.01947
SB3 Clip Fraction: 0.13918
Policy Update Magnitude: 0.95029
Value Function Update Magnitude: 1.00898

Collected Steps per Second: 19,733.66842
Overall Steps per Second: 9,952.66172

Timestep Collection Time: 2.53607
Timestep Consumption Time: 2.49233
PPO Batch Consumption Time: 0.15085
Total Iteration Time: 5.02840

Cumulative Model Updates: 2,984
Cumulative Timesteps: 12,506,760

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.00333
Policy Entropy: 0.59567
Value Function Loss: 3.08297

Mean KL Divergence: 0.02006
SB3 Clip Fraction: 0.14434
Policy Update Magnitude: 0.97675
Value Function Update Magnitude: 1.00883

Collected Steps per Second: 20,474.53877
Overall Steps per Second: 10,263.75296

Timestep Collection Time: 2.44235
Timestep Consumption Time: 2.42975
PPO Batch Consumption Time: 0.15133
Total Iteration Time: 4.87210

Cumulative Model Updates: 2,996
Cumulative Timesteps: 12,556,766

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 226.75958
Policy Entropy: 0.58321
Value Function Loss: 3.08845

Mean KL Divergence: 0.02134
SB3 Clip Fraction: 0.15059
Policy Update Magnitude: 0.93502
Value Function Update Magnitude: 0.98513

Collected Steps per Second: 20,637.97013
Overall Steps per Second: 9,825.11276

Timestep Collection Time: 2.42398
Timestep Consumption Time: 2.66767
PPO Batch Consumption Time: 0.16105
Total Iteration Time: 5.09165

Cumulative Model Updates: 3,008
Cumulative Timesteps: 12,606,792

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 245.34168
Policy Entropy: 0.57407
Value Function Loss: 2.94442

Mean KL Divergence: 0.01858
SB3 Clip Fraction: 0.13816
Policy Update Magnitude: 0.95300
Value Function Update Magnitude: 0.94759

Collected Steps per Second: 19,871.58202
Overall Steps per Second: 10,122.13178

Timestep Collection Time: 2.51706
Timestep Consumption Time: 2.42439
PPO Batch Consumption Time: 0.14470
Total Iteration Time: 4.94145

Cumulative Model Updates: 3,020
Cumulative Timesteps: 12,656,810

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 225.02803
Policy Entropy: 0.55708
Value Function Loss: 3.04301

Mean KL Divergence: 0.01723
SB3 Clip Fraction: 0.13221
Policy Update Magnitude: 0.98209
Value Function Update Magnitude: 0.98286

Collected Steps per Second: 22,083.95862
Overall Steps per Second: 10,669.67431

Timestep Collection Time: 2.26418
Timestep Consumption Time: 2.42219
PPO Batch Consumption Time: 0.14445
Total Iteration Time: 4.68637

Cumulative Model Updates: 3,032
Cumulative Timesteps: 12,706,812

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 223.43850
Policy Entropy: 0.57550
Value Function Loss: 3.03372

Mean KL Divergence: 0.01803
SB3 Clip Fraction: 0.13527
Policy Update Magnitude: 0.96844
Value Function Update Magnitude: 1.01651

Collected Steps per Second: 18,412.10932
Overall Steps per Second: 9,537.20583

Timestep Collection Time: 2.71865
Timestep Consumption Time: 2.52985
PPO Batch Consumption Time: 0.14589
Total Iteration Time: 5.24850

Cumulative Model Updates: 3,044
Cumulative Timesteps: 12,756,868

Timesteps Collected: 50,056
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 225.57046
Policy Entropy: 0.56113
Value Function Loss: 3.11350

Mean KL Divergence: 0.02454
SB3 Clip Fraction: 0.15834
Policy Update Magnitude: 0.84945
Value Function Update Magnitude: 1.03414

Collected Steps per Second: 19,721.55249
Overall Steps per Second: 10,024.02842

Timestep Collection Time: 2.53733
Timestep Consumption Time: 2.45468
PPO Batch Consumption Time: 0.14829
Total Iteration Time: 4.99200

Cumulative Model Updates: 3,056
Cumulative Timesteps: 12,806,908

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.22553
Policy Entropy: 0.54818
Value Function Loss: 3.11056

Mean KL Divergence: 0.02212
SB3 Clip Fraction: 0.14985
Policy Update Magnitude: 0.70881
Value Function Update Magnitude: 1.10479

Collected Steps per Second: 18,296.05502
Overall Steps per Second: 9,662.30845

Timestep Collection Time: 2.73316
Timestep Consumption Time: 2.44221
PPO Batch Consumption Time: 0.14669
Total Iteration Time: 5.17537

Cumulative Model Updates: 3,068
Cumulative Timesteps: 12,856,914

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.25377
Policy Entropy: 0.50872
Value Function Loss: 3.16223

Mean KL Divergence: 0.01662
SB3 Clip Fraction: 0.12825
Policy Update Magnitude: 0.66605
Value Function Update Magnitude: 1.10043

Collected Steps per Second: 19,049.63512
Overall Steps per Second: 9,802.80366

Timestep Collection Time: 2.62651
Timestep Consumption Time: 2.47754
PPO Batch Consumption Time: 0.15067
Total Iteration Time: 5.10405

Cumulative Model Updates: 3,080
Cumulative Timesteps: 12,906,948

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 256.80717
Policy Entropy: 0.49292
Value Function Loss: 3.13413

Mean KL Divergence: 0.01662
SB3 Clip Fraction: 0.12638
Policy Update Magnitude: 0.69184
Value Function Update Magnitude: 1.12118

Collected Steps per Second: 19,466.45239
Overall Steps per Second: 9,930.20951

Timestep Collection Time: 2.57047
Timestep Consumption Time: 2.46849
PPO Batch Consumption Time: 0.14573
Total Iteration Time: 5.03897

Cumulative Model Updates: 3,092
Cumulative Timesteps: 12,956,986

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.14004
Policy Entropy: 0.46459
Value Function Loss: 3.05104

Mean KL Divergence: 0.01499
SB3 Clip Fraction: 0.11648
Policy Update Magnitude: 0.65790
Value Function Update Magnitude: 1.09156

Collected Steps per Second: 20,491.78313
Overall Steps per Second: 10,090.96517

Timestep Collection Time: 2.44059
Timestep Consumption Time: 2.51553
PPO Batch Consumption Time: 0.14869
Total Iteration Time: 4.95612

Cumulative Model Updates: 3,104
Cumulative Timesteps: 13,006,998

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 236.05297
Policy Entropy: 0.44180
Value Function Loss: 3.07284

Mean KL Divergence: 0.01421
SB3 Clip Fraction: 0.11142
Policy Update Magnitude: 0.67666
Value Function Update Magnitude: 1.10986

Collected Steps per Second: 21,896.32980
Overall Steps per Second: 10,489.33195

Timestep Collection Time: 2.28531
Timestep Consumption Time: 2.48525
PPO Batch Consumption Time: 0.14867
Total Iteration Time: 4.77056

Cumulative Model Updates: 3,116
Cumulative Timesteps: 13,057,038

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 249.88578
Policy Entropy: 0.41212
Value Function Loss: 3.06904

Mean KL Divergence: 0.01349
SB3 Clip Fraction: 0.10344
Policy Update Magnitude: 0.72717
Value Function Update Magnitude: 1.16585

Collected Steps per Second: 18,387.61907
Overall Steps per Second: 9,458.25091

Timestep Collection Time: 2.72063
Timestep Consumption Time: 2.56850
PPO Batch Consumption Time: 0.14882
Total Iteration Time: 5.28914

Cumulative Model Updates: 3,128
Cumulative Timesteps: 13,107,064

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 233.69739
Policy Entropy: 0.38636
Value Function Loss: 3.10903

Mean KL Divergence: 0.01382
SB3 Clip Fraction: 0.10129
Policy Update Magnitude: 0.71100
Value Function Update Magnitude: 1.10678

Collected Steps per Second: 21,191.73295
Overall Steps per Second: 10,251.69585

Timestep Collection Time: 2.36073
Timestep Consumption Time: 2.51924
PPO Batch Consumption Time: 0.14671
Total Iteration Time: 4.87997

Cumulative Model Updates: 3,140
Cumulative Timesteps: 13,157,092

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 215.48897
Policy Entropy: 0.37362
Value Function Loss: 3.24266

Mean KL Divergence: 0.01410
SB3 Clip Fraction: 0.09765
Policy Update Magnitude: 0.75615
Value Function Update Magnitude: 1.07342

Collected Steps per Second: 21,140.05840
Overall Steps per Second: 10,360.31864

Timestep Collection Time: 2.36584
Timestep Consumption Time: 2.46162
PPO Batch Consumption Time: 0.14560
Total Iteration Time: 4.82746

Cumulative Model Updates: 3,152
Cumulative Timesteps: 13,207,106

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.04354
Policy Entropy: 0.35769
Value Function Loss: 3.25495

Mean KL Divergence: 0.01614
SB3 Clip Fraction: 0.10141
Policy Update Magnitude: 0.76044
Value Function Update Magnitude: 1.12628

Collected Steps per Second: 18,386.44057
Overall Steps per Second: 9,775.31648

Timestep Collection Time: 2.71983
Timestep Consumption Time: 2.39591
PPO Batch Consumption Time: 0.14665
Total Iteration Time: 5.11574

Cumulative Model Updates: 3,164
Cumulative Timesteps: 13,257,114

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 247.36958
Policy Entropy: 0.34341
Value Function Loss: 3.12826

Mean KL Divergence: 0.01429
SB3 Clip Fraction: 0.09532
Policy Update Magnitude: 0.73777
Value Function Update Magnitude: 1.17243

Collected Steps per Second: 19,588.21881
Overall Steps per Second: 9,875.76349

Timestep Collection Time: 2.55255
Timestep Consumption Time: 2.51035
PPO Batch Consumption Time: 0.15247
Total Iteration Time: 5.06290

Cumulative Model Updates: 3,176
Cumulative Timesteps: 13,307,114

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.18216
Policy Entropy: 0.32126
Value Function Loss: 3.04478

Mean KL Divergence: 0.01179
SB3 Clip Fraction: 0.08354
Policy Update Magnitude: 0.81898
Value Function Update Magnitude: 1.11330

Collected Steps per Second: 21,292.27202
Overall Steps per Second: 10,556.00213

Timestep Collection Time: 2.34940
Timestep Consumption Time: 2.38952
PPO Batch Consumption Time: 0.14639
Total Iteration Time: 4.73892

Cumulative Model Updates: 3,188
Cumulative Timesteps: 13,357,138

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 233.33877
Policy Entropy: 0.31934
Value Function Loss: 2.89609

Mean KL Divergence: 0.01441
SB3 Clip Fraction: 0.09399
Policy Update Magnitude: 0.75798
Value Function Update Magnitude: 1.05607

Collected Steps per Second: 20,903.63919
Overall Steps per Second: 10,224.23260

Timestep Collection Time: 2.39279
Timestep Consumption Time: 2.49931
PPO Batch Consumption Time: 0.15188
Total Iteration Time: 4.89210

Cumulative Model Updates: 3,200
Cumulative Timesteps: 13,407,156

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.82995
Policy Entropy: 0.33458
Value Function Loss: 2.92661

Mean KL Divergence: 0.01444
SB3 Clip Fraction: 0.09266
Policy Update Magnitude: 0.81949
Value Function Update Magnitude: 1.07091

Collected Steps per Second: 20,486.04307
Overall Steps per Second: 9,959.07561

Timestep Collection Time: 2.44078
Timestep Consumption Time: 2.57996
PPO Batch Consumption Time: 0.15053
Total Iteration Time: 5.02075

Cumulative Model Updates: 3,212
Cumulative Timesteps: 13,457,158

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.30498
Policy Entropy: 0.32403
Value Function Loss: 2.84538

Mean KL Divergence: 0.01308
SB3 Clip Fraction: 0.09008
Policy Update Magnitude: 0.87544
Value Function Update Magnitude: 1.17040

Collected Steps per Second: 21,358.35628
Overall Steps per Second: 10,071.47599

Timestep Collection Time: 2.34213
Timestep Consumption Time: 2.62477
PPO Batch Consumption Time: 0.16158
Total Iteration Time: 4.96690

Cumulative Model Updates: 3,224
Cumulative Timesteps: 13,507,182

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 230.81812
Policy Entropy: 0.32801
Value Function Loss: 3.00712

Mean KL Divergence: 0.01308
SB3 Clip Fraction: 0.08881
Policy Update Magnitude: 0.87768
Value Function Update Magnitude: 1.10457

Collected Steps per Second: 18,536.97027
Overall Steps per Second: 9,269.39956

Timestep Collection Time: 2.69893
Timestep Consumption Time: 2.69840
PPO Batch Consumption Time: 0.15994
Total Iteration Time: 5.39733

Cumulative Model Updates: 3,236
Cumulative Timesteps: 13,557,212

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 251.27731
Policy Entropy: 0.32223
Value Function Loss: 3.02201

Mean KL Divergence: 0.01522
SB3 Clip Fraction: 0.09283
Policy Update Magnitude: 0.85766
Value Function Update Magnitude: 1.10422

Collected Steps per Second: 19,469.58330
Overall Steps per Second: 9,926.61313

Timestep Collection Time: 2.56862
Timestep Consumption Time: 2.46935
PPO Batch Consumption Time: 0.15507
Total Iteration Time: 5.03797

Cumulative Model Updates: 3,248
Cumulative Timesteps: 13,607,222

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 221.93669
Policy Entropy: 0.30864
Value Function Loss: 3.01577

Mean KL Divergence: 0.01358
SB3 Clip Fraction: 0.08781
Policy Update Magnitude: 0.76981
Value Function Update Magnitude: 1.12970

Collected Steps per Second: 19,008.14103
Overall Steps per Second: 9,767.64762

Timestep Collection Time: 2.63066
Timestep Consumption Time: 2.48869
PPO Batch Consumption Time: 0.14965
Total Iteration Time: 5.11935

Cumulative Model Updates: 3,260
Cumulative Timesteps: 13,657,226

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 248.84365
Policy Entropy: 0.30564
Value Function Loss: 3.07017

Mean KL Divergence: 0.01159
SB3 Clip Fraction: 0.07932
Policy Update Magnitude: 0.84526
Value Function Update Magnitude: 1.09798

Collected Steps per Second: 22,269.25003
Overall Steps per Second: 10,661.36247

Timestep Collection Time: 2.24534
Timestep Consumption Time: 2.44468
PPO Batch Consumption Time: 0.14878
Total Iteration Time: 4.69002

Cumulative Model Updates: 3,272
Cumulative Timesteps: 13,707,228

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 238.49270
Policy Entropy: 0.29160
Value Function Loss: 3.07452

Mean KL Divergence: 0.01136
SB3 Clip Fraction: 0.07787
Policy Update Magnitude: 0.84679
Value Function Update Magnitude: 1.12952

Collected Steps per Second: 20,034.06720
Overall Steps per Second: 10,122.03111

Timestep Collection Time: 2.49795
Timestep Consumption Time: 2.44612
PPO Batch Consumption Time: 0.14501
Total Iteration Time: 4.94407

Cumulative Model Updates: 3,284
Cumulative Timesteps: 13,757,272

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.91842
Policy Entropy: 0.27245
Value Function Loss: 3.01783

Mean KL Divergence: 0.01191
SB3 Clip Fraction: 0.07799
Policy Update Magnitude: 0.80175
Value Function Update Magnitude: 1.20017

Collected Steps per Second: 19,571.70263
Overall Steps per Second: 9,800.01935

Timestep Collection Time: 2.55604
Timestep Consumption Time: 2.54865
PPO Batch Consumption Time: 0.14928
Total Iteration Time: 5.10468

Cumulative Model Updates: 3,296
Cumulative Timesteps: 13,807,298

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 200.97787
Policy Entropy: 0.27222
Value Function Loss: 3.05296

Mean KL Divergence: 0.01443
SB3 Clip Fraction: 0.08552
Policy Update Magnitude: 0.82672
Value Function Update Magnitude: 1.28791

Collected Steps per Second: 19,059.97516
Overall Steps per Second: 9,992.17193

Timestep Collection Time: 2.62403
Timestep Consumption Time: 2.38129
PPO Batch Consumption Time: 0.14462
Total Iteration Time: 5.00532

Cumulative Model Updates: 3,308
Cumulative Timesteps: 13,857,312

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.09217
Policy Entropy: 0.26580
Value Function Loss: 2.92867

Mean KL Divergence: 0.01370
SB3 Clip Fraction: 0.08205
Policy Update Magnitude: 0.80991
Value Function Update Magnitude: 1.28781

Collected Steps per Second: 19,936.98913
Overall Steps per Second: 10,020.02221

Timestep Collection Time: 2.50820
Timestep Consumption Time: 2.48241
PPO Batch Consumption Time: 0.14517
Total Iteration Time: 4.99061

Cumulative Model Updates: 3,320
Cumulative Timesteps: 13,907,318

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 246.26881
Policy Entropy: 0.26570
Value Function Loss: 3.06454

Mean KL Divergence: 0.01582
SB3 Clip Fraction: 0.08960
Policy Update Magnitude: 0.77005
Value Function Update Magnitude: 1.20706

Collected Steps per Second: 19,212.74807
Overall Steps per Second: 9,945.42735

Timestep Collection Time: 2.60442
Timestep Consumption Time: 2.42684
PPO Batch Consumption Time: 0.14372
Total Iteration Time: 5.03126

Cumulative Model Updates: 3,332
Cumulative Timesteps: 13,957,356

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 225.73911
Policy Entropy: 0.26330
Value Function Loss: 3.05463

Mean KL Divergence: 0.01314
SB3 Clip Fraction: 0.07992
Policy Update Magnitude: 0.83758
Value Function Update Magnitude: 1.21904

Collected Steps per Second: 23,143.83415
Overall Steps per Second: 10,635.13475

Timestep Collection Time: 2.16135
Timestep Consumption Time: 2.54211
PPO Batch Consumption Time: 0.15195
Total Iteration Time: 4.70347

Cumulative Model Updates: 3,344
Cumulative Timesteps: 14,007,378

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.05456
Policy Entropy: 0.27384
Value Function Loss: 2.95977

Mean KL Divergence: 0.01510
SB3 Clip Fraction: 0.08595
Policy Update Magnitude: 0.80683
Value Function Update Magnitude: 1.21096

Collected Steps per Second: 18,292.97107
Overall Steps per Second: 9,674.70464

Timestep Collection Time: 2.73406
Timestep Consumption Time: 2.43551
PPO Batch Consumption Time: 0.14465
Total Iteration Time: 5.16956

Cumulative Model Updates: 3,356
Cumulative Timesteps: 14,057,392

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 256.79012
Policy Entropy: 0.26615
Value Function Loss: 2.97434

Mean KL Divergence: 0.01767
SB3 Clip Fraction: 0.09149
Policy Update Magnitude: 0.77257
Value Function Update Magnitude: 1.19532

Collected Steps per Second: 21,702.60027
Overall Steps per Second: 10,738.23164

Timestep Collection Time: 2.30489
Timestep Consumption Time: 2.35342
PPO Batch Consumption Time: 0.14388
Total Iteration Time: 4.65831

Cumulative Model Updates: 3,368
Cumulative Timesteps: 14,107,414

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 247.10614
Policy Entropy: 0.26024
Value Function Loss: 3.07580

Mean KL Divergence: 0.02475
SB3 Clip Fraction: 0.10627
Policy Update Magnitude: 0.71105
Value Function Update Magnitude: 1.26501

Collected Steps per Second: 20,367.09455
Overall Steps per Second: 10,204.54094

Timestep Collection Time: 2.45661
Timestep Consumption Time: 2.44650
PPO Batch Consumption Time: 0.14369
Total Iteration Time: 4.90311

Cumulative Model Updates: 3,380
Cumulative Timesteps: 14,157,448

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 239.33946
Policy Entropy: 0.24019
Value Function Loss: 3.10539

Mean KL Divergence: 0.01682
SB3 Clip Fraction: 0.08648
Policy Update Magnitude: 0.76553
Value Function Update Magnitude: 1.25446

Collected Steps per Second: 20,839.88284
Overall Steps per Second: 10,330.00465

Timestep Collection Time: 2.40126
Timestep Consumption Time: 2.44307
PPO Batch Consumption Time: 0.15039
Total Iteration Time: 4.84433

Cumulative Model Updates: 3,392
Cumulative Timesteps: 14,207,490

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 254.92489
Policy Entropy: 0.23572
Value Function Loss: 2.99330

Mean KL Divergence: 0.01287
SB3 Clip Fraction: 0.07450
Policy Update Magnitude: 0.82111
Value Function Update Magnitude: 1.21721

Collected Steps per Second: 19,500.45856
Overall Steps per Second: 10,037.62474

Timestep Collection Time: 2.56476
Timestep Consumption Time: 2.41789
PPO Batch Consumption Time: 0.14344
Total Iteration Time: 4.98265

Cumulative Model Updates: 3,404
Cumulative Timesteps: 14,257,504

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.61766
Policy Entropy: 0.23222
Value Function Loss: 2.82558

Mean KL Divergence: 0.01089
SB3 Clip Fraction: 0.06799
Policy Update Magnitude: 0.79558
Value Function Update Magnitude: 1.21979

Collected Steps per Second: 23,234.00868
Overall Steps per Second: 10,981.19707

Timestep Collection Time: 2.15331
Timestep Consumption Time: 2.40266
PPO Batch Consumption Time: 0.14429
Total Iteration Time: 4.55597

Cumulative Model Updates: 3,416
Cumulative Timesteps: 14,307,534

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.05208
Policy Entropy: 0.23370
Value Function Loss: 2.89671

Mean KL Divergence: 0.01090
SB3 Clip Fraction: 0.06938
Policy Update Magnitude: 0.79593
Value Function Update Magnitude: 1.19200

Collected Steps per Second: 21,253.86055
Overall Steps per Second: 10,495.32859

Timestep Collection Time: 2.35458
Timestep Consumption Time: 2.41363
PPO Batch Consumption Time: 0.14561
Total Iteration Time: 4.76822

Cumulative Model Updates: 3,428
Cumulative Timesteps: 14,357,578

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.52248
Policy Entropy: 0.22908
Value Function Loss: 3.09739

Mean KL Divergence: 0.01086
SB3 Clip Fraction: 0.06901
Policy Update Magnitude: 0.80575
Value Function Update Magnitude: 1.14851

Collected Steps per Second: 22,765.85010
Overall Steps per Second: 10,473.01404

Timestep Collection Time: 2.19741
Timestep Consumption Time: 2.57924
PPO Batch Consumption Time: 0.15314
Total Iteration Time: 4.77666

Cumulative Model Updates: 3,440
Cumulative Timesteps: 14,407,604

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 226.36249
Policy Entropy: 0.22449
Value Function Loss: 3.01947

Mean KL Divergence: 0.01131
SB3 Clip Fraction: 0.06975
Policy Update Magnitude: 0.79419
Value Function Update Magnitude: 1.08906

Collected Steps per Second: 19,797.12303
Overall Steps per Second: 9,730.42278

Timestep Collection Time: 2.52643
Timestep Consumption Time: 2.61374
PPO Batch Consumption Time: 0.15965
Total Iteration Time: 5.14017

Cumulative Model Updates: 3,452
Cumulative Timesteps: 14,457,620

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.92292
Policy Entropy: 0.21058
Value Function Loss: 2.95067

Mean KL Divergence: 0.01104
SB3 Clip Fraction: 0.06845
Policy Update Magnitude: 0.79527
Value Function Update Magnitude: 1.10840

Collected Steps per Second: 19,226.16419
Overall Steps per Second: 9,736.34458

Timestep Collection Time: 2.60166
Timestep Consumption Time: 2.53579
PPO Batch Consumption Time: 0.15213
Total Iteration Time: 5.13745

Cumulative Model Updates: 3,464
Cumulative Timesteps: 14,507,640

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.59767
Policy Entropy: 0.19930
Value Function Loss: 2.85543

Mean KL Divergence: 0.01091
SB3 Clip Fraction: 0.06558
Policy Update Magnitude: 0.77399
Value Function Update Magnitude: 1.14329

Collected Steps per Second: 21,588.64039
Overall Steps per Second: 10,520.42397

Timestep Collection Time: 2.31640
Timestep Consumption Time: 2.43702
PPO Batch Consumption Time: 0.15106
Total Iteration Time: 4.75342

Cumulative Model Updates: 3,476
Cumulative Timesteps: 14,557,648

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.25685
Policy Entropy: 0.19391
Value Function Loss: 2.93473

Mean KL Divergence: 0.01114
SB3 Clip Fraction: 0.06487
Policy Update Magnitude: 0.77434
Value Function Update Magnitude: 1.06397

Collected Steps per Second: 18,511.59742
Overall Steps per Second: 9,689.49559

Timestep Collection Time: 2.70295
Timestep Consumption Time: 2.46099
PPO Batch Consumption Time: 0.14614
Total Iteration Time: 5.16394

Cumulative Model Updates: 3,488
Cumulative Timesteps: 14,607,684

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.20206
Policy Entropy: 0.19426
Value Function Loss: 2.95919

Mean KL Divergence: 0.01069
SB3 Clip Fraction: 0.06256
Policy Update Magnitude: 0.78305
Value Function Update Magnitude: 1.04711

Collected Steps per Second: 19,857.98799
Overall Steps per Second: 9,880.87716

Timestep Collection Time: 2.51889
Timestep Consumption Time: 2.54342
PPO Batch Consumption Time: 0.15338
Total Iteration Time: 5.06230

Cumulative Model Updates: 3,500
Cumulative Timesteps: 14,657,704

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 243.08955
Policy Entropy: 0.18897
Value Function Loss: 3.00868

Mean KL Divergence: 0.01228
SB3 Clip Fraction: 0.06735
Policy Update Magnitude: 0.76555
Value Function Update Magnitude: 1.10529

Collected Steps per Second: 22,333.82934
Overall Steps per Second: 10,748.52181

Timestep Collection Time: 2.23920
Timestep Consumption Time: 2.41353
PPO Batch Consumption Time: 0.14436
Total Iteration Time: 4.65273

Cumulative Model Updates: 3,512
Cumulative Timesteps: 14,707,714

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 222.64290
Policy Entropy: 0.19723
Value Function Loss: 3.01799

Mean KL Divergence: 0.01386
SB3 Clip Fraction: 0.07198
Policy Update Magnitude: 0.80520
Value Function Update Magnitude: 1.11645

Collected Steps per Second: 23,234.64437
Overall Steps per Second: 10,755.18541

Timestep Collection Time: 2.15359
Timestep Consumption Time: 2.49886
PPO Batch Consumption Time: 0.15281
Total Iteration Time: 4.65245

Cumulative Model Updates: 3,524
Cumulative Timesteps: 14,757,752

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 261.32311
Policy Entropy: 0.19454
Value Function Loss: 3.06699

Mean KL Divergence: 0.01412
SB3 Clip Fraction: 0.07239
Policy Update Magnitude: 0.81098
Value Function Update Magnitude: 1.13860

Collected Steps per Second: 21,130.33975
Overall Steps per Second: 10,568.06585

Timestep Collection Time: 2.36835
Timestep Consumption Time: 2.36705
PPO Batch Consumption Time: 0.14415
Total Iteration Time: 4.73540

Cumulative Model Updates: 3,536
Cumulative Timesteps: 14,807,796

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 241.34838
Policy Entropy: 0.19667
Value Function Loss: 3.07067

Mean KL Divergence: 0.01342
SB3 Clip Fraction: 0.06982
Policy Update Magnitude: 0.81046
Value Function Update Magnitude: 1.13201

Collected Steps per Second: 19,702.54822
Overall Steps per Second: 10,012.52065

Timestep Collection Time: 2.53977
Timestep Consumption Time: 2.45797
PPO Batch Consumption Time: 0.14390
Total Iteration Time: 4.99774

Cumulative Model Updates: 3,548
Cumulative Timesteps: 14,857,836

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.15444
Policy Entropy: 0.19435
Value Function Loss: 3.16363

Mean KL Divergence: 0.01353
SB3 Clip Fraction: 0.07059
Policy Update Magnitude: 0.77845
Value Function Update Magnitude: 1.08958

Collected Steps per Second: 19,513.86267
Overall Steps per Second: 9,934.12457

Timestep Collection Time: 2.56372
Timestep Consumption Time: 2.47226
PPO Batch Consumption Time: 0.15049
Total Iteration Time: 5.03597

Cumulative Model Updates: 3,560
Cumulative Timesteps: 14,907,864

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 268.34386
Policy Entropy: 0.19090
Value Function Loss: 3.16205

Mean KL Divergence: 0.01292
SB3 Clip Fraction: 0.06711
Policy Update Magnitude: 0.78759
Value Function Update Magnitude: 1.11522

Collected Steps per Second: 19,685.83801
Overall Steps per Second: 9,985.69335

Timestep Collection Time: 2.54122
Timestep Consumption Time: 2.46855
PPO Batch Consumption Time: 0.14505
Total Iteration Time: 5.00977

Cumulative Model Updates: 3,572
Cumulative Timesteps: 14,957,890

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 199.97712
Policy Entropy: 0.18908
Value Function Loss: 3.16462

Mean KL Divergence: 0.01280
SB3 Clip Fraction: 0.06834
Policy Update Magnitude: 0.79695
Value Function Update Magnitude: 1.09907

Collected Steps per Second: 19,293.88753
Overall Steps per Second: 9,874.75001

Timestep Collection Time: 2.59284
Timestep Consumption Time: 2.47321
PPO Batch Consumption Time: 0.15072
Total Iteration Time: 5.06605

Cumulative Model Updates: 3,584
Cumulative Timesteps: 15,007,916

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 222.13506
Policy Entropy: 0.18759
Value Function Loss: 3.08823

Mean KL Divergence: 0.01526
SB3 Clip Fraction: 0.07447
Policy Update Magnitude: 0.79274
Value Function Update Magnitude: 1.17172

Collected Steps per Second: 21,938.06896
Overall Steps per Second: 10,706.73921

Timestep Collection Time: 2.27933
Timestep Consumption Time: 2.39100
PPO Batch Consumption Time: 0.14740
Total Iteration Time: 4.67033

Cumulative Model Updates: 3,596
Cumulative Timesteps: 15,057,920

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.67254
Policy Entropy: 0.17087
Value Function Loss: 3.07947

Mean KL Divergence: 0.01804
SB3 Clip Fraction: 0.07715
Policy Update Magnitude: 0.75414
Value Function Update Magnitude: 1.24044

Collected Steps per Second: 21,886.07263
Overall Steps per Second: 10,151.27061

Timestep Collection Time: 2.28556
Timestep Consumption Time: 2.64210
PPO Batch Consumption Time: 0.16227
Total Iteration Time: 4.92766

Cumulative Model Updates: 3,608
Cumulative Timesteps: 15,107,942

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 255.61429
Policy Entropy: 0.16690
Value Function Loss: 2.94104

Mean KL Divergence: 0.01599
SB3 Clip Fraction: 0.07035
Policy Update Magnitude: 0.74771
Value Function Update Magnitude: 1.23820

Collected Steps per Second: 20,294.92605
Overall Steps per Second: 9,540.03127

Timestep Collection Time: 2.46604
Timestep Consumption Time: 2.78007
PPO Batch Consumption Time: 0.16655
Total Iteration Time: 5.24610

Cumulative Model Updates: 3,620
Cumulative Timesteps: 15,157,990

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 240.47093
Policy Entropy: 0.15557
Value Function Loss: 2.78350

Mean KL Divergence: 0.01406
SB3 Clip Fraction: 0.06412
Policy Update Magnitude: 0.69898
Value Function Update Magnitude: 1.58533

Collected Steps per Second: 19,789.36970
Overall Steps per Second: 10,018.84568

Timestep Collection Time: 2.52752
Timestep Consumption Time: 2.46487
PPO Batch Consumption Time: 0.14544
Total Iteration Time: 4.99239

Cumulative Model Updates: 3,632
Cumulative Timesteps: 15,208,008

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.78432
Policy Entropy: 0.15985
Value Function Loss: 2.68351

Mean KL Divergence: 0.01276
SB3 Clip Fraction: 0.06029
Policy Update Magnitude: 0.75435
Value Function Update Magnitude: 2.00317

Collected Steps per Second: 17,823.77511
Overall Steps per Second: 9,552.87269

Timestep Collection Time: 2.80805
Timestep Consumption Time: 2.43121
PPO Batch Consumption Time: 0.14660
Total Iteration Time: 5.23926

Cumulative Model Updates: 3,644
Cumulative Timesteps: 15,258,058

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 249.12611
Policy Entropy: 0.15855
Value Function Loss: 3.15766

Mean KL Divergence: 0.01290
SB3 Clip Fraction: 0.06127
Policy Update Magnitude: 0.73860
Value Function Update Magnitude: 1.51792

Collected Steps per Second: 20,100.05186
Overall Steps per Second: 10,325.30660

Timestep Collection Time: 2.48885
Timestep Consumption Time: 2.35614
PPO Batch Consumption Time: 0.14524
Total Iteration Time: 4.84499

Cumulative Model Updates: 3,656
Cumulative Timesteps: 15,308,084

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.36780
Policy Entropy: 0.16123
Value Function Loss: 3.32377

Mean KL Divergence: 0.01550
SB3 Clip Fraction: 0.06822
Policy Update Magnitude: 0.72122
Value Function Update Magnitude: 1.79645

Collected Steps per Second: 21,318.42120
Overall Steps per Second: 10,509.03603

Timestep Collection Time: 2.34755
Timestep Consumption Time: 2.41464
PPO Batch Consumption Time: 0.14471
Total Iteration Time: 4.76219

Cumulative Model Updates: 3,668
Cumulative Timesteps: 15,358,130

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 262.18522
Policy Entropy: 0.15487
Value Function Loss: 3.23565

Mean KL Divergence: 0.01392
SB3 Clip Fraction: 0.06470
Policy Update Magnitude: 0.75702
Value Function Update Magnitude: 1.84284

Collected Steps per Second: 18,991.92008
Overall Steps per Second: 9,891.00349

Timestep Collection Time: 2.63502
Timestep Consumption Time: 2.42453
PPO Batch Consumption Time: 0.14507
Total Iteration Time: 5.05955

Cumulative Model Updates: 3,680
Cumulative Timesteps: 15,408,174

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 252.11244
Policy Entropy: 0.15024
Value Function Loss: 3.35360

Mean KL Divergence: 0.01277
SB3 Clip Fraction: 0.06060
Policy Update Magnitude: 0.79084
Value Function Update Magnitude: 1.55313

Collected Steps per Second: 21,534.05140
Overall Steps per Second: 10,576.18283

Timestep Collection Time: 2.32423
Timestep Consumption Time: 2.40811
PPO Batch Consumption Time: 0.14493
Total Iteration Time: 4.73233

Cumulative Model Updates: 3,692
Cumulative Timesteps: 15,458,224

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 240.79027
Policy Entropy: 0.15551
Value Function Loss: 3.41322

Mean KL Divergence: 0.01487
SB3 Clip Fraction: 0.06482
Policy Update Magnitude: 0.80835
Value Function Update Magnitude: 1.27958

Collected Steps per Second: 19,312.73303
Overall Steps per Second: 10,006.11340

Timestep Collection Time: 2.58928
Timestep Consumption Time: 2.40827
PPO Batch Consumption Time: 0.14393
Total Iteration Time: 4.99754

Cumulative Model Updates: 3,704
Cumulative Timesteps: 15,508,230

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.03887
Policy Entropy: 0.16485
Value Function Loss: 3.44533

Mean KL Divergence: 0.01617
SB3 Clip Fraction: 0.06665
Policy Update Magnitude: 0.83187
Value Function Update Magnitude: 1.27400

Collected Steps per Second: 22,255.55425
Overall Steps per Second: 10,706.03850

Timestep Collection Time: 2.24789
Timestep Consumption Time: 2.42499
PPO Batch Consumption Time: 0.14439
Total Iteration Time: 4.67288

Cumulative Model Updates: 3,716
Cumulative Timesteps: 15,558,258

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.74156
Policy Entropy: 0.17090
Value Function Loss: 3.09015

Mean KL Divergence: 0.01479
SB3 Clip Fraction: 0.06549
Policy Update Magnitude: 0.82321
Value Function Update Magnitude: 1.58841

Collected Steps per Second: 22,764.67851
Overall Steps per Second: 10,820.38696

Timestep Collection Time: 2.19849
Timestep Consumption Time: 2.42685
PPO Batch Consumption Time: 0.14520
Total Iteration Time: 4.62534

Cumulative Model Updates: 3,728
Cumulative Timesteps: 15,608,306

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 247.17678
Policy Entropy: 0.17214
Value Function Loss: 3.07465

Mean KL Divergence: 0.01373
SB3 Clip Fraction: 0.06382
Policy Update Magnitude: 0.79039
Value Function Update Magnitude: 1.53801

Collected Steps per Second: 22,718.92428
Overall Steps per Second: 10,998.19435

Timestep Collection Time: 2.20319
Timestep Consumption Time: 2.34793
PPO Batch Consumption Time: 0.14524
Total Iteration Time: 4.55111

Cumulative Model Updates: 3,740
Cumulative Timesteps: 15,658,360

Timesteps Collected: 50,054
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 260.00100
Policy Entropy: 0.17674
Value Function Loss: 3.01092

Mean KL Divergence: 0.01235
SB3 Clip Fraction: 0.06108
Policy Update Magnitude: 0.78732
Value Function Update Magnitude: 1.61474

Collected Steps per Second: 21,331.67013
Overall Steps per Second: 10,461.29276

Timestep Collection Time: 2.34600
Timestep Consumption Time: 2.43773
PPO Batch Consumption Time: 0.14550
Total Iteration Time: 4.78373

Cumulative Model Updates: 3,752
Cumulative Timesteps: 15,708,404

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 236.67683
Policy Entropy: 0.17965
Value Function Loss: 3.07512

Mean KL Divergence: 0.01371
SB3 Clip Fraction: 0.06680
Policy Update Magnitude: 0.79765
Value Function Update Magnitude: 1.43322

Collected Steps per Second: 22,159.95731
Overall Steps per Second: 10,746.70994

Timestep Collection Time: 2.25831
Timestep Consumption Time: 2.39837
PPO Batch Consumption Time: 0.14327
Total Iteration Time: 4.65668

Cumulative Model Updates: 3,764
Cumulative Timesteps: 15,758,448

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 221.03182
Policy Entropy: 0.17521
Value Function Loss: 2.92118

Mean KL Divergence: 0.01410
SB3 Clip Fraction: 0.06729
Policy Update Magnitude: 0.79869
Value Function Update Magnitude: 1.51226

Collected Steps per Second: 22,750.32555
Overall Steps per Second: 10,643.34996

Timestep Collection Time: 2.19900
Timestep Consumption Time: 2.50140
PPO Batch Consumption Time: 0.14664
Total Iteration Time: 4.70040

Cumulative Model Updates: 3,776
Cumulative Timesteps: 15,808,476

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 204.53482
Policy Entropy: 0.17558
Value Function Loss: 2.83772

Mean KL Divergence: 0.01394
SB3 Clip Fraction: 0.06716
Policy Update Magnitude: 0.82775
Value Function Update Magnitude: 1.55714

Collected Steps per Second: 19,291.23036
Overall Steps per Second: 9,908.63534

Timestep Collection Time: 2.59455
Timestep Consumption Time: 2.45680
PPO Batch Consumption Time: 0.14700
Total Iteration Time: 5.05135

Cumulative Model Updates: 3,788
Cumulative Timesteps: 15,858,528

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.54930
Policy Entropy: 0.17128
Value Function Loss: 2.75253

Mean KL Divergence: 0.01494
SB3 Clip Fraction: 0.06955
Policy Update Magnitude: 0.82796
Value Function Update Magnitude: 1.55259

Collected Steps per Second: 21,818.53538
Overall Steps per Second: 10,581.12140

Timestep Collection Time: 2.29355
Timestep Consumption Time: 2.43581
PPO Batch Consumption Time: 0.14329
Total Iteration Time: 4.72937

Cumulative Model Updates: 3,800
Cumulative Timesteps: 15,908,570

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 251.95824
Policy Entropy: 0.17240
Value Function Loss: 2.79265

Mean KL Divergence: 0.01595
SB3 Clip Fraction: 0.07216
Policy Update Magnitude: 0.83878
Value Function Update Magnitude: 1.49827

Collected Steps per Second: 21,743.43580
Overall Steps per Second: 10,528.29853

Timestep Collection Time: 2.30046
Timestep Consumption Time: 2.45054
PPO Batch Consumption Time: 0.14504
Total Iteration Time: 4.75101

Cumulative Model Updates: 3,812
Cumulative Timesteps: 15,958,590

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 223.69651
Policy Entropy: 0.17184
Value Function Loss: 2.86691

Mean KL Divergence: 0.01674
SB3 Clip Fraction: 0.07243
Policy Update Magnitude: 0.79843
Value Function Update Magnitude: 1.50098

Collected Steps per Second: 21,852.44014
Overall Steps per Second: 10,687.93010

Timestep Collection Time: 2.29009
Timestep Consumption Time: 2.39220
PPO Batch Consumption Time: 0.14526
Total Iteration Time: 4.68229

Cumulative Model Updates: 3,824
Cumulative Timesteps: 16,008,634

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.10012
Policy Entropy: 0.16213
Value Function Loss: 2.83804

Mean KL Divergence: 0.01442
SB3 Clip Fraction: 0.06738
Policy Update Magnitude: 0.79766
Value Function Update Magnitude: 1.49673

Collected Steps per Second: 22,308.93908
Overall Steps per Second: 10,706.77534

Timestep Collection Time: 2.24296
Timestep Consumption Time: 2.43053
PPO Batch Consumption Time: 0.14498
Total Iteration Time: 4.67349

Cumulative Model Updates: 3,836
Cumulative Timesteps: 16,058,672

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 243.64079
Policy Entropy: 0.15937
Value Function Loss: 2.91712

Mean KL Divergence: 0.01313
SB3 Clip Fraction: 0.06305
Policy Update Magnitude: 0.79603
Value Function Update Magnitude: 1.47902

Collected Steps per Second: 21,858.85543
Overall Steps per Second: 10,727.92111

Timestep Collection Time: 2.28914
Timestep Consumption Time: 2.37514
PPO Batch Consumption Time: 0.14395
Total Iteration Time: 4.66428

Cumulative Model Updates: 3,848
Cumulative Timesteps: 16,108,710

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 221.61510
Policy Entropy: 0.15711
Value Function Loss: 3.00708

Mean KL Divergence: 0.01325
SB3 Clip Fraction: 0.06356
Policy Update Magnitude: 0.79374
Value Function Update Magnitude: 1.50094

Collected Steps per Second: 23,372.07771
Overall Steps per Second: 10,934.43369

Timestep Collection Time: 2.14110
Timestep Consumption Time: 2.43545
PPO Batch Consumption Time: 0.14564
Total Iteration Time: 4.57655

Cumulative Model Updates: 3,860
Cumulative Timesteps: 16,158,752

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 228.71574
Policy Entropy: 0.15860
Value Function Loss: 3.02762

Mean KL Divergence: 0.01487
SB3 Clip Fraction: 0.06693
Policy Update Magnitude: 0.83056
Value Function Update Magnitude: 1.48604

Collected Steps per Second: 21,691.04162
Overall Steps per Second: 10,575.10921

Timestep Collection Time: 2.30565
Timestep Consumption Time: 2.42357
PPO Batch Consumption Time: 0.14581
Total Iteration Time: 4.72922

Cumulative Model Updates: 3,872
Cumulative Timesteps: 16,208,764

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 243.66034
Policy Entropy: 0.15572
Value Function Loss: 2.99181

Mean KL Divergence: 0.01825
SB3 Clip Fraction: 0.07191
Policy Update Magnitude: 0.79734
Value Function Update Magnitude: 1.46278

Collected Steps per Second: 20,389.12370
Overall Steps per Second: 10,460.23354

Timestep Collection Time: 2.45268
Timestep Consumption Time: 2.32809
PPO Batch Consumption Time: 0.14461
Total Iteration Time: 4.78077

Cumulative Model Updates: 3,884
Cumulative Timesteps: 16,258,772

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.26842
Policy Entropy: 0.16179
Value Function Loss: 3.02663

Mean KL Divergence: 0.02184
SB3 Clip Fraction: 0.07905
Policy Update Magnitude: 0.76727
Value Function Update Magnitude: 1.75773

Collected Steps per Second: 21,571.71993
Overall Steps per Second: 10,453.45235

Timestep Collection Time: 2.31859
Timestep Consumption Time: 2.46605
PPO Batch Consumption Time: 0.14518
Total Iteration Time: 4.78464

Cumulative Model Updates: 3,896
Cumulative Timesteps: 16,308,788

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 255.11281
Policy Entropy: 0.16423
Value Function Loss: 2.96983

Mean KL Divergence: 0.02164
SB3 Clip Fraction: 0.07995
Policy Update Magnitude: 0.75187
Value Function Update Magnitude: 1.88136

Collected Steps per Second: 17,688.09139
Overall Steps per Second: 9,465.56835

Timestep Collection Time: 2.82947
Timestep Consumption Time: 2.45790
PPO Batch Consumption Time: 0.14468
Total Iteration Time: 5.28737

Cumulative Model Updates: 3,908
Cumulative Timesteps: 16,358,836

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 219.82658
Policy Entropy: 0.16926
Value Function Loss: 3.21538

Mean KL Divergence: 0.01903
SB3 Clip Fraction: 0.07665
Policy Update Magnitude: 0.84571
Value Function Update Magnitude: 1.43638

Collected Steps per Second: 21,618.56241
Overall Steps per Second: 10,527.25628

Timestep Collection Time: 2.31468
Timestep Consumption Time: 2.43870
PPO Batch Consumption Time: 0.14482
Total Iteration Time: 4.75338

Cumulative Model Updates: 3,920
Cumulative Timesteps: 16,408,876

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 239.85955
Policy Entropy: 0.16703
Value Function Loss: 3.46818

Mean KL Divergence: 0.01734
SB3 Clip Fraction: 0.07387
Policy Update Magnitude: 0.88418
Value Function Update Magnitude: 1.12879

Collected Steps per Second: 19,934.26958
Overall Steps per Second: 10,054.78101

Timestep Collection Time: 2.50995
Timestep Consumption Time: 2.46619
PPO Batch Consumption Time: 0.14439
Total Iteration Time: 4.97614

Cumulative Model Updates: 3,932
Cumulative Timesteps: 16,458,910

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 234.99213
Policy Entropy: 0.16753
Value Function Loss: 3.63479

Mean KL Divergence: 0.01742
SB3 Clip Fraction: 0.07345
Policy Update Magnitude: 0.88247
Value Function Update Magnitude: 1.03269

Collected Steps per Second: 21,692.64324
Overall Steps per Second: 10,575.26595

Timestep Collection Time: 2.30769
Timestep Consumption Time: 2.42599
PPO Batch Consumption Time: 0.14550
Total Iteration Time: 4.73369

Cumulative Model Updates: 3,944
Cumulative Timesteps: 16,508,970

Timesteps Collected: 50,060
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.72996
Policy Entropy: 0.17137
Value Function Loss: 3.70574

Mean KL Divergence: 0.01837
SB3 Clip Fraction: 0.07613
Policy Update Magnitude: 0.87302
Value Function Update Magnitude: 1.09148

Collected Steps per Second: 20,718.17285
Overall Steps per Second: 10,336.82933

Timestep Collection Time: 2.41498
Timestep Consumption Time: 2.42538
PPO Batch Consumption Time: 0.14398
Total Iteration Time: 4.84036

Cumulative Model Updates: 3,956
Cumulative Timesteps: 16,559,004

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 227.61952
Policy Entropy: 0.18277
Value Function Loss: 3.76536

Mean KL Divergence: 0.01708
SB3 Clip Fraction: 0.07434
Policy Update Magnitude: 0.90754
Value Function Update Magnitude: 0.97761

Collected Steps per Second: 21,032.26554
Overall Steps per Second: 10,450.18971

Timestep Collection Time: 2.37930
Timestep Consumption Time: 2.40932
PPO Batch Consumption Time: 0.14586
Total Iteration Time: 4.78862

Cumulative Model Updates: 3,968
Cumulative Timesteps: 16,609,046

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.09581
Policy Entropy: 0.20166
Value Function Loss: 3.96330

Mean KL Divergence: 0.02024
SB3 Clip Fraction: 0.08431
Policy Update Magnitude: 0.96176
Value Function Update Magnitude: 1.02039

Collected Steps per Second: 21,223.68172
Overall Steps per Second: 10,423.15670

Timestep Collection Time: 2.35661
Timestep Consumption Time: 2.44193
PPO Batch Consumption Time: 0.14488
Total Iteration Time: 4.79855

Cumulative Model Updates: 3,980
Cumulative Timesteps: 16,659,062

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.28953
Policy Entropy: 0.22461
Value Function Loss: 3.98749

Mean KL Divergence: 0.02656
SB3 Clip Fraction: 0.10028
Policy Update Magnitude: 0.95368
Value Function Update Magnitude: 1.16329

Collected Steps per Second: 21,151.50604
Overall Steps per Second: 10,474.23171

Timestep Collection Time: 2.36683
Timestep Consumption Time: 2.41271
PPO Batch Consumption Time: 0.14533
Total Iteration Time: 4.77954

Cumulative Model Updates: 3,992
Cumulative Timesteps: 16,709,124

Timesteps Collected: 50,062
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 276.14364
Policy Entropy: 0.23898
Value Function Loss: 4.11658

Mean KL Divergence: 0.02922
SB3 Clip Fraction: 0.10955
Policy Update Magnitude: 0.89057
Value Function Update Magnitude: 1.27569

Collected Steps per Second: 22,186.42650
Overall Steps per Second: 10,659.89245

Timestep Collection Time: 2.25390
Timestep Consumption Time: 2.43714
PPO Batch Consumption Time: 0.14644
Total Iteration Time: 4.69104

Cumulative Model Updates: 4,004
Cumulative Timesteps: 16,759,130

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 257.65584
Policy Entropy: 0.23788
Value Function Loss: 4.01102

Mean KL Divergence: 0.02911
SB3 Clip Fraction: 0.10610
Policy Update Magnitude: 0.97236
Value Function Update Magnitude: 0.99648

Collected Steps per Second: 22,703.50959
Overall Steps per Second: 10,829.93691

Timestep Collection Time: 2.20318
Timestep Consumption Time: 2.41550
PPO Batch Consumption Time: 0.14363
Total Iteration Time: 4.61868

Cumulative Model Updates: 4,016
Cumulative Timesteps: 16,809,150

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 270.05501
Policy Entropy: 0.27035
Value Function Loss: 4.22359

Mean KL Divergence: 0.03018
SB3 Clip Fraction: 0.11359
Policy Update Magnitude: 0.98677
Value Function Update Magnitude: 0.98518

Collected Steps per Second: 23,415.71310
Overall Steps per Second: 10,957.65398

Timestep Collection Time: 2.13686
Timestep Consumption Time: 2.42945
PPO Batch Consumption Time: 0.14670
Total Iteration Time: 4.56631

Cumulative Model Updates: 4,028
Cumulative Timesteps: 16,859,186

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 254.67224
Policy Entropy: 0.30069
Value Function Loss: 4.20125

Mean KL Divergence: 0.03147
SB3 Clip Fraction: 0.12308
Policy Update Magnitude: 0.98531
Value Function Update Magnitude: 1.06392

Collected Steps per Second: 20,142.15225
Overall Steps per Second: 9,962.91154

Timestep Collection Time: 2.48534
Timestep Consumption Time: 2.53930
PPO Batch Consumption Time: 0.15233
Total Iteration Time: 5.02464

Cumulative Model Updates: 4,040
Cumulative Timesteps: 16,909,246

Timesteps Collected: 50,060
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 233.33706
Policy Entropy: 0.32203
Value Function Loss: 4.27274

Mean KL Divergence: 0.02607
SB3 Clip Fraction: 0.11749
Policy Update Magnitude: 1.03730
Value Function Update Magnitude: 1.21584

Collected Steps per Second: 20,208.25240
Overall Steps per Second: 10,320.07928

Timestep Collection Time: 2.47493
Timestep Consumption Time: 2.37135
PPO Batch Consumption Time: 0.14604
Total Iteration Time: 4.84628

Cumulative Model Updates: 4,052
Cumulative Timesteps: 16,959,260

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.01609
Policy Entropy: 0.34403
Value Function Loss: 4.14442

Mean KL Divergence: 0.02370
SB3 Clip Fraction: 0.11368
Policy Update Magnitude: 1.09663
Value Function Update Magnitude: 1.23839

Collected Steps per Second: 21,214.23319
Overall Steps per Second: 10,383.06498

Timestep Collection Time: 2.35927
Timestep Consumption Time: 2.46108
PPO Batch Consumption Time: 0.14521
Total Iteration Time: 4.82035

Cumulative Model Updates: 4,064
Cumulative Timesteps: 17,009,310

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.69163
Policy Entropy: 0.39462
Value Function Loss: 4.26967

Mean KL Divergence: 0.02374
SB3 Clip Fraction: 0.12206
Policy Update Magnitude: 1.17855
Value Function Update Magnitude: 1.05152

Collected Steps per Second: 22,983.43297
Overall Steps per Second: 10,945.66226

Timestep Collection Time: 2.17705
Timestep Consumption Time: 2.39426
PPO Batch Consumption Time: 0.14441
Total Iteration Time: 4.57131

Cumulative Model Updates: 4,076
Cumulative Timesteps: 17,059,346

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.44769
Policy Entropy: 0.46361
Value Function Loss: 4.37156

Mean KL Divergence: 0.02621
SB3 Clip Fraction: 0.13774
Policy Update Magnitude: 1.25240
Value Function Update Magnitude: 1.05884

Collected Steps per Second: 21,087.68101
Overall Steps per Second: 10,348.72644

Timestep Collection Time: 2.37295
Timestep Consumption Time: 2.46243
PPO Batch Consumption Time: 0.14643
Total Iteration Time: 4.83538

Cumulative Model Updates: 4,088
Cumulative Timesteps: 17,109,386

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.07139
Policy Entropy: 0.50802
Value Function Loss: 4.44511

Mean KL Divergence: 0.02808
SB3 Clip Fraction: 0.14915
Policy Update Magnitude: 1.10853
Value Function Update Magnitude: 1.14094

Collected Steps per Second: 22,208.53593
Overall Steps per Second: 10,544.61815

Timestep Collection Time: 2.25319
Timestep Consumption Time: 2.49236
PPO Batch Consumption Time: 0.14671
Total Iteration Time: 4.74555

Cumulative Model Updates: 4,100
Cumulative Timesteps: 17,159,426

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.84812
Policy Entropy: 0.62473
Value Function Loss: 4.29994

Mean KL Divergence: 0.03489
SB3 Clip Fraction: 0.18524
Policy Update Magnitude: 0.95166
Value Function Update Magnitude: 1.07700

Collected Steps per Second: 21,381.10938
Overall Steps per Second: 10,537.26662

Timestep Collection Time: 2.33945
Timestep Consumption Time: 2.40751
PPO Batch Consumption Time: 0.14445
Total Iteration Time: 4.74696

Cumulative Model Updates: 4,112
Cumulative Timesteps: 17,209,446

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 229.80202
Policy Entropy: 0.68515
Value Function Loss: 4.31830

Mean KL Divergence: 0.02990
SB3 Clip Fraction: 0.18164
Policy Update Magnitude: 0.93673
Value Function Update Magnitude: 1.04029

Collected Steps per Second: 20,333.71696
Overall Steps per Second: 10,152.51308

Timestep Collection Time: 2.45897
Timestep Consumption Time: 2.46592
PPO Batch Consumption Time: 0.14644
Total Iteration Time: 4.92489

Cumulative Model Updates: 4,124
Cumulative Timesteps: 17,259,446

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.00277
Policy Entropy: 0.75709
Value Function Loss: 4.42312

Mean KL Divergence: 0.02509
SB3 Clip Fraction: 0.18186
Policy Update Magnitude: 0.92936
Value Function Update Magnitude: 1.00691

Collected Steps per Second: 21,286.78682
Overall Steps per Second: 10,519.68850

Timestep Collection Time: 2.35104
Timestep Consumption Time: 2.40633
PPO Batch Consumption Time: 0.14449
Total Iteration Time: 4.75737

Cumulative Model Updates: 4,136
Cumulative Timesteps: 17,309,492

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 255.54404
Policy Entropy: 0.82734
Value Function Loss: 4.52097

Mean KL Divergence: 0.02770
SB3 Clip Fraction: 0.20093
Policy Update Magnitude: 0.93648
Value Function Update Magnitude: 1.06341

Collected Steps per Second: 21,956.02975
Overall Steps per Second: 10,553.29677

Timestep Collection Time: 2.27883
Timestep Consumption Time: 2.46225
PPO Batch Consumption Time: 0.14690
Total Iteration Time: 4.74108

Cumulative Model Updates: 4,148
Cumulative Timesteps: 17,359,526

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.38592
Policy Entropy: 0.90823
Value Function Loss: 4.50015

Mean KL Divergence: 0.03508
SB3 Clip Fraction: 0.23435
Policy Update Magnitude: 1.07573
Value Function Update Magnitude: 1.03102

Collected Steps per Second: 20,918.72713
Overall Steps per Second: 10,332.49272

Timestep Collection Time: 2.39030
Timestep Consumption Time: 2.44900
PPO Batch Consumption Time: 0.14773
Total Iteration Time: 4.83930

Cumulative Model Updates: 4,160
Cumulative Timesteps: 17,409,528

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 293.85459
Policy Entropy: 0.92223
Value Function Loss: 4.34257

Mean KL Divergence: 0.03356
SB3 Clip Fraction: 0.23311
Policy Update Magnitude: 1.00189
Value Function Update Magnitude: 0.98673

Collected Steps per Second: 21,094.38870
Overall Steps per Second: 10,463.09250

Timestep Collection Time: 2.37030
Timestep Consumption Time: 2.40840
PPO Batch Consumption Time: 0.14482
Total Iteration Time: 4.77870

Cumulative Model Updates: 4,172
Cumulative Timesteps: 17,459,528

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 271.60050
Policy Entropy: 0.97411
Value Function Loss: 4.35311

Mean KL Divergence: 0.03093
SB3 Clip Fraction: 0.22794
Policy Update Magnitude: 1.16348
Value Function Update Magnitude: 0.91546

Collected Steps per Second: 19,843.72560
Overall Steps per Second: 10,056.00017

Timestep Collection Time: 2.51999
Timestep Consumption Time: 2.45276
PPO Batch Consumption Time: 0.14510
Total Iteration Time: 4.97275

Cumulative Model Updates: 4,184
Cumulative Timesteps: 17,509,534

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.80043
Policy Entropy: 1.00982
Value Function Loss: 4.41666

Mean KL Divergence: 0.03131
SB3 Clip Fraction: 0.23461
Policy Update Magnitude: 1.14008
Value Function Update Magnitude: 0.90758

Collected Steps per Second: 21,594.43376
Overall Steps per Second: 10,587.17947

Timestep Collection Time: 2.31717
Timestep Consumption Time: 2.40911
PPO Batch Consumption Time: 0.14478
Total Iteration Time: 4.72628

Cumulative Model Updates: 4,196
Cumulative Timesteps: 17,559,572

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 234.12795
Policy Entropy: 1.10085
Value Function Loss: 4.58781

Mean KL Divergence: 0.03194
SB3 Clip Fraction: 0.24427
Policy Update Magnitude: 1.21470
Value Function Update Magnitude: 0.97230

Collected Steps per Second: 22,354.39735
Overall Steps per Second: 10,674.94738

Timestep Collection Time: 2.23696
Timestep Consumption Time: 2.44746
PPO Batch Consumption Time: 0.14520
Total Iteration Time: 4.68443

Cumulative Model Updates: 4,208
Cumulative Timesteps: 17,609,578

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 254.42415
Policy Entropy: 1.19142
Value Function Loss: 4.65089

Mean KL Divergence: 0.03358
SB3 Clip Fraction: 0.25931
Policy Update Magnitude: 1.29275
Value Function Update Magnitude: 1.00766

Collected Steps per Second: 22,183.14214
Overall Steps per Second: 10,629.81280

Timestep Collection Time: 2.25577
Timestep Consumption Time: 2.45175
PPO Batch Consumption Time: 0.14561
Total Iteration Time: 4.70751

Cumulative Model Updates: 4,220
Cumulative Timesteps: 17,659,618

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 234.29272
Policy Entropy: 1.24568
Value Function Loss: 4.81020

Mean KL Divergence: 0.03172
SB3 Clip Fraction: 0.26087
Policy Update Magnitude: 1.05824
Value Function Update Magnitude: 1.00404

Collected Steps per Second: 23,867.49049
Overall Steps per Second: 10,934.52341

Timestep Collection Time: 2.09607
Timestep Consumption Time: 2.47916
PPO Batch Consumption Time: 0.14643
Total Iteration Time: 4.57523

Cumulative Model Updates: 4,232
Cumulative Timesteps: 17,709,646

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 194.28328
Policy Entropy: 1.26704
Value Function Loss: 4.78634

Mean KL Divergence: 0.02601
SB3 Clip Fraction: 0.23413
Policy Update Magnitude: 1.01565
Value Function Update Magnitude: 1.03915

Collected Steps per Second: 21,752.77925
Overall Steps per Second: 10,524.92451

Timestep Collection Time: 2.29929
Timestep Consumption Time: 2.45286
PPO Batch Consumption Time: 0.14509
Total Iteration Time: 4.75215

Cumulative Model Updates: 4,244
Cumulative Timesteps: 17,759,662

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 223.07884
Policy Entropy: 1.29219
Value Function Loss: 4.66062

Mean KL Divergence: 0.02280
SB3 Clip Fraction: 0.21829
Policy Update Magnitude: 1.11628
Value Function Update Magnitude: 1.12041

Collected Steps per Second: 21,187.35076
Overall Steps per Second: 10,503.28709

Timestep Collection Time: 2.36056
Timestep Consumption Time: 2.40119
PPO Batch Consumption Time: 0.14611
Total Iteration Time: 4.76175

Cumulative Model Updates: 4,256
Cumulative Timesteps: 17,809,676

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 219.22983
Policy Entropy: 1.31919
Value Function Loss: 4.41730

Mean KL Divergence: 0.02148
SB3 Clip Fraction: 0.21620
Policy Update Magnitude: 1.08480
Value Function Update Magnitude: 1.15449

Collected Steps per Second: 20,979.87835
Overall Steps per Second: 10,388.27577

Timestep Collection Time: 2.38409
Timestep Consumption Time: 2.43076
PPO Batch Consumption Time: 0.14498
Total Iteration Time: 4.81485

Cumulative Model Updates: 4,268
Cumulative Timesteps: 17,859,694

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 270.63776
Policy Entropy: 1.35751
Value Function Loss: 4.27426

Mean KL Divergence: 0.02080
SB3 Clip Fraction: 0.21069
Policy Update Magnitude: 1.09334
Value Function Update Magnitude: 1.13013

Collected Steps per Second: 21,437.28198
Overall Steps per Second: 10,487.24663

Timestep Collection Time: 2.33313
Timestep Consumption Time: 2.43609
PPO Batch Consumption Time: 0.14526
Total Iteration Time: 4.76922

Cumulative Model Updates: 4,280
Cumulative Timesteps: 17,909,710

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.26775
Policy Entropy: 1.38976
Value Function Loss: 4.15697

Mean KL Divergence: 0.01990
SB3 Clip Fraction: 0.20667
Policy Update Magnitude: 1.15963
Value Function Update Magnitude: 1.11725

Collected Steps per Second: 19,637.55543
Overall Steps per Second: 10,076.22373

Timestep Collection Time: 2.54645
Timestep Consumption Time: 2.41632
PPO Batch Consumption Time: 0.14780
Total Iteration Time: 4.96277

Cumulative Model Updates: 4,292
Cumulative Timesteps: 17,959,716

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.50419
Policy Entropy: 1.41769
Value Function Loss: 4.16881

Mean KL Divergence: 0.02061
SB3 Clip Fraction: 0.21115
Policy Update Magnitude: 1.19529
Value Function Update Magnitude: 1.10619

Collected Steps per Second: 21,393.94266
Overall Steps per Second: 10,397.49981

Timestep Collection Time: 2.33786
Timestep Consumption Time: 2.47253
PPO Batch Consumption Time: 0.14391
Total Iteration Time: 4.81039

Cumulative Model Updates: 4,304
Cumulative Timesteps: 18,009,732

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 255.98593
Policy Entropy: 1.45459
Value Function Loss: 4.32346

Mean KL Divergence: 0.02211
SB3 Clip Fraction: 0.21863
Policy Update Magnitude: 1.21246
Value Function Update Magnitude: 1.09863

Collected Steps per Second: 21,290.71874
Overall Steps per Second: 10,633.57641

Timestep Collection Time: 2.34882
Timestep Consumption Time: 2.35402
PPO Batch Consumption Time: 0.14532
Total Iteration Time: 4.70284

Cumulative Model Updates: 4,316
Cumulative Timesteps: 18,059,740

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 201.79547
Policy Entropy: 1.50611
Value Function Loss: 4.27985

Mean KL Divergence: 0.02260
SB3 Clip Fraction: 0.21859
Policy Update Magnitude: 1.45249
Value Function Update Magnitude: 1.10286

Collected Steps per Second: 23,517.99313
Overall Steps per Second: 11,001.23971

Timestep Collection Time: 2.12697
Timestep Consumption Time: 2.41997
PPO Batch Consumption Time: 0.14423
Total Iteration Time: 4.54694

Cumulative Model Updates: 4,328
Cumulative Timesteps: 18,109,762

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.31842
Policy Entropy: 1.61920
Value Function Loss: 4.21649

Mean KL Divergence: 0.03093
SB3 Clip Fraction: 0.27156
Policy Update Magnitude: 1.68712
Value Function Update Magnitude: 1.18564

Collected Steps per Second: 22,576.76860
Overall Steps per Second: 10,839.01467

Timestep Collection Time: 2.21546
Timestep Consumption Time: 2.39916
PPO Batch Consumption Time: 0.14491
Total Iteration Time: 4.61463

Cumulative Model Updates: 4,340
Cumulative Timesteps: 18,159,780

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 257.96902
Policy Entropy: 1.69972
Value Function Loss: 4.21732

Mean KL Divergence: 0.03544
SB3 Clip Fraction: 0.29251
Policy Update Magnitude: 1.77880
Value Function Update Magnitude: 1.09852

Collected Steps per Second: 23,584.74558
Overall Steps per Second: 10,702.53645

Timestep Collection Time: 2.12163
Timestep Consumption Time: 2.55371
PPO Batch Consumption Time: 0.15641
Total Iteration Time: 4.67534

Cumulative Model Updates: 4,352
Cumulative Timesteps: 18,209,818

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.54021
Policy Entropy: 1.79623
Value Function Loss: 4.32875

Mean KL Divergence: 0.04192
SB3 Clip Fraction: 0.32568
Policy Update Magnitude: 1.75016
Value Function Update Magnitude: 1.18887

Collected Steps per Second: 19,033.07330
Overall Steps per Second: 9,446.57306

Timestep Collection Time: 2.62816
Timestep Consumption Time: 2.66709
PPO Batch Consumption Time: 0.16510
Total Iteration Time: 5.29525

Cumulative Model Updates: 4,364
Cumulative Timesteps: 18,259,840

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 233.29736
Policy Entropy: 1.90117
Value Function Loss: 4.38771

Mean KL Divergence: 0.04005
SB3 Clip Fraction: 0.33184
Policy Update Magnitude: 1.38552
Value Function Update Magnitude: 1.18281

Collected Steps per Second: 21,839.60860
Overall Steps per Second: 10,372.90421

Timestep Collection Time: 2.29107
Timestep Consumption Time: 2.53265
PPO Batch Consumption Time: 0.15631
Total Iteration Time: 4.82372

Cumulative Model Updates: 4,376
Cumulative Timesteps: 18,309,876

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 222.92356
Policy Entropy: 2.00570
Value Function Loss: 4.23771

Mean KL Divergence: 0.03709
SB3 Clip Fraction: 0.32664
Policy Update Magnitude: 1.53651
Value Function Update Magnitude: 1.15145

Collected Steps per Second: 22,719.73894
Overall Steps per Second: 10,780.80127

Timestep Collection Time: 2.20311
Timestep Consumption Time: 2.43978
PPO Batch Consumption Time: 0.14523
Total Iteration Time: 4.64288

Cumulative Model Updates: 4,388
Cumulative Timesteps: 18,359,930

Timesteps Collected: 50,054
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.92885
Policy Entropy: 2.08350
Value Function Loss: 3.97221

Mean KL Divergence: 0.03510
SB3 Clip Fraction: 0.32522
Policy Update Magnitude: 1.35222
Value Function Update Magnitude: 1.17269

Collected Steps per Second: 21,391.62083
Overall Steps per Second: 10,499.49995

Timestep Collection Time: 2.33895
Timestep Consumption Time: 2.42642
PPO Batch Consumption Time: 0.14610
Total Iteration Time: 4.76537

Cumulative Model Updates: 4,400
Cumulative Timesteps: 18,409,964

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 195.14615
Policy Entropy: 2.14002
Value Function Loss: 3.82141

Mean KL Divergence: 0.03197
SB3 Clip Fraction: 0.31422
Policy Update Magnitude: 1.40910
Value Function Update Magnitude: 1.18820

Collected Steps per Second: 22,763.47950
Overall Steps per Second: 10,788.88682

Timestep Collection Time: 2.19949
Timestep Consumption Time: 2.44121
PPO Batch Consumption Time: 0.14486
Total Iteration Time: 4.64070

Cumulative Model Updates: 4,412
Cumulative Timesteps: 18,460,032

Timesteps Collected: 50,068
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.42475
Policy Entropy: 2.17278
Value Function Loss: 3.98206

Mean KL Divergence: 0.03113
SB3 Clip Fraction: 0.31180
Policy Update Magnitude: 1.52713
Value Function Update Magnitude: 1.15630

Collected Steps per Second: 23,796.62409
Overall Steps per Second: 10,877.15359

Timestep Collection Time: 2.10156
Timestep Consumption Time: 2.49615
PPO Batch Consumption Time: 0.15110
Total Iteration Time: 4.59771

Cumulative Model Updates: 4,424
Cumulative Timesteps: 18,510,042

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 229.36403
Policy Entropy: 2.21480
Value Function Loss: 4.11883

Mean KL Divergence: 0.03042
SB3 Clip Fraction: 0.30897
Policy Update Magnitude: 1.52492
Value Function Update Magnitude: 1.33236

Collected Steps per Second: 23,303.93821
Overall Steps per Second: 10,824.54359

Timestep Collection Time: 2.14556
Timestep Consumption Time: 2.47357
PPO Batch Consumption Time: 0.14933
Total Iteration Time: 4.61913

Cumulative Model Updates: 4,436
Cumulative Timesteps: 18,560,042

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 165.73461
Policy Entropy: 2.23928
Value Function Loss: 4.10691

Mean KL Divergence: 0.02963
SB3 Clip Fraction: 0.30490
Policy Update Magnitude: 1.63126
Value Function Update Magnitude: 1.24400

Collected Steps per Second: 23,488.16654
Overall Steps per Second: 10,995.18521

Timestep Collection Time: 2.12873
Timestep Consumption Time: 2.41871
PPO Batch Consumption Time: 0.14431
Total Iteration Time: 4.54744

Cumulative Model Updates: 4,448
Cumulative Timesteps: 18,610,042

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 177.64529
Policy Entropy: 2.28436
Value Function Loss: 3.89987

Mean KL Divergence: 0.02975
SB3 Clip Fraction: 0.31453
Policy Update Magnitude: 1.46135
Value Function Update Magnitude: 1.16991

Collected Steps per Second: 23,816.05727
Overall Steps per Second: 11,127.00355

Timestep Collection Time: 2.10127
Timestep Consumption Time: 2.39626
PPO Batch Consumption Time: 0.14371
Total Iteration Time: 4.49753

Cumulative Model Updates: 4,460
Cumulative Timesteps: 18,660,086

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 267.63898
Policy Entropy: 2.30969
Value Function Loss: 3.71439

Mean KL Divergence: 0.02819
SB3 Clip Fraction: 0.30024
Policy Update Magnitude: 1.43126
Value Function Update Magnitude: 1.10263

Collected Steps per Second: 24,055.19201
Overall Steps per Second: 11,099.74004

Timestep Collection Time: 2.07963
Timestep Consumption Time: 2.42732
PPO Batch Consumption Time: 0.14524
Total Iteration Time: 4.50695

Cumulative Model Updates: 4,472
Cumulative Timesteps: 18,710,112

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 201.69158
Policy Entropy: 2.33958
Value Function Loss: 3.80882

Mean KL Divergence: 0.02656
SB3 Clip Fraction: 0.29430
Policy Update Magnitude: 1.44522
Value Function Update Magnitude: 1.09859

Collected Steps per Second: 22,765.56718
Overall Steps per Second: 10,855.03932

Timestep Collection Time: 2.19709
Timestep Consumption Time: 2.41072
PPO Batch Consumption Time: 0.14457
Total Iteration Time: 4.60781

Cumulative Model Updates: 4,484
Cumulative Timesteps: 18,760,130

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 204.50296
Policy Entropy: 2.35692
Value Function Loss: 3.89573

Mean KL Divergence: 0.02545
SB3 Clip Fraction: 0.28468
Policy Update Magnitude: 1.43828
Value Function Update Magnitude: 1.11240

Collected Steps per Second: 23,156.92098
Overall Steps per Second: 10,973.04172

Timestep Collection Time: 2.15918
Timestep Consumption Time: 2.39744
PPO Batch Consumption Time: 0.14391
Total Iteration Time: 4.55662

Cumulative Model Updates: 4,496
Cumulative Timesteps: 18,810,130

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.12796
Policy Entropy: 2.37881
Value Function Loss: 3.99360

Mean KL Divergence: 0.02605
SB3 Clip Fraction: 0.29137
Policy Update Magnitude: 1.49592
Value Function Update Magnitude: 1.21527

Collected Steps per Second: 21,339.83079
Overall Steps per Second: 10,472.10432

Timestep Collection Time: 2.34397
Timestep Consumption Time: 2.43253
PPO Batch Consumption Time: 0.14519
Total Iteration Time: 4.77650

Cumulative Model Updates: 4,508
Cumulative Timesteps: 18,860,150

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.73040
Policy Entropy: 2.39966
Value Function Loss: 3.77141

Mean KL Divergence: 0.02621
SB3 Clip Fraction: 0.29476
Policy Update Magnitude: 1.42048
Value Function Update Magnitude: 1.20294

Collected Steps per Second: 21,094.29168
Overall Steps per Second: 10,374.94653

Timestep Collection Time: 2.37183
Timestep Consumption Time: 2.45056
PPO Batch Consumption Time: 0.14621
Total Iteration Time: 4.82239

Cumulative Model Updates: 4,520
Cumulative Timesteps: 18,910,182

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 240.82298
Policy Entropy: 2.42212
Value Function Loss: 3.86855

Mean KL Divergence: 0.02639
SB3 Clip Fraction: 0.29617
Policy Update Magnitude: 1.54847
Value Function Update Magnitude: 1.15119

Collected Steps per Second: 21,381.22454
Overall Steps per Second: 10,434.13597

Timestep Collection Time: 2.33990
Timestep Consumption Time: 2.45494
PPO Batch Consumption Time: 0.14735
Total Iteration Time: 4.79484

Cumulative Model Updates: 4,532
Cumulative Timesteps: 18,960,212

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 207.41906
Policy Entropy: 2.44482
Value Function Loss: 3.89523

Mean KL Divergence: 0.02681
SB3 Clip Fraction: 0.30166
Policy Update Magnitude: 1.49169
Value Function Update Magnitude: 1.15861

Collected Steps per Second: 19,820.45106
Overall Steps per Second: 10,144.90692

Timestep Collection Time: 2.52557
Timestep Consumption Time: 2.40873
PPO Batch Consumption Time: 0.14475
Total Iteration Time: 4.93430

Cumulative Model Updates: 4,544
Cumulative Timesteps: 19,010,270

Timesteps Collected: 50,058
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 171.76381
Policy Entropy: 2.45375
Value Function Loss: 3.93443

Mean KL Divergence: 0.02723
SB3 Clip Fraction: 0.30333
Policy Update Magnitude: 1.50973
Value Function Update Magnitude: 1.18704

Collected Steps per Second: 21,451.86577
Overall Steps per Second: 10,466.98684

Timestep Collection Time: 2.33257
Timestep Consumption Time: 2.44798
PPO Batch Consumption Time: 0.14468
Total Iteration Time: 4.78055

Cumulative Model Updates: 4,556
Cumulative Timesteps: 19,060,308

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 207.73914
Policy Entropy: 2.48120
Value Function Loss: 3.94190

Mean KL Divergence: 0.02935
SB3 Clip Fraction: 0.32160
Policy Update Magnitude: 1.51660
Value Function Update Magnitude: 1.21837

Collected Steps per Second: 20,750.00364
Overall Steps per Second: 10,480.11701

Timestep Collection Time: 2.41118
Timestep Consumption Time: 2.36281
PPO Batch Consumption Time: 0.14680
Total Iteration Time: 4.77399

Cumulative Model Updates: 4,568
Cumulative Timesteps: 19,110,340

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 190.99173
Policy Entropy: 2.50143
Value Function Loss: 4.00155

Mean KL Divergence: 0.02967
SB3 Clip Fraction: 0.32341
Policy Update Magnitude: 1.53035
Value Function Update Magnitude: 1.23685

Collected Steps per Second: 20,793.95674
Overall Steps per Second: 10,279.10197

Timestep Collection Time: 2.40599
Timestep Consumption Time: 2.46117
PPO Batch Consumption Time: 0.14475
Total Iteration Time: 4.86716

Cumulative Model Updates: 4,580
Cumulative Timesteps: 19,160,370

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 173.73702
Policy Entropy: 2.52467
Value Function Loss: 4.07378

Mean KL Divergence: 0.03011
SB3 Clip Fraction: 0.32795
Policy Update Magnitude: 1.64862
Value Function Update Magnitude: 1.21530

Collected Steps per Second: 21,793.39458
Overall Steps per Second: 10,672.73634

Timestep Collection Time: 2.29537
Timestep Consumption Time: 2.39171
PPO Batch Consumption Time: 0.14468
Total Iteration Time: 4.68708

Cumulative Model Updates: 4,592
Cumulative Timesteps: 19,210,394

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 186.38308
Policy Entropy: 2.54436
Value Function Loss: 3.99463

Mean KL Divergence: 0.03166
SB3 Clip Fraction: 0.33671
Policy Update Magnitude: 1.60445
Value Function Update Magnitude: 1.14395

Collected Steps per Second: 20,520.69563
Overall Steps per Second: 10,240.46635

Timestep Collection Time: 2.43832
Timestep Consumption Time: 2.44779
PPO Batch Consumption Time: 0.14692
Total Iteration Time: 4.88611

Cumulative Model Updates: 4,604
Cumulative Timesteps: 19,260,430

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.26979
Policy Entropy: 2.58188
Value Function Loss: 3.90288

Mean KL Divergence: 0.03381
SB3 Clip Fraction: 0.35296
Policy Update Magnitude: 1.63589
Value Function Update Magnitude: 1.15309

Collected Steps per Second: 21,768.91127
Overall Steps per Second: 10,572.01254

Timestep Collection Time: 2.29933
Timestep Consumption Time: 2.43524
PPO Batch Consumption Time: 0.14490
Total Iteration Time: 4.73458

Cumulative Model Updates: 4,616
Cumulative Timesteps: 19,310,484

Timesteps Collected: 50,054
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.43049
Policy Entropy: 2.61366
Value Function Loss: 3.81070

Mean KL Divergence: 0.03210
SB3 Clip Fraction: 0.34032
Policy Update Magnitude: 1.81752
Value Function Update Magnitude: 1.16078

Collected Steps per Second: 21,004.31115
Overall Steps per Second: 10,503.88399

Timestep Collection Time: 2.38104
Timestep Consumption Time: 2.38025
PPO Batch Consumption Time: 0.14405
Total Iteration Time: 4.76129

Cumulative Model Updates: 4,628
Cumulative Timesteps: 19,360,496

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.78435
Policy Entropy: 2.65520
Value Function Loss: 3.76487

Mean KL Divergence: 0.03297
SB3 Clip Fraction: 0.35269
Policy Update Magnitude: 1.92805
Value Function Update Magnitude: 1.15103

Collected Steps per Second: 21,769.64981
Overall Steps per Second: 10,502.42050

Timestep Collection Time: 2.29797
Timestep Consumption Time: 2.46531
PPO Batch Consumption Time: 0.14714
Total Iteration Time: 4.76328

Cumulative Model Updates: 4,640
Cumulative Timesteps: 19,410,522

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.27411
Policy Entropy: 2.70773
Value Function Loss: 3.74247

Mean KL Divergence: 0.03591
SB3 Clip Fraction: 0.37143
Policy Update Magnitude: 1.81561
Value Function Update Magnitude: 1.10450

Collected Steps per Second: 20,676.06963
Overall Steps per Second: 10,391.21142

Timestep Collection Time: 2.42058
Timestep Consumption Time: 2.39580
PPO Batch Consumption Time: 0.14447
Total Iteration Time: 4.81638

Cumulative Model Updates: 4,652
Cumulative Timesteps: 19,460,570

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 174.96403
Policy Entropy: 2.74400
Value Function Loss: 3.71252

Mean KL Divergence: 0.03506
SB3 Clip Fraction: 0.36826
Policy Update Magnitude: 1.68251
Value Function Update Magnitude: 1.47520

Collected Steps per Second: 21,255.97014
Overall Steps per Second: 10,433.75517

Timestep Collection Time: 2.35237
Timestep Consumption Time: 2.43996
PPO Batch Consumption Time: 0.14466
Total Iteration Time: 4.79233

Cumulative Model Updates: 4,664
Cumulative Timesteps: 19,510,572

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 175.63218
Policy Entropy: 2.79239
Value Function Loss: 3.71856

Mean KL Divergence: 0.03188
SB3 Clip Fraction: 0.35151
Policy Update Magnitude: 1.58988
Value Function Update Magnitude: 1.49870

Collected Steps per Second: 21,397.77478
Overall Steps per Second: 10,496.24129

Timestep Collection Time: 2.33922
Timestep Consumption Time: 2.42954
PPO Batch Consumption Time: 0.14609
Total Iteration Time: 4.76875

Cumulative Model Updates: 4,676
Cumulative Timesteps: 19,560,626

Timesteps Collected: 50,054
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 199.36186
Policy Entropy: 2.83766
Value Function Loss: 3.61062

Mean KL Divergence: 0.03037
SB3 Clip Fraction: 0.34552
Policy Update Magnitude: 1.65151
Value Function Update Magnitude: 1.34006

Collected Steps per Second: 19,967.23943
Overall Steps per Second: 10,067.65794

Timestep Collection Time: 2.50631
Timestep Consumption Time: 2.46446
PPO Batch Consumption Time: 0.15039
Total Iteration Time: 4.97077

Cumulative Model Updates: 4,688
Cumulative Timesteps: 19,610,670

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 189.93296
Policy Entropy: 2.88850
Value Function Loss: 3.71820

Mean KL Divergence: 0.03161
SB3 Clip Fraction: 0.35753
Policy Update Magnitude: 1.69490
Value Function Update Magnitude: 1.19246

Collected Steps per Second: 18,970.35656
Overall Steps per Second: 9,847.56448

Timestep Collection Time: 2.63759
Timestep Consumption Time: 2.44346
PPO Batch Consumption Time: 0.14328
Total Iteration Time: 5.08105

Cumulative Model Updates: 4,700
Cumulative Timesteps: 19,660,706

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 179.82431
Policy Entropy: 2.92679
Value Function Loss: 3.67430

Mean KL Divergence: 0.03158
SB3 Clip Fraction: 0.35860
Policy Update Magnitude: 1.62814
Value Function Update Magnitude: 1.15751

Collected Steps per Second: 22,048.04039
Overall Steps per Second: 10,869.57749

Timestep Collection Time: 2.26995
Timestep Consumption Time: 2.33446
PPO Batch Consumption Time: 0.13747
Total Iteration Time: 4.60441

Cumulative Model Updates: 4,712
Cumulative Timesteps: 19,710,754

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 156.20924
Policy Entropy: 2.95033
Value Function Loss: 3.55529

Mean KL Divergence: 0.03169
SB3 Clip Fraction: 0.35757
Policy Update Magnitude: 1.84838
Value Function Update Magnitude: 1.22544

Collected Steps per Second: 21,103.06465
Overall Steps per Second: 10,828.67158

Timestep Collection Time: 2.37008
Timestep Consumption Time: 2.24877
PPO Batch Consumption Time: 0.13796
Total Iteration Time: 4.61885

Cumulative Model Updates: 4,724
Cumulative Timesteps: 19,760,770

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.00039
Policy Entropy: 2.98321
Value Function Loss: 3.41225

Mean KL Divergence: 0.03455
SB3 Clip Fraction: 0.38063
Policy Update Magnitude: 1.89307
Value Function Update Magnitude: 1.19357

Collected Steps per Second: 22,759.11018
Overall Steps per Second: 10,777.85907

Timestep Collection Time: 2.19710
Timestep Consumption Time: 2.44241
PPO Batch Consumption Time: 0.14427
Total Iteration Time: 4.63951

Cumulative Model Updates: 4,736
Cumulative Timesteps: 19,810,774

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 178.06049
Policy Entropy: 3.00155
Value Function Loss: 3.41948

Mean KL Divergence: 0.03565
SB3 Clip Fraction: 0.38209
Policy Update Magnitude: 1.86564
Value Function Update Magnitude: 1.16043

Collected Steps per Second: 19,482.86992
Overall Steps per Second: 9,816.16217

Timestep Collection Time: 2.56708
Timestep Consumption Time: 2.52799
PPO Batch Consumption Time: 0.14440
Total Iteration Time: 5.09507

Cumulative Model Updates: 4,748
Cumulative Timesteps: 19,860,788

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 201.54483
Policy Entropy: 3.02406
Value Function Loss: 3.43583

Mean KL Divergence: 0.03483
SB3 Clip Fraction: 0.37697
Policy Update Magnitude: 1.87921
Value Function Update Magnitude: 1.21793

Collected Steps per Second: 19,611.93610
Overall Steps per Second: 9,765.37000

Timestep Collection Time: 2.54957
Timestep Consumption Time: 2.57077
PPO Batch Consumption Time: 0.15054
Total Iteration Time: 5.12034

Cumulative Model Updates: 4,760
Cumulative Timesteps: 19,910,790

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 184.41899
Policy Entropy: 3.04326
Value Function Loss: 3.50382

Mean KL Divergence: 0.03473
SB3 Clip Fraction: 0.37989
Policy Update Magnitude: 1.85086
Value Function Update Magnitude: 1.20004

Collected Steps per Second: 21,588.31260
Overall Steps per Second: 10,734.30534

Timestep Collection Time: 2.31672
Timestep Consumption Time: 2.34255
PPO Batch Consumption Time: 0.14085
Total Iteration Time: 4.65927

Cumulative Model Updates: 4,772
Cumulative Timesteps: 19,960,804

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 182.01780
Policy Entropy: 3.07644
Value Function Loss: 3.45060

Mean KL Divergence: 0.03507
SB3 Clip Fraction: 0.38571
Policy Update Magnitude: 1.81026
Value Function Update Magnitude: 1.15951

Collected Steps per Second: 24,724.02158
Overall Steps per Second: 11,350.11648

Timestep Collection Time: 2.02370
Timestep Consumption Time: 2.38454
PPO Batch Consumption Time: 0.14274
Total Iteration Time: 4.40824

Cumulative Model Updates: 4,784
Cumulative Timesteps: 20,010,838

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 151.02373
Policy Entropy: 3.09170
Value Function Loss: 3.40656

Mean KL Divergence: 0.03484
SB3 Clip Fraction: 0.38242
Policy Update Magnitude: 1.79214
Value Function Update Magnitude: 1.17540

Collected Steps per Second: 23,485.33593
Overall Steps per Second: 11,043.51482

Timestep Collection Time: 2.13061
Timestep Consumption Time: 2.40038
PPO Batch Consumption Time: 0.14555
Total Iteration Time: 4.53098

Cumulative Model Updates: 4,796
Cumulative Timesteps: 20,060,876

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 179.36141
Policy Entropy: 3.10578
Value Function Loss: 3.46102

Mean KL Divergence: 0.03510
SB3 Clip Fraction: 0.38345
Policy Update Magnitude: 1.80422
Value Function Update Magnitude: 1.20142

Collected Steps per Second: 19,051.15558
Overall Steps per Second: 9,733.67375

Timestep Collection Time: 2.62619
Timestep Consumption Time: 2.51390
PPO Batch Consumption Time: 0.14917
Total Iteration Time: 5.14009

Cumulative Model Updates: 4,808
Cumulative Timesteps: 20,110,908

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 163.32973
Policy Entropy: 3.12585
Value Function Loss: 3.48158

Mean KL Divergence: 0.03469
SB3 Clip Fraction: 0.38077
Policy Update Magnitude: 1.79872
Value Function Update Magnitude: 1.26292

Collected Steps per Second: 22,204.14915
Overall Steps per Second: 10,585.77894

Timestep Collection Time: 2.25381
Timestep Consumption Time: 2.47366
PPO Batch Consumption Time: 0.14066
Total Iteration Time: 4.72747

Cumulative Model Updates: 4,820
Cumulative Timesteps: 20,160,952

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.17719
Policy Entropy: 3.13649
Value Function Loss: 3.64293

Mean KL Divergence: 0.03635
SB3 Clip Fraction: 0.39070
Policy Update Magnitude: 1.80211
Value Function Update Magnitude: 1.25201

Collected Steps per Second: 22,018.96281
Overall Steps per Second: 10,726.08313

Timestep Collection Time: 2.27286
Timestep Consumption Time: 2.39296
PPO Batch Consumption Time: 0.14492
Total Iteration Time: 4.66582

Cumulative Model Updates: 4,832
Cumulative Timesteps: 20,210,998

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 158.50154
Policy Entropy: 3.15026
Value Function Loss: 3.52993

Mean KL Divergence: 0.03532
SB3 Clip Fraction: 0.38607
Policy Update Magnitude: 1.81042
Value Function Update Magnitude: 1.36066

Collected Steps per Second: 17,311.68259
Overall Steps per Second: 8,833.43240

Timestep Collection Time: 2.88938
Timestep Consumption Time: 2.77320
PPO Batch Consumption Time: 0.16642
Total Iteration Time: 5.66258

Cumulative Model Updates: 4,844
Cumulative Timesteps: 20,261,018

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 167.90963
Policy Entropy: 3.15952
Value Function Loss: 3.53113

Mean KL Divergence: 0.03599
SB3 Clip Fraction: 0.39197
Policy Update Magnitude: 1.82253
Value Function Update Magnitude: 1.43441

Collected Steps per Second: 21,692.00031
Overall Steps per Second: 10,460.52779

Timestep Collection Time: 2.30573
Timestep Consumption Time: 2.47567
PPO Batch Consumption Time: 0.14592
Total Iteration Time: 4.78140

Cumulative Model Updates: 4,856
Cumulative Timesteps: 20,311,034

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.45963
Policy Entropy: 3.17887
Value Function Loss: 3.42834

Mean KL Divergence: 0.03558
SB3 Clip Fraction: 0.39108
Policy Update Magnitude: 1.79730
Value Function Update Magnitude: 1.49615

Collected Steps per Second: 14,446.44474
Overall Steps per Second: 7,561.12947

Timestep Collection Time: 3.46410
Timestep Consumption Time: 3.15448
PPO Batch Consumption Time: 0.15768
Total Iteration Time: 6.61859

Cumulative Model Updates: 4,868
Cumulative Timesteps: 20,361,078

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.76425
Policy Entropy: 3.18596
Value Function Loss: 3.52088

Mean KL Divergence: 0.03561
SB3 Clip Fraction: 0.39278
Policy Update Magnitude: 1.81042
Value Function Update Magnitude: 1.41616

Collected Steps per Second: 12,849.00647
Overall Steps per Second: 7,010.73264

Timestep Collection Time: 3.89695
Timestep Consumption Time: 3.24524
PPO Batch Consumption Time: 0.15519
Total Iteration Time: 7.14219

Cumulative Model Updates: 4,880
Cumulative Timesteps: 20,411,150

Timesteps Collected: 50,072
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 165.32341
Policy Entropy: 3.18130
Value Function Loss: 3.66972

Mean KL Divergence: 0.03585
SB3 Clip Fraction: 0.38895
Policy Update Magnitude: 1.82236
Value Function Update Magnitude: 1.44426

Collected Steps per Second: 14,092.50193
Overall Steps per Second: 7,693.42168

Timestep Collection Time: 3.55068
Timestep Consumption Time: 2.95332
PPO Batch Consumption Time: 0.15469
Total Iteration Time: 6.50400

Cumulative Model Updates: 4,892
Cumulative Timesteps: 20,461,188

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 180.71144
Policy Entropy: 3.19070
Value Function Loss: 3.69245

Mean KL Divergence: 0.03532
SB3 Clip Fraction: 0.38553
Policy Update Magnitude: 1.81664
Value Function Update Magnitude: 1.49604

Collected Steps per Second: 14,725.68893
Overall Steps per Second: 7,754.38097

Timestep Collection Time: 3.39719
Timestep Consumption Time: 3.05413
PPO Batch Consumption Time: 0.16074
Total Iteration Time: 6.45132

Cumulative Model Updates: 4,904
Cumulative Timesteps: 20,511,214

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 187.67075
Policy Entropy: 3.20368
Value Function Loss: 3.68122

Mean KL Divergence: 0.03657
SB3 Clip Fraction: 0.39624
Policy Update Magnitude: 1.85888
Value Function Update Magnitude: 1.55930

Collected Steps per Second: 14,889.90519
Overall Steps per Second: 8,213.18891

Timestep Collection Time: 3.36188
Timestep Consumption Time: 2.73296
PPO Batch Consumption Time: 0.14810
Total Iteration Time: 6.09483

Cumulative Model Updates: 4,916
Cumulative Timesteps: 20,561,272

Timesteps Collected: 50,058
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 176.33493
Policy Entropy: 3.21608
Value Function Loss: 3.39945

Mean KL Divergence: 0.03603
SB3 Clip Fraction: 0.39297
Policy Update Magnitude: 1.83114
Value Function Update Magnitude: 1.51464

Collected Steps per Second: 18,384.24168
Overall Steps per Second: 9,910.07716

Timestep Collection Time: 2.72124
Timestep Consumption Time: 2.32695
PPO Batch Consumption Time: 0.13888
Total Iteration Time: 5.04819

Cumulative Model Updates: 4,928
Cumulative Timesteps: 20,611,300

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.22055
Policy Entropy: 3.22318
Value Function Loss: 3.58820

Mean KL Divergence: 0.03612
SB3 Clip Fraction: 0.39315
Policy Update Magnitude: 1.81575
Value Function Update Magnitude: 1.43484

Collected Steps per Second: 19,452.76768
Overall Steps per Second: 9,508.45054

Timestep Collection Time: 2.57238
Timestep Consumption Time: 2.69030
PPO Batch Consumption Time: 0.14915
Total Iteration Time: 5.26269

Cumulative Model Updates: 4,940
Cumulative Timesteps: 20,661,340

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 194.62960
Policy Entropy: 3.23174
Value Function Loss: 3.54466

Mean KL Divergence: 0.03683
SB3 Clip Fraction: 0.39934
Policy Update Magnitude: 1.78245
Value Function Update Magnitude: 1.48479

Collected Steps per Second: 17,707.16892
Overall Steps per Second: 9,281.33105

Timestep Collection Time: 2.82654
Timestep Consumption Time: 2.56601
PPO Batch Consumption Time: 0.13837
Total Iteration Time: 5.39255

Cumulative Model Updates: 4,952
Cumulative Timesteps: 20,711,390

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.46173
Policy Entropy: 3.24717
Value Function Loss: 3.60384

Mean KL Divergence: 0.03725
SB3 Clip Fraction: 0.39993
Policy Update Magnitude: 1.77237
Value Function Update Magnitude: 1.44477

Collected Steps per Second: 22,089.29002
Overall Steps per Second: 10,760.17194

Timestep Collection Time: 2.26445
Timestep Consumption Time: 2.38418
PPO Batch Consumption Time: 0.14026
Total Iteration Time: 4.64862

Cumulative Model Updates: 4,964
Cumulative Timesteps: 20,761,410

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 154.48589
Policy Entropy: 3.25413
Value Function Loss: 3.35856

Mean KL Divergence: 0.03635
SB3 Clip Fraction: 0.39555
Policy Update Magnitude: 1.75125
Value Function Update Magnitude: 1.45051

Collected Steps per Second: 21,425.97525
Overall Steps per Second: 10,592.80259

Timestep Collection Time: 2.33558
Timestep Consumption Time: 2.38857
PPO Batch Consumption Time: 0.13868
Total Iteration Time: 4.72415

Cumulative Model Updates: 4,976
Cumulative Timesteps: 20,811,452

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 192.65731
Policy Entropy: 3.26479
Value Function Loss: 3.26245

Mean KL Divergence: 0.03555
SB3 Clip Fraction: 0.39518
Policy Update Magnitude: 1.74162
Value Function Update Magnitude: 1.40093

Collected Steps per Second: 20,106.55106
Overall Steps per Second: 10,370.95393

Timestep Collection Time: 2.48874
Timestep Consumption Time: 2.33627
PPO Batch Consumption Time: 0.14158
Total Iteration Time: 4.82501

Cumulative Model Updates: 4,988
Cumulative Timesteps: 20,861,492

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 184.45866
Policy Entropy: 3.26920
Value Function Loss: 3.44133

Mean KL Divergence: 0.03606
SB3 Clip Fraction: 0.39758
Policy Update Magnitude: 1.77119
Value Function Update Magnitude: 1.30962

Collected Steps per Second: 20,768.52081
Overall Steps per Second: 10,401.56042

Timestep Collection Time: 2.40826
Timestep Consumption Time: 2.40025
PPO Batch Consumption Time: 0.14032
Total Iteration Time: 4.80851

Cumulative Model Updates: 5,000
Cumulative Timesteps: 20,911,508

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 186.95041
Policy Entropy: 3.26951
Value Function Loss: 3.44188

Mean KL Divergence: 0.03603
SB3 Clip Fraction: 0.39311
Policy Update Magnitude: 1.77621
Value Function Update Magnitude: 1.39163

Collected Steps per Second: 19,785.37387
Overall Steps per Second: 10,317.61766

Timestep Collection Time: 2.52773
Timestep Consumption Time: 2.31952
PPO Batch Consumption Time: 0.13864
Total Iteration Time: 4.84724

Cumulative Model Updates: 5,012
Cumulative Timesteps: 20,961,520

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 171.01874
Policy Entropy: 3.28345
Value Function Loss: 3.49473

Mean KL Divergence: 0.03632
SB3 Clip Fraction: 0.39774
Policy Update Magnitude: 1.76164
Value Function Update Magnitude: 1.34836

Collected Steps per Second: 24,837.20761
Overall Steps per Second: 11,544.33323

Timestep Collection Time: 2.01327
Timestep Consumption Time: 2.31821
PPO Batch Consumption Time: 0.13730
Total Iteration Time: 4.33148

Cumulative Model Updates: 5,024
Cumulative Timesteps: 21,011,524

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 192.77509
Policy Entropy: 3.29445
Value Function Loss: 3.42545

Mean KL Divergence: 0.03624
SB3 Clip Fraction: 0.39804
Policy Update Magnitude: 1.74699
Value Function Update Magnitude: 1.35485

Collected Steps per Second: 20,799.63974
Overall Steps per Second: 10,429.29511

Timestep Collection Time: 2.40495
Timestep Consumption Time: 2.39135
PPO Batch Consumption Time: 0.14269
Total Iteration Time: 4.79630

Cumulative Model Updates: 5,036
Cumulative Timesteps: 21,061,546

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 149.27185
Policy Entropy: 3.30555
Value Function Loss: 3.65724

Mean KL Divergence: 0.03612
SB3 Clip Fraction: 0.39437
Policy Update Magnitude: 1.77399
Value Function Update Magnitude: 1.40571

Collected Steps per Second: 22,970.11263
Overall Steps per Second: 11,182.51208

Timestep Collection Time: 2.17735
Timestep Consumption Time: 2.29517
PPO Batch Consumption Time: 0.13833
Total Iteration Time: 4.47252

Cumulative Model Updates: 5,048
Cumulative Timesteps: 21,111,560

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.46636
Policy Entropy: 3.31184
Value Function Loss: 3.74063

Mean KL Divergence: 0.03696
SB3 Clip Fraction: 0.39973
Policy Update Magnitude: 1.79378
Value Function Update Magnitude: 1.40994

Collected Steps per Second: 24,401.39519
Overall Steps per Second: 11,391.90730

Timestep Collection Time: 2.04964
Timestep Consumption Time: 2.34067
PPO Batch Consumption Time: 0.14028
Total Iteration Time: 4.39031

Cumulative Model Updates: 5,060
Cumulative Timesteps: 21,161,574

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 174.71745
Policy Entropy: 3.31896
Value Function Loss: 3.73556

Mean KL Divergence: 0.03694
SB3 Clip Fraction: 0.40013
Policy Update Magnitude: 1.76315
Value Function Update Magnitude: 1.58154

Collected Steps per Second: 24,988.41752
Overall Steps per Second: 11,626.10732

Timestep Collection Time: 2.00389
Timestep Consumption Time: 2.30314
PPO Batch Consumption Time: 0.13860
Total Iteration Time: 4.30703

Cumulative Model Updates: 5,072
Cumulative Timesteps: 21,211,648

Timesteps Collected: 50,074
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.29840
Policy Entropy: 3.31767
Value Function Loss: 3.58195

Mean KL Divergence: 0.03663
SB3 Clip Fraction: 0.39737
Policy Update Magnitude: 1.77660
Value Function Update Magnitude: 1.54075

Collected Steps per Second: 24,174.50839
Overall Steps per Second: 11,247.52843

Timestep Collection Time: 2.06937
Timestep Consumption Time: 2.37836
PPO Batch Consumption Time: 0.14308
Total Iteration Time: 4.44773

Cumulative Model Updates: 5,084
Cumulative Timesteps: 21,261,674

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.22663
Policy Entropy: 3.31603
Value Function Loss: 3.46362

Mean KL Divergence: 0.03631
SB3 Clip Fraction: 0.39701
Policy Update Magnitude: 1.77386
Value Function Update Magnitude: 1.43695

Collected Steps per Second: 23,388.76719
Overall Steps per Second: 11,207.83355

Timestep Collection Time: 2.13821
Timestep Consumption Time: 2.32385
PPO Batch Consumption Time: 0.13813
Total Iteration Time: 4.46206

Cumulative Model Updates: 5,096
Cumulative Timesteps: 21,311,684

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 171.40858
Policy Entropy: 3.31921
Value Function Loss: 3.73859

Mean KL Divergence: 0.03694
SB3 Clip Fraction: 0.40104
Policy Update Magnitude: 1.81507
Value Function Update Magnitude: 1.35771

Collected Steps per Second: 22,612.54523
Overall Steps per Second: 11,157.34235

Timestep Collection Time: 2.21160
Timestep Consumption Time: 2.27065
PPO Batch Consumption Time: 0.13939
Total Iteration Time: 4.48225

Cumulative Model Updates: 5,108
Cumulative Timesteps: 21,361,694

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 186.44281
Policy Entropy: 3.32237
Value Function Loss: 3.64383

Mean KL Divergence: 0.03671
SB3 Clip Fraction: 0.39762
Policy Update Magnitude: 1.78456
Value Function Update Magnitude: 1.34424

Collected Steps per Second: 22,404.69437
Overall Steps per Second: 10,877.27827

Timestep Collection Time: 2.23212
Timestep Consumption Time: 2.36554
PPO Batch Consumption Time: 0.13910
Total Iteration Time: 4.59766

Cumulative Model Updates: 5,120
Cumulative Timesteps: 21,411,704

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 151.67425
Policy Entropy: 3.32356
Value Function Loss: 3.56225

Mean KL Divergence: 0.03645
SB3 Clip Fraction: 0.39596
Policy Update Magnitude: 1.77364
Value Function Update Magnitude: 1.31476

Collected Steps per Second: 23,061.11514
Overall Steps per Second: 11,271.51844

Timestep Collection Time: 2.16824
Timestep Consumption Time: 2.26790
PPO Batch Consumption Time: 0.13784
Total Iteration Time: 4.43614

Cumulative Model Updates: 5,132
Cumulative Timesteps: 21,461,706

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 181.82752
Policy Entropy: 3.32635
Value Function Loss: 3.38460

Mean KL Divergence: 0.03601
SB3 Clip Fraction: 0.39591
Policy Update Magnitude: 1.75869
Value Function Update Magnitude: 1.32457

Collected Steps per Second: 24,331.95573
Overall Steps per Second: 11,331.74786

Timestep Collection Time: 2.05623
Timestep Consumption Time: 2.35898
PPO Batch Consumption Time: 0.13791
Total Iteration Time: 4.41521

Cumulative Model Updates: 5,144
Cumulative Timesteps: 21,511,738

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 173.36032
Policy Entropy: 3.32912
Value Function Loss: 3.55923

Mean KL Divergence: 0.03593
SB3 Clip Fraction: 0.39500
Policy Update Magnitude: 1.78383
Value Function Update Magnitude: 1.39774

Collected Steps per Second: 23,345.84420
Overall Steps per Second: 11,279.49046

Timestep Collection Time: 2.14334
Timestep Consumption Time: 2.29286
PPO Batch Consumption Time: 0.13771
Total Iteration Time: 4.43619

Cumulative Model Updates: 5,156
Cumulative Timesteps: 21,561,776

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 198.62182
Policy Entropy: 3.33335
Value Function Loss: 3.71863

Mean KL Divergence: 0.03686
SB3 Clip Fraction: 0.40094
Policy Update Magnitude: 1.80670
Value Function Update Magnitude: 1.54541

Collected Steps per Second: 23,647.47642
Overall Steps per Second: 11,275.28755

Timestep Collection Time: 2.11481
Timestep Consumption Time: 2.32055
PPO Batch Consumption Time: 0.13763
Total Iteration Time: 4.43536

Cumulative Model Updates: 5,168
Cumulative Timesteps: 21,611,786

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 163.82653
Policy Entropy: 3.33493
Value Function Loss: 3.72086

Mean KL Divergence: 0.03782
SB3 Clip Fraction: 0.40567
Policy Update Magnitude: 1.79825
Value Function Update Magnitude: 1.68020

Collected Steps per Second: 22,985.14019
Overall Steps per Second: 11,090.43666

Timestep Collection Time: 2.17688
Timestep Consumption Time: 2.33475
PPO Batch Consumption Time: 0.13742
Total Iteration Time: 4.51163

Cumulative Model Updates: 5,180
Cumulative Timesteps: 21,661,822

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.21718
Policy Entropy: 3.33073
Value Function Loss: 3.70328

Mean KL Divergence: 0.03805
SB3 Clip Fraction: 0.40848
Policy Update Magnitude: 1.79870
Value Function Update Magnitude: 1.56904

Collected Steps per Second: 22,921.88571
Overall Steps per Second: 11,114.46962

Timestep Collection Time: 2.18263
Timestep Consumption Time: 2.31871
PPO Batch Consumption Time: 0.13781
Total Iteration Time: 4.50134

Cumulative Model Updates: 5,192
Cumulative Timesteps: 21,711,852

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 243.23101
Policy Entropy: 3.32981
Value Function Loss: 3.45205

Mean KL Divergence: 0.03749
SB3 Clip Fraction: 0.40601
Policy Update Magnitude: 1.76015
Value Function Update Magnitude: 1.57297

Collected Steps per Second: 23,903.67195
Overall Steps per Second: 11,391.01738

Timestep Collection Time: 2.09374
Timestep Consumption Time: 2.29990
PPO Batch Consumption Time: 0.13762
Total Iteration Time: 4.39364

Cumulative Model Updates: 5,204
Cumulative Timesteps: 21,761,900

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 161.51769
Policy Entropy: 3.33394
Value Function Loss: 3.32201

Mean KL Divergence: 0.03691
SB3 Clip Fraction: 0.40056
Policy Update Magnitude: 1.72976
Value Function Update Magnitude: 1.59363

Collected Steps per Second: 22,599.42048
Overall Steps per Second: 11,159.49650

Timestep Collection Time: 2.21298
Timestep Consumption Time: 2.26859
PPO Batch Consumption Time: 0.13817
Total Iteration Time: 4.48156

Cumulative Model Updates: 5,216
Cumulative Timesteps: 21,811,912

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 170.17321
Policy Entropy: 3.33977
Value Function Loss: 3.21114

Mean KL Divergence: 0.03599
SB3 Clip Fraction: 0.39682
Policy Update Magnitude: 1.73207
Value Function Update Magnitude: 1.44940

Collected Steps per Second: 24,614.89903
Overall Steps per Second: 11,442.21773

Timestep Collection Time: 2.03275
Timestep Consumption Time: 2.34018
PPO Batch Consumption Time: 0.13862
Total Iteration Time: 4.37293

Cumulative Model Updates: 5,228
Cumulative Timesteps: 21,861,948

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 201.69945
Policy Entropy: 3.33702
Value Function Loss: 3.33107

Mean KL Divergence: 0.03591
SB3 Clip Fraction: 0.39783
Policy Update Magnitude: 1.76914
Value Function Update Magnitude: 1.45454

Collected Steps per Second: 24,335.42256
Overall Steps per Second: 11,680.11474

Timestep Collection Time: 2.05577
Timestep Consumption Time: 2.22741
PPO Batch Consumption Time: 0.13897
Total Iteration Time: 4.28318

Cumulative Model Updates: 5,240
Cumulative Timesteps: 21,911,976

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 180.23268
Policy Entropy: 3.34376
Value Function Loss: 3.52264

Mean KL Divergence: 0.03699
SB3 Clip Fraction: 0.40249
Policy Update Magnitude: 1.76254
Value Function Update Magnitude: 1.46605

Collected Steps per Second: 23,327.05868
Overall Steps per Second: 11,425.60846

Timestep Collection Time: 2.14541
Timestep Consumption Time: 2.23476
PPO Batch Consumption Time: 0.13951
Total Iteration Time: 4.38016

Cumulative Model Updates: 5,252
Cumulative Timesteps: 21,962,022

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 178.07664
Policy Entropy: 3.33968
Value Function Loss: 3.48824

Mean KL Divergence: 0.03769
SB3 Clip Fraction: 0.40540
Policy Update Magnitude: 1.72729
Value Function Update Magnitude: 1.42392

Collected Steps per Second: 24,340.17384
Overall Steps per Second: 11,595.52652

Timestep Collection Time: 2.05644
Timestep Consumption Time: 2.26023
PPO Batch Consumption Time: 0.13796
Total Iteration Time: 4.31666

Cumulative Model Updates: 5,264
Cumulative Timesteps: 22,012,076

Timesteps Collected: 50,054
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.00518
Policy Entropy: 3.35213
Value Function Loss: 3.51653

Mean KL Divergence: 0.03769
SB3 Clip Fraction: 0.40424
Policy Update Magnitude: 1.72032
Value Function Update Magnitude: 1.36408

Collected Steps per Second: 25,070.18700
Overall Steps per Second: 11,835.81456

Timestep Collection Time: 1.99560
Timestep Consumption Time: 2.23140
PPO Batch Consumption Time: 0.13816
Total Iteration Time: 4.22700

Cumulative Model Updates: 5,276
Cumulative Timesteps: 22,062,106

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 146.26253
Policy Entropy: 3.35215
Value Function Loss: 3.42082

Mean KL Divergence: 0.03763
SB3 Clip Fraction: 0.40609
Policy Update Magnitude: 1.74055
Value Function Update Magnitude: 1.37568

Collected Steps per Second: 23,818.40395
Overall Steps per Second: 11,482.27796

Timestep Collection Time: 2.10073
Timestep Consumption Time: 2.25694
PPO Batch Consumption Time: 0.13874
Total Iteration Time: 4.35767

Cumulative Model Updates: 5,288
Cumulative Timesteps: 22,112,142

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 168.43852
Policy Entropy: 3.35614
Value Function Loss: 3.48433

Mean KL Divergence: 0.03800
SB3 Clip Fraction: 0.40808
Policy Update Magnitude: 1.74609
Value Function Update Magnitude: 1.35228

Collected Steps per Second: 23,368.05580
Overall Steps per Second: 11,492.41063

Timestep Collection Time: 2.14087
Timestep Consumption Time: 2.21226
PPO Batch Consumption Time: 0.14194
Total Iteration Time: 4.35313

Cumulative Model Updates: 5,300
Cumulative Timesteps: 22,162,170

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.57721
Policy Entropy: 3.35807
Value Function Loss: 3.56191

Mean KL Divergence: 0.03893
SB3 Clip Fraction: 0.41193
Policy Update Magnitude: 1.74379
Value Function Update Magnitude: 1.41723

Collected Steps per Second: 23,239.94439
Overall Steps per Second: 11,374.59488

Timestep Collection Time: 2.15302
Timestep Consumption Time: 2.24591
PPO Batch Consumption Time: 0.13936
Total Iteration Time: 4.39893

Cumulative Model Updates: 5,312
Cumulative Timesteps: 22,212,206

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.36050
Policy Entropy: 3.35963
Value Function Loss: 3.45742

Mean KL Divergence: 0.03831
SB3 Clip Fraction: 0.40928
Policy Update Magnitude: 1.72167
Value Function Update Magnitude: 1.40617

Collected Steps per Second: 23,624.18973
Overall Steps per Second: 11,523.12962

Timestep Collection Time: 2.11690
Timestep Consumption Time: 2.22307
PPO Batch Consumption Time: 0.13812
Total Iteration Time: 4.33997

Cumulative Model Updates: 5,324
Cumulative Timesteps: 22,262,216

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 200.78695
Policy Entropy: 3.36313
Value Function Loss: 3.49627

Mean KL Divergence: 0.03798
SB3 Clip Fraction: 0.40961
Policy Update Magnitude: 1.75719
Value Function Update Magnitude: 1.45138

Collected Steps per Second: 24,894.16670
Overall Steps per Second: 11,801.55062

Timestep Collection Time: 2.00979
Timestep Consumption Time: 2.22965
PPO Batch Consumption Time: 0.13819
Total Iteration Time: 4.23944

Cumulative Model Updates: 5,336
Cumulative Timesteps: 22,312,248

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 183.27661
Policy Entropy: 3.35992
Value Function Loss: 3.43173

Mean KL Divergence: 0.03834
SB3 Clip Fraction: 0.41050
Policy Update Magnitude: 1.72464
Value Function Update Magnitude: 1.44780

Collected Steps per Second: 23,680.31247
Overall Steps per Second: 11,458.30682

Timestep Collection Time: 2.11171
Timestep Consumption Time: 2.25246
PPO Batch Consumption Time: 0.13977
Total Iteration Time: 4.36417

Cumulative Model Updates: 5,348
Cumulative Timesteps: 22,362,254

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 173.79676
Policy Entropy: 3.35930
Value Function Loss: 3.55060

Mean KL Divergence: 0.03867
SB3 Clip Fraction: 0.41240
Policy Update Magnitude: 1.73119
Value Function Update Magnitude: 1.45483

Collected Steps per Second: 23,179.04752
Overall Steps per Second: 11,540.41572

Timestep Collection Time: 2.16031
Timestep Consumption Time: 2.17870
PPO Batch Consumption Time: 0.13887
Total Iteration Time: 4.33901

Cumulative Model Updates: 5,360
Cumulative Timesteps: 22,412,328

Timesteps Collected: 50,074
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.78557
Policy Entropy: 3.35604
Value Function Loss: 3.55455

Mean KL Divergence: 0.03836
SB3 Clip Fraction: 0.41094
Policy Update Magnitude: 1.74335
Value Function Update Magnitude: 1.43003

Collected Steps per Second: 25,355.78039
Overall Steps per Second: 11,929.22372

Timestep Collection Time: 1.97383
Timestep Consumption Time: 2.22158
PPO Batch Consumption Time: 0.13767
Total Iteration Time: 4.19541

Cumulative Model Updates: 5,372
Cumulative Timesteps: 22,462,376

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 169.10738
Policy Entropy: 3.35965
Value Function Loss: 3.57810

Mean KL Divergence: 0.03914
SB3 Clip Fraction: 0.41323
Policy Update Magnitude: 1.73359
Value Function Update Magnitude: 1.43788

Collected Steps per Second: 23,800.67789
Overall Steps per Second: 11,726.25238

Timestep Collection Time: 2.10212
Timestep Consumption Time: 2.16454
PPO Batch Consumption Time: 0.13724
Total Iteration Time: 4.26667

Cumulative Model Updates: 5,384
Cumulative Timesteps: 22,512,408

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.75739
Policy Entropy: 3.36021
Value Function Loss: 3.60374

Mean KL Divergence: 0.04010
SB3 Clip Fraction: 0.41907
Policy Update Magnitude: 1.75939
Value Function Update Magnitude: 1.57608

Collected Steps per Second: 25,160.54342
Overall Steps per Second: 11,874.21308

Timestep Collection Time: 1.98859
Timestep Consumption Time: 2.22508
PPO Batch Consumption Time: 0.13838
Total Iteration Time: 4.21367

Cumulative Model Updates: 5,396
Cumulative Timesteps: 22,562,442

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 204.19157
Policy Entropy: 3.35378
Value Function Loss: 3.73354

Mean KL Divergence: 0.04031
SB3 Clip Fraction: 0.41838
Policy Update Magnitude: 1.78346
Value Function Update Magnitude: 1.62066

Collected Steps per Second: 24,374.11455
Overall Steps per Second: 11,708.19850

Timestep Collection Time: 2.05292
Timestep Consumption Time: 2.22084
PPO Batch Consumption Time: 0.13876
Total Iteration Time: 4.27376

Cumulative Model Updates: 5,408
Cumulative Timesteps: 22,612,480

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 158.73499
Policy Entropy: 3.35278
Value Function Loss: 3.79289

Mean KL Divergence: 0.04074
SB3 Clip Fraction: 0.42205
Policy Update Magnitude: 1.76384
Value Function Update Magnitude: 1.62660

Collected Steps per Second: 23,770.18302
Overall Steps per Second: 11,495.05013

Timestep Collection Time: 2.10348
Timestep Consumption Time: 2.24622
PPO Batch Consumption Time: 0.13932
Total Iteration Time: 4.34970

Cumulative Model Updates: 5,420
Cumulative Timesteps: 22,662,480

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 200.39950
Policy Entropy: 3.34775
Value Function Loss: 3.75020

Mean KL Divergence: 0.04134
SB3 Clip Fraction: 0.42596
Policy Update Magnitude: 1.76520
Value Function Update Magnitude: 1.57846

Collected Steps per Second: 24,861.42065
Overall Steps per Second: 11,713.44814

Timestep Collection Time: 2.01300
Timestep Consumption Time: 2.25953
PPO Batch Consumption Time: 0.13926
Total Iteration Time: 4.27252

Cumulative Model Updates: 5,432
Cumulative Timesteps: 22,712,526

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 199.21355
Policy Entropy: 3.35461
Value Function Loss: 3.72621

Mean KL Divergence: 0.04130
SB3 Clip Fraction: 0.42609
Policy Update Magnitude: 1.76927
Value Function Update Magnitude: 1.60610

Collected Steps per Second: 24,946.94076
Overall Steps per Second: 11,967.24012

Timestep Collection Time: 2.00578
Timestep Consumption Time: 2.17547
PPO Batch Consumption Time: 0.13852
Total Iteration Time: 4.18125

Cumulative Model Updates: 5,444
Cumulative Timesteps: 22,762,564

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 186.61614
Policy Entropy: 3.34915
Value Function Loss: 3.89530

Mean KL Divergence: 0.04103
SB3 Clip Fraction: 0.42088
Policy Update Magnitude: 1.78340
Value Function Update Magnitude: 1.85878

Collected Steps per Second: 24,516.62897
Overall Steps per Second: 11,702.19415

Timestep Collection Time: 2.04049
Timestep Consumption Time: 2.23443
PPO Batch Consumption Time: 0.13835
Total Iteration Time: 4.27492

Cumulative Model Updates: 5,456
Cumulative Timesteps: 22,812,590

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 180.25703
Policy Entropy: 3.35362
Value Function Loss: 3.75098

Mean KL Divergence: 0.04077
SB3 Clip Fraction: 0.41758
Policy Update Magnitude: 1.75036
Value Function Update Magnitude: 1.80212

Collected Steps per Second: 24,946.01271
Overall Steps per Second: 11,818.95549

Timestep Collection Time: 2.00561
Timestep Consumption Time: 2.22759
PPO Batch Consumption Time: 0.13942
Total Iteration Time: 4.23320

Cumulative Model Updates: 5,468
Cumulative Timesteps: 22,862,622

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.38839
Policy Entropy: 3.35028
Value Function Loss: 3.71751

Mean KL Divergence: 0.04020
SB3 Clip Fraction: 0.41679
Policy Update Magnitude: 1.76353
Value Function Update Magnitude: 1.65653

Collected Steps per Second: 25,178.81935
Overall Steps per Second: 11,875.08442

Timestep Collection Time: 1.98667
Timestep Consumption Time: 2.22568
PPO Batch Consumption Time: 0.13808
Total Iteration Time: 4.21235

Cumulative Model Updates: 5,480
Cumulative Timesteps: 22,912,644

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 190.68567
Policy Entropy: 3.35540
Value Function Loss: 3.42351

Mean KL Divergence: 0.03992
SB3 Clip Fraction: 0.41874
Policy Update Magnitude: 1.73639
Value Function Update Magnitude: 1.58940

Collected Steps per Second: 23,662.69929
Overall Steps per Second: 11,462.63313

Timestep Collection Time: 2.11472
Timestep Consumption Time: 2.25077
PPO Batch Consumption Time: 0.13849
Total Iteration Time: 4.36549

Cumulative Model Updates: 5,492
Cumulative Timesteps: 22,962,684

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 188.49641
Policy Entropy: 3.35602
Value Function Loss: 3.35231

Mean KL Divergence: 0.03968
SB3 Clip Fraction: 0.41597
Policy Update Magnitude: 1.71729
Value Function Update Magnitude: 1.61881

Collected Steps per Second: 23,609.62860
Overall Steps per Second: 11,640.74658

Timestep Collection Time: 2.11981
Timestep Consumption Time: 2.17957
PPO Batch Consumption Time: 0.13831
Total Iteration Time: 4.29938

Cumulative Model Updates: 5,504
Cumulative Timesteps: 23,012,732

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 173.44716
Policy Entropy: 3.36488
Value Function Loss: 3.41887

Mean KL Divergence: 0.03918
SB3 Clip Fraction: 0.41601
Policy Update Magnitude: 1.74955
Value Function Update Magnitude: 1.58188

Collected Steps per Second: 24,773.17250
Overall Steps per Second: 11,792.83023

Timestep Collection Time: 2.01993
Timestep Consumption Time: 2.22333
PPO Batch Consumption Time: 0.13781
Total Iteration Time: 4.24326

Cumulative Model Updates: 5,516
Cumulative Timesteps: 23,062,772

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 165.25734
Policy Entropy: 3.36437
Value Function Loss: 3.59012

Mean KL Divergence: 0.04053
SB3 Clip Fraction: 0.42378
Policy Update Magnitude: 1.73835
Value Function Update Magnitude: 1.55668

Collected Steps per Second: 23,619.98806
Overall Steps per Second: 11,690.90842

Timestep Collection Time: 2.11880
Timestep Consumption Time: 2.16196
PPO Batch Consumption Time: 0.13852
Total Iteration Time: 4.28076

Cumulative Model Updates: 5,528
Cumulative Timesteps: 23,112,818

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 192.80931
Policy Entropy: 3.36299
Value Function Loss: 3.61615

Mean KL Divergence: 0.04140
SB3 Clip Fraction: 0.42563
Policy Update Magnitude: 1.72067
Value Function Update Magnitude: 1.51393

Collected Steps per Second: 24,572.25765
Overall Steps per Second: 11,639.17217

Timestep Collection Time: 2.03661
Timestep Consumption Time: 2.26301
PPO Batch Consumption Time: 0.13942
Total Iteration Time: 4.29962

Cumulative Model Updates: 5,540
Cumulative Timesteps: 23,162,862

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 181.53225
Policy Entropy: 3.36071
Value Function Loss: 3.64745

Mean KL Divergence: 0.04146
SB3 Clip Fraction: 0.42452
Policy Update Magnitude: 1.73808
Value Function Update Magnitude: 1.50649

Collected Steps per Second: 25,997.16502
Overall Steps per Second: 12,090.61222

Timestep Collection Time: 1.92467
Timestep Consumption Time: 2.21375
PPO Batch Consumption Time: 0.13799
Total Iteration Time: 4.13842

Cumulative Model Updates: 5,552
Cumulative Timesteps: 23,212,898

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 183.02508
Policy Entropy: 3.36537
Value Function Loss: 3.60669

Mean KL Divergence: 0.04107
SB3 Clip Fraction: 0.42100
Policy Update Magnitude: 1.72135
Value Function Update Magnitude: 1.56465

Collected Steps per Second: 24,783.61516
Overall Steps per Second: 11,812.68699

Timestep Collection Time: 2.01891
Timestep Consumption Time: 2.21687
PPO Batch Consumption Time: 0.13762
Total Iteration Time: 4.23578

Cumulative Model Updates: 5,564
Cumulative Timesteps: 23,262,934

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 186.64932
Policy Entropy: 3.36083
Value Function Loss: 3.52201

Mean KL Divergence: 0.04070
SB3 Clip Fraction: 0.41963
Policy Update Magnitude: 1.74564
Value Function Update Magnitude: 1.67349

Collected Steps per Second: 24,931.48852
Overall Steps per Second: 11,764.39781

Timestep Collection Time: 2.00558
Timestep Consumption Time: 2.24471
PPO Batch Consumption Time: 0.13871
Total Iteration Time: 4.25028

Cumulative Model Updates: 5,576
Cumulative Timesteps: 23,312,936

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 215.25348
Policy Entropy: 3.36422
Value Function Loss: 3.39230

Mean KL Divergence: 0.04118
SB3 Clip Fraction: 0.42517
Policy Update Magnitude: 1.72720
Value Function Update Magnitude: 1.71359

Collected Steps per Second: 23,730.62758
Overall Steps per Second: 11,649.37252

Timestep Collection Time: 2.10732
Timestep Consumption Time: 2.18544
PPO Batch Consumption Time: 0.13865
Total Iteration Time: 4.29276

Cumulative Model Updates: 5,588
Cumulative Timesteps: 23,362,944

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 185.74989
Policy Entropy: 3.35888
Value Function Loss: 3.56557

Mean KL Divergence: 0.04123
SB3 Clip Fraction: 0.42318
Policy Update Magnitude: 1.75327
Value Function Update Magnitude: 1.62303

Collected Steps per Second: 24,303.30848
Overall Steps per Second: 11,702.90403

Timestep Collection Time: 2.05807
Timestep Consumption Time: 2.21591
PPO Batch Consumption Time: 0.13766
Total Iteration Time: 4.27398

Cumulative Model Updates: 5,600
Cumulative Timesteps: 23,412,962

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 179.06890
Policy Entropy: 3.35514
Value Function Loss: 3.69351

Mean KL Divergence: 0.04174
SB3 Clip Fraction: 0.42336
Policy Update Magnitude: 1.74184
Value Function Update Magnitude: 1.59492

Collected Steps per Second: 24,901.82179
Overall Steps per Second: 11,973.03704

Timestep Collection Time: 2.00829
Timestep Consumption Time: 2.16860
PPO Batch Consumption Time: 0.13790
Total Iteration Time: 4.17689

Cumulative Model Updates: 5,612
Cumulative Timesteps: 23,462,972

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 187.31843
Policy Entropy: 3.34974
Value Function Loss: 3.73850

Mean KL Divergence: 0.04146
SB3 Clip Fraction: 0.42228
Policy Update Magnitude: 1.74425
Value Function Update Magnitude: 1.56699

Collected Steps per Second: 22,832.68897
Overall Steps per Second: 11,319.56446

Timestep Collection Time: 2.19151
Timestep Consumption Time: 2.22898
PPO Batch Consumption Time: 0.13751
Total Iteration Time: 4.42049

Cumulative Model Updates: 5,624
Cumulative Timesteps: 23,513,010

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 184.52379
Policy Entropy: 3.34680
Value Function Loss: 3.63459

Mean KL Divergence: 0.04115
SB3 Clip Fraction: 0.42359
Policy Update Magnitude: 1.71649
Value Function Update Magnitude: 1.51571

Collected Steps per Second: 24,404.29340
Overall Steps per Second: 11,738.57791

Timestep Collection Time: 2.04972
Timestep Consumption Time: 2.21161
PPO Batch Consumption Time: 0.13776
Total Iteration Time: 4.26133

Cumulative Model Updates: 5,636
Cumulative Timesteps: 23,563,032

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 181.01439
Policy Entropy: 3.35524
Value Function Loss: 3.53743

Mean KL Divergence: 0.04020
SB3 Clip Fraction: 0.41782
Policy Update Magnitude: 1.69528
Value Function Update Magnitude: 1.50137

Collected Steps per Second: 25,487.92406
Overall Steps per Second: 11,991.37752

Timestep Collection Time: 1.96250
Timestep Consumption Time: 2.20883
PPO Batch Consumption Time: 0.13801
Total Iteration Time: 4.17133

Cumulative Model Updates: 5,648
Cumulative Timesteps: 23,613,052

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 244.83840
Policy Entropy: 3.35434
Value Function Loss: 3.50269

Mean KL Divergence: 0.04008
SB3 Clip Fraction: 0.42055
Policy Update Magnitude: 1.71069
Value Function Update Magnitude: 1.51893

Collected Steps per Second: 23,884.57438
Overall Steps per Second: 11,512.96256

Timestep Collection Time: 2.09424
Timestep Consumption Time: 2.25043
PPO Batch Consumption Time: 0.13884
Total Iteration Time: 4.34467

Cumulative Model Updates: 5,660
Cumulative Timesteps: 23,663,072

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 182.86012
Policy Entropy: 3.35402
Value Function Loss: 3.64646

Mean KL Divergence: 0.04138
SB3 Clip Fraction: 0.42783
Policy Update Magnitude: 1.73035
Value Function Update Magnitude: 1.54788

Collected Steps per Second: 23,103.81932
Overall Steps per Second: 11,371.29494

Timestep Collection Time: 2.16553
Timestep Consumption Time: 2.23432
PPO Batch Consumption Time: 0.13854
Total Iteration Time: 4.39985

Cumulative Model Updates: 5,672
Cumulative Timesteps: 23,713,104

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 155.89483
Policy Entropy: 3.34791
Value Function Loss: 3.65362

Mean KL Divergence: 0.04215
SB3 Clip Fraction: 0.42954
Policy Update Magnitude: 1.71491
Value Function Update Magnitude: 1.54184

Collected Steps per Second: 22,949.00967
Overall Steps per Second: 11,285.36586

Timestep Collection Time: 2.17988
Timestep Consumption Time: 2.25294
PPO Batch Consumption Time: 0.13915
Total Iteration Time: 4.43282

Cumulative Model Updates: 5,684
Cumulative Timesteps: 23,763,130

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 215.67499
Policy Entropy: 3.34246
Value Function Loss: 3.57095

Mean KL Divergence: 0.04143
SB3 Clip Fraction: 0.42429
Policy Update Magnitude: 1.69611
Value Function Update Magnitude: 1.47085

Collected Steps per Second: 23,291.83036
Overall Steps per Second: 11,492.82641

Timestep Collection Time: 2.14702
Timestep Consumption Time: 2.20422
PPO Batch Consumption Time: 0.13969
Total Iteration Time: 4.35124

Cumulative Model Updates: 5,696
Cumulative Timesteps: 23,813,138

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.82044
Policy Entropy: 3.33438
Value Function Loss: 3.52886

Mean KL Divergence: 0.04072
SB3 Clip Fraction: 0.42395
Policy Update Magnitude: 1.71269
Value Function Update Magnitude: 1.48569

Collected Steps per Second: 22,002.08888
Overall Steps per Second: 11,035.01098

Timestep Collection Time: 2.27251
Timestep Consumption Time: 2.25852
PPO Batch Consumption Time: 0.13935
Total Iteration Time: 4.53103

Cumulative Model Updates: 5,708
Cumulative Timesteps: 23,863,138

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.21822
Policy Entropy: 3.33211
Value Function Loss: 3.50553

Mean KL Divergence: 0.04137
SB3 Clip Fraction: 0.42665
Policy Update Magnitude: 1.74129
Value Function Update Magnitude: 1.49206

Collected Steps per Second: 25,811.11144
Overall Steps per Second: 11,980.75864

Timestep Collection Time: 1.93777
Timestep Consumption Time: 2.23692
PPO Batch Consumption Time: 0.13934
Total Iteration Time: 4.17469

Cumulative Model Updates: 5,720
Cumulative Timesteps: 23,913,154

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.78651
Policy Entropy: 3.32709
Value Function Loss: 3.63550

Mean KL Divergence: 0.04302
SB3 Clip Fraction: 0.43355
Policy Update Magnitude: 1.75205
Value Function Update Magnitude: 1.48620

Collected Steps per Second: 26,832.46663
Overall Steps per Second: 12,038.64978

Timestep Collection Time: 1.86438
Timestep Consumption Time: 2.29107
PPO Batch Consumption Time: 0.14255
Total Iteration Time: 4.15545

Cumulative Model Updates: 5,732
Cumulative Timesteps: 23,963,180

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.85011
Policy Entropy: 3.33319
Value Function Loss: 3.75411

Mean KL Divergence: 0.04337
SB3 Clip Fraction: 0.43384
Policy Update Magnitude: 1.79818
Value Function Update Magnitude: 1.46065

Collected Steps per Second: 24,617.34632
Overall Steps per Second: 11,753.41548

Timestep Collection Time: 2.03214
Timestep Consumption Time: 2.22415
PPO Batch Consumption Time: 0.13920
Total Iteration Time: 4.25629

Cumulative Model Updates: 5,744
Cumulative Timesteps: 24,013,206

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.55746
Policy Entropy: 3.33541
Value Function Loss: 3.72029

Mean KL Divergence: 0.04284
SB3 Clip Fraction: 0.43225
Policy Update Magnitude: 1.74099
Value Function Update Magnitude: 1.43435

Collected Steps per Second: 25,100.86423
Overall Steps per Second: 12,002.98218

Timestep Collection Time: 1.99340
Timestep Consumption Time: 2.17523
PPO Batch Consumption Time: 0.13866
Total Iteration Time: 4.16863

Cumulative Model Updates: 5,756
Cumulative Timesteps: 24,063,242

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.14902
Policy Entropy: 3.34055
Value Function Loss: 3.63525

Mean KL Divergence: 0.04268
SB3 Clip Fraction: 0.43288
Policy Update Magnitude: 1.73143
Value Function Update Magnitude: 1.44989

Collected Steps per Second: 24,210.98233
Overall Steps per Second: 11,663.54403

Timestep Collection Time: 2.06642
Timestep Consumption Time: 2.22302
PPO Batch Consumption Time: 0.13708
Total Iteration Time: 4.28943

Cumulative Model Updates: 5,768
Cumulative Timesteps: 24,113,272

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 175.40825
Policy Entropy: 3.33965
Value Function Loss: 3.74321

Mean KL Divergence: 0.04271
SB3 Clip Fraction: 0.43235
Policy Update Magnitude: 1.74920
Value Function Update Magnitude: 1.46897

Collected Steps per Second: 24,358.67929
Overall Steps per Second: 11,728.44905

Timestep Collection Time: 2.05340
Timestep Consumption Time: 2.21128
PPO Batch Consumption Time: 0.13839
Total Iteration Time: 4.26467

Cumulative Model Updates: 5,780
Cumulative Timesteps: 24,163,290

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.98413
Policy Entropy: 3.33877
Value Function Loss: 3.73972

Mean KL Divergence: 0.04283
SB3 Clip Fraction: 0.43430
Policy Update Magnitude: 1.73863
Value Function Update Magnitude: 1.51722

Collected Steps per Second: 26,601.87296
Overall Steps per Second: 12,119.39832

Timestep Collection Time: 1.88002
Timestep Consumption Time: 2.24659
PPO Batch Consumption Time: 0.13793
Total Iteration Time: 4.12661

Cumulative Model Updates: 5,792
Cumulative Timesteps: 24,213,302

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 242.32400
Policy Entropy: 3.33690
Value Function Loss: 3.70009

Mean KL Divergence: 0.04376
SB3 Clip Fraction: 0.43655
Policy Update Magnitude: 1.70583
Value Function Update Magnitude: 1.49333

Collected Steps per Second: 24,852.11363
Overall Steps per Second: 11,859.63293

Timestep Collection Time: 2.01295
Timestep Consumption Time: 2.20523
PPO Batch Consumption Time: 0.13877
Total Iteration Time: 4.21817

Cumulative Model Updates: 5,804
Cumulative Timesteps: 24,263,328

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 172.21209
Policy Entropy: 3.33369
Value Function Loss: 3.67115

Mean KL Divergence: 0.04272
SB3 Clip Fraction: 0.42945
Policy Update Magnitude: 1.71563
Value Function Update Magnitude: 1.42075

Collected Steps per Second: 23,777.25934
Overall Steps per Second: 11,640.26851

Timestep Collection Time: 2.10352
Timestep Consumption Time: 2.19329
PPO Batch Consumption Time: 0.14026
Total Iteration Time: 4.29681

Cumulative Model Updates: 5,816
Cumulative Timesteps: 24,313,344

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.62558
Policy Entropy: 3.33750
Value Function Loss: 3.75870

Mean KL Divergence: 0.04275
SB3 Clip Fraction: 0.43136
Policy Update Magnitude: 1.71905
Value Function Update Magnitude: 1.39062

Collected Steps per Second: 24,428.33069
Overall Steps per Second: 11,671.50857

Timestep Collection Time: 2.04885
Timestep Consumption Time: 2.23937
PPO Batch Consumption Time: 0.13815
Total Iteration Time: 4.28822

Cumulative Model Updates: 5,828
Cumulative Timesteps: 24,363,394

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 182.23921
Policy Entropy: 3.33696
Value Function Loss: 3.79727

Mean KL Divergence: 0.04371
SB3 Clip Fraction: 0.43652
Policy Update Magnitude: 1.71809
Value Function Update Magnitude: 1.47216

Collected Steps per Second: 26,713.67105
Overall Steps per Second: 12,170.75516

Timestep Collection Time: 1.87245
Timestep Consumption Time: 2.23740
PPO Batch Consumption Time: 0.13928
Total Iteration Time: 4.10985

Cumulative Model Updates: 5,840
Cumulative Timesteps: 24,413,414

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.97587
Policy Entropy: 3.33297
Value Function Loss: 3.75447

Mean KL Divergence: 0.04382
SB3 Clip Fraction: 0.43517
Policy Update Magnitude: 1.72530
Value Function Update Magnitude: 1.49608

Collected Steps per Second: 25,511.75245
Overall Steps per Second: 11,938.58738

Timestep Collection Time: 1.96059
Timestep Consumption Time: 2.22902
PPO Batch Consumption Time: 0.13827
Total Iteration Time: 4.18961

Cumulative Model Updates: 5,852
Cumulative Timesteps: 24,463,432

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 223.31905
Policy Entropy: 3.32481
Value Function Loss: 3.85762

Mean KL Divergence: 0.04366
SB3 Clip Fraction: 0.43541
Policy Update Magnitude: 1.72619
Value Function Update Magnitude: 1.52090

Collected Steps per Second: 25,168.12737
Overall Steps per Second: 11,838.55475

Timestep Collection Time: 1.98863
Timestep Consumption Time: 2.23909
PPO Batch Consumption Time: 0.13915
Total Iteration Time: 4.22771

Cumulative Model Updates: 5,864
Cumulative Timesteps: 24,513,482

Timesteps Collected: 50,050
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.69808
Policy Entropy: 3.32111
Value Function Loss: 3.90803

Mean KL Divergence: 0.04347
SB3 Clip Fraction: 0.43369
Policy Update Magnitude: 1.72263
Value Function Update Magnitude: 1.55069

Collected Steps per Second: 25,155.31651
Overall Steps per Second: 11,785.57712

Timestep Collection Time: 1.98900
Timestep Consumption Time: 2.25636
PPO Batch Consumption Time: 0.13908
Total Iteration Time: 4.24536

Cumulative Model Updates: 5,876
Cumulative Timesteps: 24,563,516

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.82126
Policy Entropy: 3.32091
Value Function Loss: 3.93816

Mean KL Divergence: 0.04283
SB3 Clip Fraction: 0.42930
Policy Update Magnitude: 1.71511
Value Function Update Magnitude: 1.55222

Collected Steps per Second: 24,530.95002
Overall Steps per Second: 11,668.01489

Timestep Collection Time: 2.03938
Timestep Consumption Time: 2.24824
PPO Batch Consumption Time: 0.13877
Total Iteration Time: 4.28762

Cumulative Model Updates: 5,888
Cumulative Timesteps: 24,613,544

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 189.19864
Policy Entropy: 3.31806
Value Function Loss: 3.80911

Mean KL Divergence: 0.04271
SB3 Clip Fraction: 0.42879
Policy Update Magnitude: 1.69121
Value Function Update Magnitude: 1.51356

Collected Steps per Second: 22,522.31354
Overall Steps per Second: 11,266.52282

Timestep Collection Time: 2.22011
Timestep Consumption Time: 2.21799
PPO Batch Consumption Time: 0.14054
Total Iteration Time: 4.43810

Cumulative Model Updates: 5,900
Cumulative Timesteps: 24,663,546

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 192.04414
Policy Entropy: 3.31743
Value Function Loss: 3.72939

Mean KL Divergence: 0.04258
SB3 Clip Fraction: 0.42793
Policy Update Magnitude: 1.70125
Value Function Update Magnitude: 1.49088

Collected Steps per Second: 23,661.89406
Overall Steps per Second: 11,458.27840

Timestep Collection Time: 2.11386
Timestep Consumption Time: 2.25137
PPO Batch Consumption Time: 0.13970
Total Iteration Time: 4.36523

Cumulative Model Updates: 5,912
Cumulative Timesteps: 24,713,564

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 189.89153
Policy Entropy: 3.32581
Value Function Loss: 3.64921

Mean KL Divergence: 0.04234
SB3 Clip Fraction: 0.42743
Policy Update Magnitude: 1.70042
Value Function Update Magnitude: 1.45814

Collected Steps per Second: 23,173.28634
Overall Steps per Second: 11,351.48565

Timestep Collection Time: 2.15835
Timestep Consumption Time: 2.24777
PPO Batch Consumption Time: 0.14007
Total Iteration Time: 4.40612

Cumulative Model Updates: 5,924
Cumulative Timesteps: 24,763,580

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.57999
Policy Entropy: 3.31866
Value Function Loss: 3.61059

Mean KL Divergence: 0.04186
SB3 Clip Fraction: 0.42489
Policy Update Magnitude: 1.68969
Value Function Update Magnitude: 1.47550

Collected Steps per Second: 24,171.39233
Overall Steps per Second: 11,771.63120

Timestep Collection Time: 2.06988
Timestep Consumption Time: 2.18033
PPO Batch Consumption Time: 0.13936
Total Iteration Time: 4.25022

Cumulative Model Updates: 5,936
Cumulative Timesteps: 24,813,612

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 171.12795
Policy Entropy: 3.31749
Value Function Loss: 3.55868

Mean KL Divergence: 0.04163
SB3 Clip Fraction: 0.42337
Policy Update Magnitude: 1.67946
Value Function Update Magnitude: 1.47471

Collected Steps per Second: 23,667.91045
Overall Steps per Second: 11,480.37447

Timestep Collection Time: 2.11476
Timestep Consumption Time: 2.24503
PPO Batch Consumption Time: 0.13879
Total Iteration Time: 4.35979

Cumulative Model Updates: 5,948
Cumulative Timesteps: 24,863,664

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 187.30721
Policy Entropy: 3.31564
Value Function Loss: 3.72035

Mean KL Divergence: 0.04158
SB3 Clip Fraction: 0.42509
Policy Update Magnitude: 1.71134
Value Function Update Magnitude: 1.49872

Collected Steps per Second: 23,067.93795
Overall Steps per Second: 11,447.33125

Timestep Collection Time: 2.16768
Timestep Consumption Time: 2.20050
PPO Batch Consumption Time: 0.13914
Total Iteration Time: 4.36818

Cumulative Model Updates: 5,960
Cumulative Timesteps: 24,913,668

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.85740
Policy Entropy: 3.31927
Value Function Loss: 3.66923

Mean KL Divergence: 0.04188
SB3 Clip Fraction: 0.42679
Policy Update Magnitude: 1.69527
Value Function Update Magnitude: 1.57104

Collected Steps per Second: 24,575.81084
Overall Steps per Second: 11,710.22927

Timestep Collection Time: 2.03566
Timestep Consumption Time: 2.23650
PPO Batch Consumption Time: 0.13761
Total Iteration Time: 4.27216

Cumulative Model Updates: 5,972
Cumulative Timesteps: 24,963,696

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.74536
Policy Entropy: 3.31905
Value Function Loss: 3.63575

Mean KL Divergence: 0.04199
SB3 Clip Fraction: 0.42447
Policy Update Magnitude: 1.68080
Value Function Update Magnitude: 1.56604

Collected Steps per Second: 24,974.92470
Overall Steps per Second: 11,881.32209

Timestep Collection Time: 2.00257
Timestep Consumption Time: 2.20690
PPO Batch Consumption Time: 0.13801
Total Iteration Time: 4.20946

Cumulative Model Updates: 5,984
Cumulative Timesteps: 25,013,710

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 226.36856
Policy Entropy: 3.32464
Value Function Loss: 3.48241

Mean KL Divergence: 0.04130
SB3 Clip Fraction: 0.42438
Policy Update Magnitude: 1.68535
Value Function Update Magnitude: 1.50707

Collected Steps per Second: 25,143.96131
Overall Steps per Second: 11,942.97127

Timestep Collection Time: 1.99038
Timestep Consumption Time: 2.20004
PPO Batch Consumption Time: 0.13756
Total Iteration Time: 4.19041

Cumulative Model Updates: 5,996
Cumulative Timesteps: 25,063,756

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 188.80542
Policy Entropy: 3.32356
Value Function Loss: 3.41951

Mean KL Divergence: 0.04112
SB3 Clip Fraction: 0.42311
Policy Update Magnitude: 1.67654
Value Function Update Magnitude: 1.52468

Collected Steps per Second: 24,139.22743
Overall Steps per Second: 11,710.40266

Timestep Collection Time: 2.07206
Timestep Consumption Time: 2.19918
PPO Batch Consumption Time: 0.13756
Total Iteration Time: 4.27125

Cumulative Model Updates: 6,008
Cumulative Timesteps: 25,113,774

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.41274
Policy Entropy: 3.32454
Value Function Loss: 3.39802

Mean KL Divergence: 0.04107
SB3 Clip Fraction: 0.42372
Policy Update Magnitude: 1.66299
Value Function Update Magnitude: 1.57235

Collected Steps per Second: 24,461.69615
Overall Steps per Second: 11,880.63163

Timestep Collection Time: 2.04728
Timestep Consumption Time: 2.16798
PPO Batch Consumption Time: 0.13742
Total Iteration Time: 4.21526

Cumulative Model Updates: 6,020
Cumulative Timesteps: 25,163,854

Timesteps Collected: 50,080
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.45708
Policy Entropy: 3.31869
Value Function Loss: 3.41356

Mean KL Divergence: 0.04191
SB3 Clip Fraction: 0.42774
Policy Update Magnitude: 1.68354
Value Function Update Magnitude: 1.56291

Collected Steps per Second: 25,333.42778
Overall Steps per Second: 11,893.71889

Timestep Collection Time: 1.97510
Timestep Consumption Time: 2.23183
PPO Batch Consumption Time: 0.13737
Total Iteration Time: 4.20693

Cumulative Model Updates: 6,032
Cumulative Timesteps: 25,213,890

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 186.93989
Policy Entropy: 3.31256
Value Function Loss: 3.64810

Mean KL Divergence: 0.04304
SB3 Clip Fraction: 0.43353
Policy Update Magnitude: 1.71210
Value Function Update Magnitude: 1.59088

Collected Steps per Second: 25,018.49230
Overall Steps per Second: 11,922.56823

Timestep Collection Time: 1.99948
Timestep Consumption Time: 2.19626
PPO Batch Consumption Time: 0.13789
Total Iteration Time: 4.19574

Cumulative Model Updates: 6,044
Cumulative Timesteps: 25,263,914

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 204.40506
Policy Entropy: 3.31429
Value Function Loss: 3.67824

Mean KL Divergence: 0.04309
SB3 Clip Fraction: 0.42876
Policy Update Magnitude: 1.69588
Value Function Update Magnitude: 1.60884

Collected Steps per Second: 25,694.80079
Overall Steps per Second: 12,026.82744

Timestep Collection Time: 1.94685
Timestep Consumption Time: 2.21251
PPO Batch Consumption Time: 0.13731
Total Iteration Time: 4.15937

Cumulative Model Updates: 6,056
Cumulative Timesteps: 25,313,938

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 167.27343
Policy Entropy: 3.30538
Value Function Loss: 3.58650

Mean KL Divergence: 0.04297
SB3 Clip Fraction: 0.42829
Policy Update Magnitude: 1.68537
Value Function Update Magnitude: 1.61743

Collected Steps per Second: 23,961.60356
Overall Steps per Second: 11,682.36470

Timestep Collection Time: 2.08717
Timestep Consumption Time: 2.19381
PPO Batch Consumption Time: 0.13728
Total Iteration Time: 4.28098

Cumulative Model Updates: 6,068
Cumulative Timesteps: 25,363,950

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 178.08407
Policy Entropy: 3.30763
Value Function Loss: 3.55418

Mean KL Divergence: 0.04313
SB3 Clip Fraction: 0.43415
Policy Update Magnitude: 1.69154
Value Function Update Magnitude: 1.60707

Collected Steps per Second: 26,087.06438
Overall Steps per Second: 12,220.45162

Timestep Collection Time: 1.91881
Timestep Consumption Time: 2.17728
PPO Batch Consumption Time: 0.13834
Total Iteration Time: 4.09608

Cumulative Model Updates: 6,080
Cumulative Timesteps: 25,414,006

Timesteps Collected: 50,056
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 280.52649
Policy Entropy: 3.30691
Value Function Loss: 3.68544

Mean KL Divergence: 0.04303
SB3 Clip Fraction: 0.43197
Policy Update Magnitude: 1.71083
Value Function Update Magnitude: 1.68110

Collected Steps per Second: 24,613.09923
Overall Steps per Second: 11,712.97714

Timestep Collection Time: 2.03298
Timestep Consumption Time: 2.23903
PPO Batch Consumption Time: 0.13822
Total Iteration Time: 4.27201

Cumulative Model Updates: 6,092
Cumulative Timesteps: 25,464,044

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 184.85802
Policy Entropy: 3.30225
Value Function Loss: 3.78095

Mean KL Divergence: 0.04341
SB3 Clip Fraction: 0.43033
Policy Update Magnitude: 1.69845
Value Function Update Magnitude: 1.65737

Collected Steps per Second: 25,859.94930
Overall Steps per Second: 12,038.86826

Timestep Collection Time: 1.93427
Timestep Consumption Time: 2.22061
PPO Batch Consumption Time: 0.13884
Total Iteration Time: 4.15488

Cumulative Model Updates: 6,104
Cumulative Timesteps: 25,514,064

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 168.39739
Policy Entropy: 3.29502
Value Function Loss: 3.82913

Mean KL Divergence: 0.04400
SB3 Clip Fraction: 0.43368
Policy Update Magnitude: 1.68584
Value Function Update Magnitude: 1.60654

Collected Steps per Second: 26,197.61401
Overall Steps per Second: 12,071.71043

Timestep Collection Time: 1.90865
Timestep Consumption Time: 2.23343
PPO Batch Consumption Time: 0.13782
Total Iteration Time: 4.14208

Cumulative Model Updates: 6,116
Cumulative Timesteps: 25,564,066

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.51397
Policy Entropy: 3.28591
Value Function Loss: 3.74508

Mean KL Divergence: 0.04409
SB3 Clip Fraction: 0.43477
Policy Update Magnitude: 1.68271
Value Function Update Magnitude: 1.64542

Collected Steps per Second: 24,113.12548
Overall Steps per Second: 11,529.80901

Timestep Collection Time: 2.07555
Timestep Consumption Time: 2.26520
PPO Batch Consumption Time: 0.13777
Total Iteration Time: 4.34075

Cumulative Model Updates: 6,128
Cumulative Timesteps: 25,614,114

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 217.18183
Policy Entropy: 3.28002
Value Function Loss: 3.67255

Mean KL Divergence: 0.04421
SB3 Clip Fraction: 0.43668
Policy Update Magnitude: 1.67704
Value Function Update Magnitude: 1.60031

Collected Steps per Second: 27,408.57046
Overall Steps per Second: 12,257.44797

Timestep Collection Time: 1.82476
Timestep Consumption Time: 2.25554
PPO Batch Consumption Time: 0.14085
Total Iteration Time: 4.08029

Cumulative Model Updates: 6,140
Cumulative Timesteps: 25,664,128

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 175.12089
Policy Entropy: 3.27905
Value Function Loss: 3.79233

Mean KL Divergence: 0.04442
SB3 Clip Fraction: 0.43620
Policy Update Magnitude: 1.71557
Value Function Update Magnitude: 1.58790

Collected Steps per Second: 25,653.23768
Overall Steps per Second: 11,938.01554

Timestep Collection Time: 1.95079
Timestep Consumption Time: 2.24120
PPO Batch Consumption Time: 0.13879
Total Iteration Time: 4.19199

Cumulative Model Updates: 6,152
Cumulative Timesteps: 25,714,172

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 188.84321
Policy Entropy: 3.27647
Value Function Loss: 3.96020

Mean KL Divergence: 0.04521
SB3 Clip Fraction: 0.44280
Policy Update Magnitude: 1.71508
Value Function Update Magnitude: 1.60436

Collected Steps per Second: 24,459.15854
Overall Steps per Second: 11,862.27109

Timestep Collection Time: 2.04619
Timestep Consumption Time: 2.17290
PPO Batch Consumption Time: 0.13861
Total Iteration Time: 4.21909

Cumulative Model Updates: 6,164
Cumulative Timesteps: 25,764,220

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 179.93522
Policy Entropy: 3.27836
Value Function Loss: 3.96716

Mean KL Divergence: 0.04534
SB3 Clip Fraction: 0.44088
Policy Update Magnitude: 1.70154
Value Function Update Magnitude: 1.59070

Collected Steps per Second: 25,589.75763
Overall Steps per Second: 11,942.29975

Timestep Collection Time: 1.95524
Timestep Consumption Time: 2.23441
PPO Batch Consumption Time: 0.13794
Total Iteration Time: 4.18965

Cumulative Model Updates: 6,176
Cumulative Timesteps: 25,814,254

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.36828
Policy Entropy: 3.28065
Value Function Loss: 3.74779

Mean KL Divergence: 0.04475
SB3 Clip Fraction: 0.43769
Policy Update Magnitude: 1.68693
Value Function Update Magnitude: 1.55413

Collected Steps per Second: 25,864.50848
Overall Steps per Second: 12,047.14026

Timestep Collection Time: 1.93439
Timestep Consumption Time: 2.21863
PPO Batch Consumption Time: 0.13774
Total Iteration Time: 4.15302

Cumulative Model Updates: 6,188
Cumulative Timesteps: 25,864,286

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 175.91147
Policy Entropy: 3.28285
Value Function Loss: 3.71408

Mean KL Divergence: 0.04429
SB3 Clip Fraction: 0.43823
Policy Update Magnitude: 1.70302
Value Function Update Magnitude: 1.54531

Collected Steps per Second: 24,341.81899
Overall Steps per Second: 11,852.49697

Timestep Collection Time: 2.05432
Timestep Consumption Time: 2.16470
PPO Batch Consumption Time: 0.13815
Total Iteration Time: 4.21903

Cumulative Model Updates: 6,200
Cumulative Timesteps: 25,914,292

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 187.55644
Policy Entropy: 3.28318
Value Function Loss: 3.62073

Mean KL Divergence: 0.04327
SB3 Clip Fraction: 0.43351
Policy Update Magnitude: 1.68056
Value Function Update Magnitude: 1.52260

Collected Steps per Second: 25,274.72955
Overall Steps per Second: 11,872.51495

Timestep Collection Time: 1.97921
Timestep Consumption Time: 2.23422
PPO Batch Consumption Time: 0.13825
Total Iteration Time: 4.21343

Cumulative Model Updates: 6,212
Cumulative Timesteps: 25,964,316

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 191.95552
Policy Entropy: 3.27684
Value Function Loss: 3.75525

Mean KL Divergence: 0.04347
SB3 Clip Fraction: 0.43376
Policy Update Magnitude: 1.69514
Value Function Update Magnitude: 1.54197

Collected Steps per Second: 25,926.02979
Overall Steps per Second: 12,200.94106

Timestep Collection Time: 1.92910
Timestep Consumption Time: 2.17009
PPO Batch Consumption Time: 0.13787
Total Iteration Time: 4.09919

Cumulative Model Updates: 6,224
Cumulative Timesteps: 26,014,330

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 188.73679
Policy Entropy: 3.27421
Value Function Loss: 3.70634

Mean KL Divergence: 0.04354
SB3 Clip Fraction: 0.43685
Policy Update Magnitude: 1.69948
Value Function Update Magnitude: 1.49779

Collected Steps per Second: 25,252.29837
Overall Steps per Second: 11,852.06341

Timestep Collection Time: 1.98121
Timestep Consumption Time: 2.24000
PPO Batch Consumption Time: 0.13809
Total Iteration Time: 4.22121

Cumulative Model Updates: 6,236
Cumulative Timesteps: 26,064,360

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.50769
Policy Entropy: 3.27587
Value Function Loss: 3.85579

Mean KL Divergence: 0.04457
SB3 Clip Fraction: 0.43957
Policy Update Magnitude: 1.70528
Value Function Update Magnitude: 1.49430

Collected Steps per Second: 25,594.86134
Overall Steps per Second: 11,989.12722

Timestep Collection Time: 1.95500
Timestep Consumption Time: 2.21861
PPO Batch Consumption Time: 0.13798
Total Iteration Time: 4.17361

Cumulative Model Updates: 6,248
Cumulative Timesteps: 26,114,398

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 180.50531
Policy Entropy: 3.27667
Value Function Loss: 3.90244

Mean KL Divergence: 0.04525
SB3 Clip Fraction: 0.44220
Policy Update Magnitude: 1.71366
Value Function Update Magnitude: 1.49763

Collected Steps per Second: 25,703.23746
Overall Steps per Second: 12,025.35142

Timestep Collection Time: 1.94707
Timestep Consumption Time: 2.21464
PPO Batch Consumption Time: 0.13764
Total Iteration Time: 4.16171

Cumulative Model Updates: 6,260
Cumulative Timesteps: 26,164,444

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 150.35615
Policy Entropy: 3.27528
Value Function Loss: 3.86716

Mean KL Divergence: 0.04552
SB3 Clip Fraction: 0.44144
Policy Update Magnitude: 1.70549
Value Function Update Magnitude: 1.62541

Collected Steps per Second: 24,515.79540
Overall Steps per Second: 11,777.25327

Timestep Collection Time: 2.04130
Timestep Consumption Time: 2.20791
PPO Batch Consumption Time: 0.13798
Total Iteration Time: 4.24921

Cumulative Model Updates: 6,272
Cumulative Timesteps: 26,214,488

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 262.50229
Policy Entropy: 3.27170
Value Function Loss: 3.69070

Mean KL Divergence: 0.04512
SB3 Clip Fraction: 0.43923
Policy Update Magnitude: 1.68289
Value Function Update Magnitude: 1.69991

Collected Steps per Second: 25,082.25982
Overall Steps per Second: 12,036.79602

Timestep Collection Time: 1.99535
Timestep Consumption Time: 2.16256
PPO Batch Consumption Time: 0.13796
Total Iteration Time: 4.15792

Cumulative Model Updates: 6,284
Cumulative Timesteps: 26,264,536

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 180.56455
Policy Entropy: 3.26790
Value Function Loss: 3.59781

Mean KL Divergence: 0.04402
SB3 Clip Fraction: 0.43540
Policy Update Magnitude: 1.67767
Value Function Update Magnitude: 1.71293

Collected Steps per Second: 26,123.95445
Overall Steps per Second: 11,963.67869

Timestep Collection Time: 1.91541
Timestep Consumption Time: 2.26709
PPO Batch Consumption Time: 0.13744
Total Iteration Time: 4.18249

Cumulative Model Updates: 6,296
Cumulative Timesteps: 26,314,574

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 231.31086
Policy Entropy: 3.26418
Value Function Loss: 3.63942

Mean KL Divergence: 0.04399
SB3 Clip Fraction: 0.43466
Policy Update Magnitude: 1.68504
Value Function Update Magnitude: 2.03892

Collected Steps per Second: 25,304.38743
Overall Steps per Second: 11,899.43422

Timestep Collection Time: 1.97610
Timestep Consumption Time: 2.22612
PPO Batch Consumption Time: 0.13850
Total Iteration Time: 4.20222

Cumulative Model Updates: 6,308
Cumulative Timesteps: 26,364,578

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 188.24807
Policy Entropy: 3.27040
Value Function Loss: 3.47442

Mean KL Divergence: 0.04383
SB3 Clip Fraction: 0.43370
Policy Update Magnitude: 1.63872
Value Function Update Magnitude: 2.23458

Collected Steps per Second: 27,346.21213
Overall Steps per Second: 12,281.16684

Timestep Collection Time: 1.82943
Timestep Consumption Time: 2.24412
PPO Batch Consumption Time: 0.13805
Total Iteration Time: 4.07355

Cumulative Model Updates: 6,320
Cumulative Timesteps: 26,414,606

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 235.83007
Policy Entropy: 3.26953
Value Function Loss: 3.45171

Mean KL Divergence: 0.04252
SB3 Clip Fraction: 0.42285
Policy Update Magnitude: 1.62486
Value Function Update Magnitude: 2.03419

Collected Steps per Second: 25,622.43976
Overall Steps per Second: 12,049.00687

Timestep Collection Time: 1.95282
Timestep Consumption Time: 2.19989
PPO Batch Consumption Time: 0.13714
Total Iteration Time: 4.15271

Cumulative Model Updates: 6,332
Cumulative Timesteps: 26,464,642

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.17635
Policy Entropy: 3.25889
Value Function Loss: 3.12866

Mean KL Divergence: 0.04152
SB3 Clip Fraction: 0.41625
Policy Update Magnitude: 1.64376
Value Function Update Magnitude: 2.63379

Collected Steps per Second: 24,380.10154
Overall Steps per Second: 11,779.67529

Timestep Collection Time: 2.05225
Timestep Consumption Time: 2.19524
PPO Batch Consumption Time: 0.13805
Total Iteration Time: 4.24749

Cumulative Model Updates: 6,344
Cumulative Timesteps: 26,514,676

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.15345
Policy Entropy: 3.25036
Value Function Loss: 3.21346

Mean KL Divergence: 0.04238
SB3 Clip Fraction: 0.42412
Policy Update Magnitude: 1.69597
Value Function Update Magnitude: 3.34899

Collected Steps per Second: 25,563.24923
Overall Steps per Second: 11,960.52773

Timestep Collection Time: 1.95805
Timestep Consumption Time: 2.22689
PPO Batch Consumption Time: 0.13798
Total Iteration Time: 4.18493

Cumulative Model Updates: 6,356
Cumulative Timesteps: 26,564,730

Timesteps Collected: 50,054
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 203.79666
Policy Entropy: 3.24431
Value Function Loss: 3.34556

Mean KL Divergence: 0.04314
SB3 Clip Fraction: 0.42782
Policy Update Magnitude: 1.68570
Value Function Update Magnitude: 2.68347

Collected Steps per Second: 26,034.89271
Overall Steps per Second: 12,080.32057

Timestep Collection Time: 1.92219
Timestep Consumption Time: 2.22042
PPO Batch Consumption Time: 0.13880
Total Iteration Time: 4.14261

Cumulative Model Updates: 6,368
Cumulative Timesteps: 26,614,774

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 217.16095
Policy Entropy: 3.23221
Value Function Loss: 3.52950

Mean KL Divergence: 0.04284
SB3 Clip Fraction: 0.42589
Policy Update Magnitude: 1.65436
Value Function Update Magnitude: 1.88912

Collected Steps per Second: 25,412.93350
Overall Steps per Second: 11,801.41382

Timestep Collection Time: 1.96860
Timestep Consumption Time: 2.27055
PPO Batch Consumption Time: 0.14025
Total Iteration Time: 4.23915

Cumulative Model Updates: 6,380
Cumulative Timesteps: 26,664,802

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 188.19246
Policy Entropy: 3.22935
Value Function Loss: 3.46790

Mean KL Divergence: 0.04211
SB3 Clip Fraction: 0.41935
Policy Update Magnitude: 1.61105
Value Function Update Magnitude: 1.87297

Collected Steps per Second: 24,749.26318
Overall Steps per Second: 11,775.48009

Timestep Collection Time: 2.02285
Timestep Consumption Time: 2.22870
PPO Batch Consumption Time: 0.13774
Total Iteration Time: 4.25155

Cumulative Model Updates: 6,392
Cumulative Timesteps: 26,714,866

Timesteps Collected: 50,064
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 238.91936
Policy Entropy: 3.22196
Value Function Loss: 3.19768

Mean KL Divergence: 0.04076
SB3 Clip Fraction: 0.41021
Policy Update Magnitude: 1.55434
Value Function Update Magnitude: 2.24000

Collected Steps per Second: 22,939.83160
Overall Steps per Second: 11,289.97266

Timestep Collection Time: 2.18162
Timestep Consumption Time: 2.25116
PPO Batch Consumption Time: 0.13845
Total Iteration Time: 4.43278

Cumulative Model Updates: 6,404
Cumulative Timesteps: 26,764,912

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 186.02124
Policy Entropy: 3.22523
Value Function Loss: 2.70704

Mean KL Divergence: 0.03881
SB3 Clip Fraction: 0.40078
Policy Update Magnitude: 1.52691
Value Function Update Magnitude: 3.16105

Collected Steps per Second: 24,523.83708
Overall Steps per Second: 11,618.90745

Timestep Collection Time: 2.03981
Timestep Consumption Time: 2.26558
PPO Batch Consumption Time: 0.14069
Total Iteration Time: 4.30540

Cumulative Model Updates: 6,416
Cumulative Timesteps: 26,814,936

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 209.28385
Policy Entropy: 3.19890
Value Function Loss: 2.66350

Mean KL Divergence: 0.03849
SB3 Clip Fraction: 0.40333
Policy Update Magnitude: 1.56185
Value Function Update Magnitude: 2.32961

Collected Steps per Second: 25,575.50900
Overall Steps per Second: 12,115.34142

Timestep Collection Time: 1.95672
Timestep Consumption Time: 2.17391
PPO Batch Consumption Time: 0.13810
Total Iteration Time: 4.13063

Cumulative Model Updates: 6,428
Cumulative Timesteps: 26,864,980

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.56899
Policy Entropy: 3.18316
Value Function Loss: 2.71798

Mean KL Divergence: 0.03933
SB3 Clip Fraction: 0.40601
Policy Update Magnitude: 1.53590
Value Function Update Magnitude: 2.24305

Collected Steps per Second: 25,478.73986
Overall Steps per Second: 11,919.14792

Timestep Collection Time: 1.96352
Timestep Consumption Time: 2.23376
PPO Batch Consumption Time: 0.13787
Total Iteration Time: 4.19728

Cumulative Model Updates: 6,440
Cumulative Timesteps: 26,915,008

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 184.03406
Policy Entropy: 3.17035
Value Function Loss: 2.67585

Mean KL Divergence: 0.03983
SB3 Clip Fraction: 0.40514
Policy Update Magnitude: 1.50984
Value Function Update Magnitude: 2.29336

Collected Steps per Second: 24,326.81324
Overall Steps per Second: 11,760.61692

Timestep Collection Time: 2.05625
Timestep Consumption Time: 2.19710
PPO Batch Consumption Time: 0.13756
Total Iteration Time: 4.25335

Cumulative Model Updates: 6,452
Cumulative Timesteps: 26,965,030

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 244.09997
Policy Entropy: 3.16098
Value Function Loss: 2.51253

Mean KL Divergence: 0.03911
SB3 Clip Fraction: 0.40372
Policy Update Magnitude: 1.50579
Value Function Update Magnitude: 2.40094

Collected Steps per Second: 25,769.41669
Overall Steps per Second: 12,155.90030

Timestep Collection Time: 1.94091
Timestep Consumption Time: 2.17364
PPO Batch Consumption Time: 0.13779
Total Iteration Time: 4.11455

Cumulative Model Updates: 6,464
Cumulative Timesteps: 27,015,046

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 190.49695
Policy Entropy: 3.14841
Value Function Loss: 2.51604

Mean KL Divergence: 0.03961
SB3 Clip Fraction: 0.40662
Policy Update Magnitude: 1.54029
Value Function Update Magnitude: 2.30168

Collected Steps per Second: 22,438.84281
Overall Steps per Second: 11,213.36162

Timestep Collection Time: 2.22971
Timestep Consumption Time: 2.23211
PPO Batch Consumption Time: 0.13742
Total Iteration Time: 4.46182

Cumulative Model Updates: 6,476
Cumulative Timesteps: 27,065,078

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 192.34260
Policy Entropy: 3.12400
Value Function Loss: 2.54933

Mean KL Divergence: 0.04047
SB3 Clip Fraction: 0.41004
Policy Update Magnitude: 1.53034
Value Function Update Magnitude: 2.09344

Collected Steps per Second: 23,626.96553
Overall Steps per Second: 11,641.08039

Timestep Collection Time: 2.11648
Timestep Consumption Time: 2.17917
PPO Batch Consumption Time: 0.13835
Total Iteration Time: 4.29565

Cumulative Model Updates: 6,488
Cumulative Timesteps: 27,115,084

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 179.80832
Policy Entropy: 3.09431
Value Function Loss: 2.79344

Mean KL Divergence: 0.04120
SB3 Clip Fraction: 0.41403
Policy Update Magnitude: 1.55199
Value Function Update Magnitude: 1.98079

Collected Steps per Second: 21,683.48114
Overall Steps per Second: 10,929.70551

Timestep Collection Time: 2.30793
Timestep Consumption Time: 2.27078
PPO Batch Consumption Time: 0.13821
Total Iteration Time: 4.57871

Cumulative Model Updates: 6,500
Cumulative Timesteps: 27,165,128

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 186.59290
Policy Entropy: 3.07624
Value Function Loss: 2.68128

Mean KL Divergence: 0.04084
SB3 Clip Fraction: 0.41076
Policy Update Magnitude: 1.53166
Value Function Update Magnitude: 1.84172

Collected Steps per Second: 21,458.19257
Overall Steps per Second: 10,910.63342

Timestep Collection Time: 2.33067
Timestep Consumption Time: 2.25311
PPO Batch Consumption Time: 0.13787
Total Iteration Time: 4.58379

Cumulative Model Updates: 6,512
Cumulative Timesteps: 27,215,140

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 207.48901
Policy Entropy: 3.05312
Value Function Loss: 2.74363

Mean KL Divergence: 0.04132
SB3 Clip Fraction: 0.41380
Policy Update Magnitude: 1.52871
Value Function Update Magnitude: 1.95163

Collected Steps per Second: 23,704.51909
Overall Steps per Second: 11,488.35434

Timestep Collection Time: 2.11124
Timestep Consumption Time: 2.24499
PPO Batch Consumption Time: 0.13754
Total Iteration Time: 4.35624

Cumulative Model Updates: 6,524
Cumulative Timesteps: 27,265,186

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.68966
Policy Entropy: 3.03450
Value Function Loss: 2.79927

Mean KL Divergence: 0.04171
SB3 Clip Fraction: 0.41876
Policy Update Magnitude: 1.56288
Value Function Update Magnitude: 1.93685

Collected Steps per Second: 21,644.53655
Overall Steps per Second: 10,994.31694

Timestep Collection Time: 2.31135
Timestep Consumption Time: 2.23901
PPO Batch Consumption Time: 0.13925
Total Iteration Time: 4.55035

Cumulative Model Updates: 6,536
Cumulative Timesteps: 27,315,214

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 236.86448
Policy Entropy: 3.01016
Value Function Loss: 2.92431

Mean KL Divergence: 0.04335
SB3 Clip Fraction: 0.42493
Policy Update Magnitude: 1.58015
Value Function Update Magnitude: 1.92043

Collected Steps per Second: 23,074.25924
Overall Steps per Second: 11,509.64516

Timestep Collection Time: 2.16952
Timestep Consumption Time: 2.17988
PPO Batch Consumption Time: 0.13888
Total Iteration Time: 4.34940

Cumulative Model Updates: 6,548
Cumulative Timesteps: 27,365,274

Timesteps Collected: 50,060
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.69034
Policy Entropy: 2.99450
Value Function Loss: 2.93789

Mean KL Divergence: 0.04482
SB3 Clip Fraction: 0.42914
Policy Update Magnitude: 1.59306
Value Function Update Magnitude: 1.91543

Collected Steps per Second: 22,750.20403
Overall Steps per Second: 11,238.52298

Timestep Collection Time: 2.19866
Timestep Consumption Time: 2.25210
PPO Batch Consumption Time: 0.13841
Total Iteration Time: 4.45076

Cumulative Model Updates: 6,560
Cumulative Timesteps: 27,415,294

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 229.78159
Policy Entropy: 2.97510
Value Function Loss: 2.78777

Mean KL Divergence: 0.04486
SB3 Clip Fraction: 0.42649
Policy Update Magnitude: 1.57489
Value Function Update Magnitude: 1.90757

Collected Steps per Second: 21,764.44107
Overall Steps per Second: 10,966.09874

Timestep Collection Time: 2.29797
Timestep Consumption Time: 2.26281
PPO Batch Consumption Time: 0.13927
Total Iteration Time: 4.56078

Cumulative Model Updates: 6,572
Cumulative Timesteps: 27,465,308

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 205.24575
Policy Entropy: 2.95950
Value Function Loss: 2.77056

Mean KL Divergence: 0.04469
SB3 Clip Fraction: 0.42368
Policy Update Magnitude: 1.56622
Value Function Update Magnitude: 1.92757

Collected Steps per Second: 24,037.72799
Overall Steps per Second: 11,550.39748

Timestep Collection Time: 2.08090
Timestep Consumption Time: 2.24969
PPO Batch Consumption Time: 0.13900
Total Iteration Time: 4.33059

Cumulative Model Updates: 6,584
Cumulative Timesteps: 27,515,328

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 194.86124
Policy Entropy: 2.93834
Value Function Loss: 2.75117

Mean KL Divergence: 0.04402
SB3 Clip Fraction: 0.42593
Policy Update Magnitude: 1.55789
Value Function Update Magnitude: 1.95511

Collected Steps per Second: 24,688.25550
Overall Steps per Second: 11,727.31370

Timestep Collection Time: 2.02623
Timestep Consumption Time: 2.23937
PPO Batch Consumption Time: 0.13884
Total Iteration Time: 4.26560

Cumulative Model Updates: 6,596
Cumulative Timesteps: 27,565,352

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.75029
Policy Entropy: 2.91583
Value Function Loss: 2.77700

Mean KL Divergence: 0.04526
SB3 Clip Fraction: 0.42933
Policy Update Magnitude: 1.55427
Value Function Update Magnitude: 1.91579

Collected Steps per Second: 23,727.46525
Overall Steps per Second: 11,655.65531

Timestep Collection Time: 2.10811
Timestep Consumption Time: 2.18337
PPO Batch Consumption Time: 0.13867
Total Iteration Time: 4.29148

Cumulative Model Updates: 6,608
Cumulative Timesteps: 27,615,372

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.37322
Policy Entropy: 2.88405
Value Function Loss: 2.75986

Mean KL Divergence: 0.04558
SB3 Clip Fraction: 0.42920
Policy Update Magnitude: 1.56783
Value Function Update Magnitude: 1.96916

Collected Steps per Second: 23,188.29088
Overall Steps per Second: 11,291.23342

Timestep Collection Time: 2.15902
Timestep Consumption Time: 2.27486
PPO Batch Consumption Time: 0.13855
Total Iteration Time: 4.43388

Cumulative Model Updates: 6,620
Cumulative Timesteps: 27,665,436

Timesteps Collected: 50,064
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.09832
Policy Entropy: 2.84880
Value Function Loss: 2.74833

Mean KL Divergence: 0.04643
SB3 Clip Fraction: 0.43222
Policy Update Magnitude: 1.56123
Value Function Update Magnitude: 1.96129

Collected Steps per Second: 23,753.91980
Overall Steps per Second: 11,573.76376

Timestep Collection Time: 2.10626
Timestep Consumption Time: 2.21662
PPO Batch Consumption Time: 0.13804
Total Iteration Time: 4.32288

Cumulative Model Updates: 6,632
Cumulative Timesteps: 27,715,468

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 244.64953
Policy Entropy: 2.84410
Value Function Loss: 2.81059

Mean KL Divergence: 0.04617
SB3 Clip Fraction: 0.42940
Policy Update Magnitude: 1.56140
Value Function Update Magnitude: 1.86148

Collected Steps per Second: 24,734.40376
Overall Steps per Second: 11,701.10168

Timestep Collection Time: 2.02277
Timestep Consumption Time: 2.25307
PPO Batch Consumption Time: 0.13810
Total Iteration Time: 4.27584

Cumulative Model Updates: 6,644
Cumulative Timesteps: 27,765,500

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 198.01455
Policy Entropy: 2.82629
Value Function Loss: 2.74153

Mean KL Divergence: 0.04619
SB3 Clip Fraction: 0.42912
Policy Update Magnitude: 1.55251
Value Function Update Magnitude: 1.95544

Collected Steps per Second: 22,709.64818
Overall Steps per Second: 11,254.13731

Timestep Collection Time: 2.20276
Timestep Consumption Time: 2.24218
PPO Batch Consumption Time: 0.13776
Total Iteration Time: 4.44494

Cumulative Model Updates: 6,656
Cumulative Timesteps: 27,815,524

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.43499
Policy Entropy: 2.81092
Value Function Loss: 2.78651

Mean KL Divergence: 0.04696
SB3 Clip Fraction: 0.43370
Policy Update Magnitude: 1.56320
Value Function Update Magnitude: 1.90672

Collected Steps per Second: 23,721.94647
Overall Steps per Second: 11,672.25542

Timestep Collection Time: 2.10910
Timestep Consumption Time: 2.17730
PPO Batch Consumption Time: 0.13855
Total Iteration Time: 4.28640

Cumulative Model Updates: 6,668
Cumulative Timesteps: 27,865,556

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 235.53388
Policy Entropy: 2.77431
Value Function Loss: 2.70671

Mean KL Divergence: 0.04694
SB3 Clip Fraction: 0.43237
Policy Update Magnitude: 1.54890
Value Function Update Magnitude: 1.95907

Collected Steps per Second: 23,727.79259
Overall Steps per Second: 11,537.58161

Timestep Collection Time: 2.10900
Timestep Consumption Time: 2.22830
PPO Batch Consumption Time: 0.13749
Total Iteration Time: 4.33730

Cumulative Model Updates: 6,680
Cumulative Timesteps: 27,915,598

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 193.96705
Policy Entropy: 2.74562
Value Function Loss: 2.68779

Mean KL Divergence: 0.04746
SB3 Clip Fraction: 0.43354
Policy Update Magnitude: 1.54202
Value Function Update Magnitude: 1.65527

Collected Steps per Second: 22,891.93343
Overall Steps per Second: 11,414.00070

Timestep Collection Time: 2.18575
Timestep Consumption Time: 2.19799
PPO Batch Consumption Time: 0.13746
Total Iteration Time: 4.38374

Cumulative Model Updates: 6,692
Cumulative Timesteps: 27,965,634

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 210.11223
Policy Entropy: 2.72589
Value Function Loss: 2.80856

Mean KL Divergence: 0.04708
SB3 Clip Fraction: 0.43420
Policy Update Magnitude: 1.57511
Value Function Update Magnitude: 1.67137

Collected Steps per Second: 23,748.75752
Overall Steps per Second: 11,524.45808

Timestep Collection Time: 2.10537
Timestep Consumption Time: 2.23323
PPO Batch Consumption Time: 0.13859
Total Iteration Time: 4.33860

Cumulative Model Updates: 6,704
Cumulative Timesteps: 28,015,634

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 248.94487
Policy Entropy: 2.70035
Value Function Loss: 3.07193

Mean KL Divergence: 0.04872
SB3 Clip Fraction: 0.43908
Policy Update Magnitude: 1.56692
Value Function Update Magnitude: 1.75422

Collected Steps per Second: 24,755.18732
Overall Steps per Second: 11,732.79292

Timestep Collection Time: 2.02067
Timestep Consumption Time: 2.24277
PPO Batch Consumption Time: 0.13900
Total Iteration Time: 4.26343

Cumulative Model Updates: 6,716
Cumulative Timesteps: 28,065,656

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.19257
Policy Entropy: 2.67490
Value Function Loss: 3.08422

Mean KL Divergence: 0.05124
SB3 Clip Fraction: 0.44519
Policy Update Magnitude: 1.59280
Value Function Update Magnitude: 1.56671

Collected Steps per Second: 23,312.63326
Overall Steps per Second: 11,613.28153

Timestep Collection Time: 2.14527
Timestep Consumption Time: 2.16117
PPO Batch Consumption Time: 0.13787
Total Iteration Time: 4.30645

Cumulative Model Updates: 6,728
Cumulative Timesteps: 28,115,668

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 234.82543
Policy Entropy: 2.65356
Value Function Loss: 2.98452

Mean KL Divergence: 0.05307
SB3 Clip Fraction: 0.45138
Policy Update Magnitude: 1.59381
Value Function Update Magnitude: 1.78919

Collected Steps per Second: 24,348.52580
Overall Steps per Second: 11,639.77900

Timestep Collection Time: 2.05450
Timestep Consumption Time: 2.24318
PPO Batch Consumption Time: 0.13973
Total Iteration Time: 4.29768

Cumulative Model Updates: 6,740
Cumulative Timesteps: 28,165,692

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 229.75964
Policy Entropy: 2.63145
Value Function Loss: 2.81176

Mean KL Divergence: 0.05269
SB3 Clip Fraction: 0.44740
Policy Update Magnitude: 1.55624
Value Function Update Magnitude: 1.67319

Collected Steps per Second: 23,472.56507
Overall Steps per Second: 11,711.37685

Timestep Collection Time: 2.13159
Timestep Consumption Time: 2.14066
PPO Batch Consumption Time: 0.13662
Total Iteration Time: 4.27226

Cumulative Model Updates: 6,752
Cumulative Timesteps: 28,215,726

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 196.90315
Policy Entropy: 2.60599
Value Function Loss: 2.82889

Mean KL Divergence: 0.05141
SB3 Clip Fraction: 0.44585
Policy Update Magnitude: 1.56799
Value Function Update Magnitude: 1.70814

Collected Steps per Second: 23,099.11547
Overall Steps per Second: 11,277.17311

Timestep Collection Time: 2.16536
Timestep Consumption Time: 2.26997
PPO Batch Consumption Time: 0.13767
Total Iteration Time: 4.43533

Cumulative Model Updates: 6,764
Cumulative Timesteps: 28,265,744

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 206.66069
Policy Entropy: 2.57620
Value Function Loss: 2.84387

Mean KL Divergence: 0.05154
SB3 Clip Fraction: 0.44760
Policy Update Magnitude: 1.58797
Value Function Update Magnitude: 1.84993

Collected Steps per Second: 22,804.51993
Overall Steps per Second: 11,337.22038

Timestep Collection Time: 2.19255
Timestep Consumption Time: 2.21770
PPO Batch Consumption Time: 0.13864
Total Iteration Time: 4.41025

Cumulative Model Updates: 6,776
Cumulative Timesteps: 28,315,744

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 212.41735
Policy Entropy: 2.54813
Value Function Loss: 3.09229

Mean KL Divergence: 0.05344
SB3 Clip Fraction: 0.44930
Policy Update Magnitude: 1.59682
Value Function Update Magnitude: 1.80124

Collected Steps per Second: 23,757.70352
Overall Steps per Second: 11,571.42484

Timestep Collection Time: 2.10618
Timestep Consumption Time: 2.21809
PPO Batch Consumption Time: 0.13786
Total Iteration Time: 4.32427

Cumulative Model Updates: 6,788
Cumulative Timesteps: 28,365,782

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 208.06985
Policy Entropy: 2.52860
Value Function Loss: 2.99886

Mean KL Divergence: 0.05504
SB3 Clip Fraction: 0.45092
Policy Update Magnitude: 1.57617
Value Function Update Magnitude: 1.64941

Collected Steps per Second: 22,049.01528
Overall Steps per Second: 11,139.87518

Timestep Collection Time: 2.26940
Timestep Consumption Time: 2.22239
PPO Batch Consumption Time: 0.13826
Total Iteration Time: 4.49179

Cumulative Model Updates: 6,800
Cumulative Timesteps: 28,415,820

Timesteps Collected: 50,038
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 233.34142
Policy Entropy: 2.50752
Value Function Loss: 3.13140

Mean KL Divergence: 0.05647
SB3 Clip Fraction: 0.45517
Policy Update Magnitude: 1.61035
Value Function Update Magnitude: 1.72485

Collected Steps per Second: 23,376.35658
Overall Steps per Second: 11,652.87095

Timestep Collection Time: 2.13891
Timestep Consumption Time: 2.15187
PPO Batch Consumption Time: 0.13777
Total Iteration Time: 4.29079

Cumulative Model Updates: 6,812
Cumulative Timesteps: 28,465,820

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 240.93409
Policy Entropy: 2.48142
Value Function Loss: 3.11148

Mean KL Divergence: 0.05797
SB3 Clip Fraction: 0.45945
Policy Update Magnitude: 1.60193
Value Function Update Magnitude: 1.71644

Collected Steps per Second: 23,308.60878
Overall Steps per Second: 11,413.23459

Timestep Collection Time: 2.14556
Timestep Consumption Time: 2.23620
PPO Batch Consumption Time: 0.13779
Total Iteration Time: 4.38176

Cumulative Model Updates: 6,824
Cumulative Timesteps: 28,515,830

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 210.80290
Policy Entropy: 2.46283
Value Function Loss: 3.18834

Mean KL Divergence: 0.05881
SB3 Clip Fraction: 0.45968
Policy Update Magnitude: 1.60651
Value Function Update Magnitude: 1.73150

Collected Steps per Second: 22,845.80901
Overall Steps per Second: 11,505.46511

Timestep Collection Time: 2.18955
Timestep Consumption Time: 2.15812
PPO Batch Consumption Time: 0.13781
Total Iteration Time: 4.34767

Cumulative Model Updates: 6,836
Cumulative Timesteps: 28,565,852

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 264.51046
Policy Entropy: 2.43502
Value Function Loss: 3.04560

Mean KL Divergence: 0.05865
SB3 Clip Fraction: 0.45867
Policy Update Magnitude: 1.59957
Value Function Update Magnitude: 1.79510

Collected Steps per Second: 23,860.11276
Overall Steps per Second: 11,502.59387

Timestep Collection Time: 2.09664
Timestep Consumption Time: 2.25247
PPO Batch Consumption Time: 0.13788
Total Iteration Time: 4.34911

Cumulative Model Updates: 6,848
Cumulative Timesteps: 28,615,878

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 256.67717
Policy Entropy: 2.40972
Value Function Loss: 3.30519

Mean KL Divergence: 0.05881
SB3 Clip Fraction: 0.46134
Policy Update Magnitude: 1.59418
Value Function Update Magnitude: 1.56745

Collected Steps per Second: 22,964.68575
Overall Steps per Second: 11,382.40390

Timestep Collection Time: 2.17821
Timestep Consumption Time: 2.21646
PPO Batch Consumption Time: 0.13781
Total Iteration Time: 4.39468

Cumulative Model Updates: 6,860
Cumulative Timesteps: 28,665,900

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 234.59016
Policy Entropy: 2.37554
Value Function Loss: 3.23125

Mean KL Divergence: 0.05957
SB3 Clip Fraction: 0.45864
Policy Update Magnitude: 1.57910
Value Function Update Magnitude: 1.28095

Collected Steps per Second: 23,053.38944
Overall Steps per Second: 11,546.51979

Timestep Collection Time: 2.16940
Timestep Consumption Time: 2.16195
PPO Batch Consumption Time: 0.13798
Total Iteration Time: 4.33135

Cumulative Model Updates: 6,872
Cumulative Timesteps: 28,715,912

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 255.98938
Policy Entropy: 2.33886
Value Function Loss: 3.26689

Mean KL Divergence: 0.05886
SB3 Clip Fraction: 0.45451
Policy Update Magnitude: 1.61194
Value Function Update Magnitude: 1.63346

Collected Steps per Second: 20,658.75709
Overall Steps per Second: 10,689.33813

Timestep Collection Time: 2.42086
Timestep Consumption Time: 2.25782
PPO Batch Consumption Time: 0.13825
Total Iteration Time: 4.67868

Cumulative Model Updates: 6,884
Cumulative Timesteps: 28,765,924

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 229.99865
Policy Entropy: 2.32019
Value Function Loss: 3.14355

Mean KL Divergence: 0.05830
SB3 Clip Fraction: 0.45563
Policy Update Magnitude: 1.61304
Value Function Update Magnitude: 1.44098

Collected Steps per Second: 22,525.11084
Overall Steps per Second: 11,395.86892

Timestep Collection Time: 2.22152
Timestep Consumption Time: 2.16954
PPO Batch Consumption Time: 0.13829
Total Iteration Time: 4.39106

Cumulative Model Updates: 6,896
Cumulative Timesteps: 28,815,964

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.61479
Policy Entropy: 2.29581
Value Function Loss: 3.16898

Mean KL Divergence: 0.05870
SB3 Clip Fraction: 0.45573
Policy Update Magnitude: 1.59242
Value Function Update Magnitude: 1.41851

Collected Steps per Second: 24,150.63787
Overall Steps per Second: 11,633.13534

Timestep Collection Time: 2.07100
Timestep Consumption Time: 2.22844
PPO Batch Consumption Time: 0.13845
Total Iteration Time: 4.29944

Cumulative Model Updates: 6,908
Cumulative Timesteps: 28,865,980

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.04369
Policy Entropy: 2.28271
Value Function Loss: 3.05503

Mean KL Divergence: 0.05969
SB3 Clip Fraction: 0.45749
Policy Update Magnitude: 1.58509
Value Function Update Magnitude: 1.37641

Collected Steps per Second: 23,852.67362
Overall Steps per Second: 11,460.16821

Timestep Collection Time: 2.09712
Timestep Consumption Time: 2.26773
PPO Batch Consumption Time: 0.14014
Total Iteration Time: 4.36486

Cumulative Model Updates: 6,920
Cumulative Timesteps: 28,916,002

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 202.47954
Policy Entropy: 2.26185
Value Function Loss: 3.00676

Mean KL Divergence: 0.05910
SB3 Clip Fraction: 0.45361
Policy Update Magnitude: 1.54790
Value Function Update Magnitude: 1.17677

Collected Steps per Second: 25,331.67144
Overall Steps per Second: 11,990.97296

Timestep Collection Time: 1.97397
Timestep Consumption Time: 2.19617
PPO Batch Consumption Time: 0.13802
Total Iteration Time: 4.17014

Cumulative Model Updates: 6,932
Cumulative Timesteps: 28,966,006

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 278.04931
Policy Entropy: 2.24692
Value Function Loss: 3.12728

Mean KL Divergence: 0.06039
SB3 Clip Fraction: 0.45384
Policy Update Magnitude: 1.58535
Value Function Update Magnitude: 1.15133

Collected Steps per Second: 23,737.99381
Overall Steps per Second: 11,503.25290

Timestep Collection Time: 2.10751
Timestep Consumption Time: 2.24152
PPO Batch Consumption Time: 0.13834
Total Iteration Time: 4.34903

Cumulative Model Updates: 6,944
Cumulative Timesteps: 29,016,034

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 247.47638
Policy Entropy: 2.22061
Value Function Loss: 3.20775

Mean KL Divergence: 0.06164
SB3 Clip Fraction: 0.45830
Policy Update Magnitude: 1.57789
Value Function Update Magnitude: 1.11985

Collected Steps per Second: 25,116.73573
Overall Steps per Second: 11,951.08042

Timestep Collection Time: 1.99142
Timestep Consumption Time: 2.19381
PPO Batch Consumption Time: 0.13741
Total Iteration Time: 4.18523

Cumulative Model Updates: 6,956
Cumulative Timesteps: 29,066,052

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 221.53807
Policy Entropy: 2.20019
Value Function Loss: 3.23627

Mean KL Divergence: 0.06131
SB3 Clip Fraction: 0.45241
Policy Update Magnitude: 1.59388
Value Function Update Magnitude: 1.25183

Collected Steps per Second: 25,133.31058
Overall Steps per Second: 11,778.03706

Timestep Collection Time: 1.98979
Timestep Consumption Time: 2.25625
PPO Batch Consumption Time: 0.13840
Total Iteration Time: 4.24604

Cumulative Model Updates: 6,968
Cumulative Timesteps: 29,116,062

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 274.77356
Policy Entropy: 2.17932
Value Function Loss: 3.07009

Mean KL Divergence: 0.05904
SB3 Clip Fraction: 0.44799
Policy Update Magnitude: 1.55455
Value Function Update Magnitude: 1.17433

Collected Steps per Second: 24,623.88753
Overall Steps per Second: 11,772.78903

Timestep Collection Time: 2.03282
Timestep Consumption Time: 2.21902
PPO Batch Consumption Time: 0.13789
Total Iteration Time: 4.25184

Cumulative Model Updates: 6,980
Cumulative Timesteps: 29,166,118

Timesteps Collected: 50,056
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 219.34178
Policy Entropy: 2.16140
Value Function Loss: 3.01468

Mean KL Divergence: 0.05945
SB3 Clip Fraction: 0.45013
Policy Update Magnitude: 1.55075
Value Function Update Magnitude: 1.11827

Collected Steps per Second: 25,605.68484
Overall Steps per Second: 12,132.59389

Timestep Collection Time: 1.95394
Timestep Consumption Time: 2.16983
PPO Batch Consumption Time: 0.13837
Total Iteration Time: 4.12377

Cumulative Model Updates: 6,992
Cumulative Timesteps: 29,216,150

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 222.14094
Policy Entropy: 2.14061
Value Function Loss: 2.93810

Mean KL Divergence: 0.06096
SB3 Clip Fraction: 0.45109
Policy Update Magnitude: 1.52535
Value Function Update Magnitude: 1.11180

Collected Steps per Second: 25,411.48846
Overall Steps per Second: 11,895.51915

Timestep Collection Time: 1.96793
Timestep Consumption Time: 2.23601
PPO Batch Consumption Time: 0.13910
Total Iteration Time: 4.20394

Cumulative Model Updates: 7,004
Cumulative Timesteps: 29,266,158

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 285.42440
Policy Entropy: 2.11140
Value Function Loss: 2.95934

Mean KL Divergence: 0.06100
SB3 Clip Fraction: 0.45031
Policy Update Magnitude: 1.54067
Value Function Update Magnitude: 1.12982

Collected Steps per Second: 25,247.86450
Overall Steps per Second: 11,999.58016

Timestep Collection Time: 1.98195
Timestep Consumption Time: 2.18820
PPO Batch Consumption Time: 0.13901
Total Iteration Time: 4.17015

Cumulative Model Updates: 7,016
Cumulative Timesteps: 29,316,198

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 219.57718
Policy Entropy: 2.08697
Value Function Loss: 2.91472

Mean KL Divergence: 0.06205
SB3 Clip Fraction: 0.44977
Policy Update Magnitude: 1.51655
Value Function Update Magnitude: 1.11860

Collected Steps per Second: 25,306.74672
Overall Steps per Second: 11,849.38637

Timestep Collection Time: 1.97781
Timestep Consumption Time: 2.24620
PPO Batch Consumption Time: 0.13792
Total Iteration Time: 4.22402

Cumulative Model Updates: 7,028
Cumulative Timesteps: 29,366,250

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.91575
Policy Entropy: 2.05272
Value Function Loss: 3.03334

Mean KL Divergence: 0.06324
SB3 Clip Fraction: 0.44943
Policy Update Magnitude: 1.51844
Value Function Update Magnitude: 1.21684

Collected Steps per Second: 22,348.20149
Overall Steps per Second: 11,228.14263

Timestep Collection Time: 2.23857
Timestep Consumption Time: 2.21702
PPO Batch Consumption Time: 0.13833
Total Iteration Time: 4.45559

Cumulative Model Updates: 7,040
Cumulative Timesteps: 29,416,278

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 251.71554
Policy Entropy: 2.04142
Value Function Loss: 3.07927

Mean KL Divergence: 0.06373
SB3 Clip Fraction: 0.45152
Policy Update Magnitude: 1.52128
Value Function Update Magnitude: 1.18149

Collected Steps per Second: 25,143.42491
Overall Steps per Second: 12,042.26968

Timestep Collection Time: 1.98994
Timestep Consumption Time: 2.16492
PPO Batch Consumption Time: 0.13813
Total Iteration Time: 4.15486

Cumulative Model Updates: 7,052
Cumulative Timesteps: 29,466,312

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.43275
Policy Entropy: 2.01773
Value Function Loss: 3.15607

Mean KL Divergence: 0.06588
SB3 Clip Fraction: 0.45396
Policy Update Magnitude: 1.51392
Value Function Update Magnitude: 1.15373

Collected Steps per Second: 23,636.10451
Overall Steps per Second: 11,485.57444

Timestep Collection Time: 2.11693
Timestep Consumption Time: 2.23949
PPO Batch Consumption Time: 0.13835
Total Iteration Time: 4.35642

Cumulative Model Updates: 7,064
Cumulative Timesteps: 29,516,348

Timesteps Collected: 50,036
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 260.17787
Policy Entropy: 1.99668
Value Function Loss: 3.08799

Mean KL Divergence: 0.06579
SB3 Clip Fraction: 0.44996
Policy Update Magnitude: 1.50768
Value Function Update Magnitude: 1.16844

Collected Steps per Second: 25,211.30425
Overall Steps per Second: 11,954.99562

Timestep Collection Time: 1.98340
Timestep Consumption Time: 2.19929
PPO Batch Consumption Time: 0.13908
Total Iteration Time: 4.18269

Cumulative Model Updates: 7,076
Cumulative Timesteps: 29,566,352

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 297.85337
Policy Entropy: 1.97202
Value Function Loss: 3.07024

Mean KL Divergence: 0.06662
SB3 Clip Fraction: 0.45377
Policy Update Magnitude: 1.50227
Value Function Update Magnitude: 1.15516

Collected Steps per Second: 24,046.69489
Overall Steps per Second: 11,533.03254

Timestep Collection Time: 2.07937
Timestep Consumption Time: 2.25618
PPO Batch Consumption Time: 0.13808
Total Iteration Time: 4.33555

Cumulative Model Updates: 7,088
Cumulative Timesteps: 29,616,354

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 241.88345
Policy Entropy: 1.96107
Value Function Loss: 2.93492

Mean KL Divergence: 0.06586
SB3 Clip Fraction: 0.44698
Policy Update Magnitude: 1.55540
Value Function Update Magnitude: 1.29287

Collected Steps per Second: 24,406.45153
Overall Steps per Second: 11,731.40193

Timestep Collection Time: 2.04962
Timestep Consumption Time: 2.21449
PPO Batch Consumption Time: 0.13750
Total Iteration Time: 4.26411

Cumulative Model Updates: 7,100
Cumulative Timesteps: 29,666,378

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 218.70343
Policy Entropy: 1.93997
Value Function Loss: 2.89551

Mean KL Divergence: 0.06672
SB3 Clip Fraction: 0.44754
Policy Update Magnitude: 1.50782
Value Function Update Magnitude: 1.34201

Collected Steps per Second: 25,185.58936
Overall Steps per Second: 12,040.25791

Timestep Collection Time: 1.98558
Timestep Consumption Time: 2.16782
PPO Batch Consumption Time: 0.13811
Total Iteration Time: 4.15340

Cumulative Model Updates: 7,112
Cumulative Timesteps: 29,716,386

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.29966
Policy Entropy: 1.92326
Value Function Loss: 2.85611

Mean KL Divergence: 0.06671
SB3 Clip Fraction: 0.44953
Policy Update Magnitude: 1.49675
Value Function Update Magnitude: 1.25126

Collected Steps per Second: 24,864.77135
Overall Steps per Second: 11,810.12378

Timestep Collection Time: 2.01265
Timestep Consumption Time: 2.22473
PPO Batch Consumption Time: 0.13758
Total Iteration Time: 4.23738

Cumulative Model Updates: 7,124
Cumulative Timesteps: 29,766,430

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 245.75978
Policy Entropy: 1.89341
Value Function Loss: 2.88537

Mean KL Divergence: 0.06738
SB3 Clip Fraction: 0.44939
Policy Update Magnitude: 1.48654
Value Function Update Magnitude: 1.13766

Collected Steps per Second: 24,051.48980
Overall Steps per Second: 11,777.28809

Timestep Collection Time: 2.08054
Timestep Consumption Time: 2.16832
PPO Batch Consumption Time: 0.13806
Total Iteration Time: 4.24886

Cumulative Model Updates: 7,136
Cumulative Timesteps: 29,816,470

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.84134
Policy Entropy: 1.86582
Value Function Loss: 3.09127

Mean KL Divergence: 0.06810
SB3 Clip Fraction: 0.44610
Policy Update Magnitude: 1.50760
Value Function Update Magnitude: 1.10893

Collected Steps per Second: 24,570.88063
Overall Steps per Second: 11,592.36398

Timestep Collection Time: 2.03493
Timestep Consumption Time: 2.27825
PPO Batch Consumption Time: 0.14005
Total Iteration Time: 4.31318

Cumulative Model Updates: 7,148
Cumulative Timesteps: 29,866,470

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 254.44088
Policy Entropy: 1.83486
Value Function Loss: 3.04539

Mean KL Divergence: 0.06975
SB3 Clip Fraction: 0.44690
Policy Update Magnitude: 1.47651
Value Function Update Magnitude: 1.08968

Collected Steps per Second: 25,760.98790
Overall Steps per Second: 11,972.84700

Timestep Collection Time: 1.94270
Timestep Consumption Time: 2.23725
PPO Batch Consumption Time: 0.13971
Total Iteration Time: 4.17996

Cumulative Model Updates: 7,160
Cumulative Timesteps: 29,916,516

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 229.71965
Policy Entropy: 1.81992
Value Function Loss: 3.10277

Mean KL Divergence: 0.06956
SB3 Clip Fraction: 0.44153
Policy Update Magnitude: 1.49090
Value Function Update Magnitude: 1.08002

Collected Steps per Second: 24,074.62962
Overall Steps per Second: 11,801.84028

Timestep Collection Time: 2.07737
Timestep Consumption Time: 2.16027
PPO Batch Consumption Time: 0.13742
Total Iteration Time: 4.23764

Cumulative Model Updates: 7,172
Cumulative Timesteps: 29,966,528

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 245.22719
Policy Entropy: 1.80253
Value Function Loss: 3.12008

Mean KL Divergence: 0.07058
SB3 Clip Fraction: 0.44345
Policy Update Magnitude: 1.51576
Value Function Update Magnitude: 1.09892

Collected Steps per Second: 25,740.11577
Overall Steps per Second: 12,010.72565

Timestep Collection Time: 1.94420
Timestep Consumption Time: 2.22241
PPO Batch Consumption Time: 0.13826
Total Iteration Time: 4.16661

Cumulative Model Updates: 7,184
Cumulative Timesteps: 30,016,572

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 244.16308
Policy Entropy: 1.78232
Value Function Loss: 3.19951

Mean KL Divergence: 0.07353
SB3 Clip Fraction: 0.44807
Policy Update Magnitude: 1.50360
Value Function Update Magnitude: 1.15303

Collected Steps per Second: 24,442.59531
Overall Steps per Second: 11,885.09162

Timestep Collection Time: 2.04733
Timestep Consumption Time: 2.16316
PPO Batch Consumption Time: 0.13777
Total Iteration Time: 4.21048

Cumulative Model Updates: 7,196
Cumulative Timesteps: 30,066,614

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 228.31745
Policy Entropy: 1.76634
Value Function Loss: 3.07860

Mean KL Divergence: 0.07431
SB3 Clip Fraction: 0.44726
Policy Update Magnitude: 1.47524
Value Function Update Magnitude: 1.14612

Collected Steps per Second: 23,931.68152
Overall Steps per Second: 11,468.20446

Timestep Collection Time: 2.08953
Timestep Consumption Time: 2.27087
PPO Batch Consumption Time: 0.14198
Total Iteration Time: 4.36040

Cumulative Model Updates: 7,208
Cumulative Timesteps: 30,116,620

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 254.90150
Policy Entropy: 1.73366
Value Function Loss: 2.93886

Mean KL Divergence: 0.07188
SB3 Clip Fraction: 0.44457
Policy Update Magnitude: 1.46961
Value Function Update Magnitude: 1.12508

Collected Steps per Second: 23,295.78116
Overall Steps per Second: 11,413.03290

Timestep Collection Time: 2.14734
Timestep Consumption Time: 2.23572
PPO Batch Consumption Time: 0.13909
Total Iteration Time: 4.38306

Cumulative Model Updates: 7,220
Cumulative Timesteps: 30,166,644

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 277.55723
Policy Entropy: 1.70448
Value Function Loss: 2.93588

Mean KL Divergence: 0.07268
SB3 Clip Fraction: 0.44130
Policy Update Magnitude: 1.48762
Value Function Update Magnitude: 1.11871

Collected Steps per Second: 23,114.50552
Overall Steps per Second: 11,339.13581

Timestep Collection Time: 2.16539
Timestep Consumption Time: 2.24870
PPO Batch Consumption Time: 0.13878
Total Iteration Time: 4.41409

Cumulative Model Updates: 7,232
Cumulative Timesteps: 30,216,696

Timesteps Collected: 50,052
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 250.71677
Policy Entropy: 1.67243
Value Function Loss: 3.11569

Mean KL Divergence: 0.07525
SB3 Clip Fraction: 0.44381
Policy Update Magnitude: 1.49617
Value Function Update Magnitude: 1.12533

Collected Steps per Second: 23,633.04086
Overall Steps per Second: 11,507.41899

Timestep Collection Time: 2.11712
Timestep Consumption Time: 2.23086
PPO Batch Consumption Time: 0.13775
Total Iteration Time: 4.34798

Cumulative Model Updates: 7,244
Cumulative Timesteps: 30,266,730

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.03933
Policy Entropy: 1.64900
Value Function Loss: 3.18122

Mean KL Divergence: 0.07867
SB3 Clip Fraction: 0.44663
Policy Update Magnitude: 1.48049
Value Function Update Magnitude: 1.14676

Collected Steps per Second: 23,065.52865
Overall Steps per Second: 11,530.66035

Timestep Collection Time: 2.16860
Timestep Consumption Time: 2.16940
PPO Batch Consumption Time: 0.13843
Total Iteration Time: 4.33800

Cumulative Model Updates: 7,256
Cumulative Timesteps: 30,316,750

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 319.16576
Policy Entropy: 1.63044
Value Function Loss: 3.23285

Mean KL Divergence: 0.07960
SB3 Clip Fraction: 0.44517
Policy Update Magnitude: 1.50152
Value Function Update Magnitude: 1.21438

Collected Steps per Second: 23,697.26484
Overall Steps per Second: 11,548.76125

Timestep Collection Time: 2.11180
Timestep Consumption Time: 2.22147
PPO Batch Consumption Time: 0.13768
Total Iteration Time: 4.33328

Cumulative Model Updates: 7,268
Cumulative Timesteps: 30,366,794

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 238.57850
Policy Entropy: 1.60892
Value Function Loss: 3.00405

Mean KL Divergence: 0.07808
SB3 Clip Fraction: 0.43893
Policy Update Magnitude: 1.48275
Value Function Update Magnitude: 1.26868

Collected Steps per Second: 24,846.97476
Overall Steps per Second: 11,698.57678

Timestep Collection Time: 2.01248
Timestep Consumption Time: 2.26189
PPO Batch Consumption Time: 0.14231
Total Iteration Time: 4.27437

Cumulative Model Updates: 7,280
Cumulative Timesteps: 30,416,798

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 253.95987
Policy Entropy: 1.59318
Value Function Loss: 2.98075

Mean KL Divergence: 0.07719
SB3 Clip Fraction: 0.44022
Policy Update Magnitude: 1.47644
Value Function Update Magnitude: 1.17515

Collected Steps per Second: 22,785.67552
Overall Steps per Second: 11,267.20141

Timestep Collection Time: 2.19436
Timestep Consumption Time: 2.24330
PPO Batch Consumption Time: 0.13745
Total Iteration Time: 4.43766

Cumulative Model Updates: 7,292
Cumulative Timesteps: 30,466,798

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 233.15067
Policy Entropy: 1.57273
Value Function Loss: 3.01219

Mean KL Divergence: 0.07711
SB3 Clip Fraction: 0.43814
Policy Update Magnitude: 1.47796
Value Function Update Magnitude: 1.16815

Collected Steps per Second: 24,205.22900
Overall Steps per Second: 11,655.14277

Timestep Collection Time: 2.06732
Timestep Consumption Time: 2.22606
PPO Batch Consumption Time: 0.13799
Total Iteration Time: 4.29338

Cumulative Model Updates: 7,304
Cumulative Timesteps: 30,516,838

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 252.14882
Policy Entropy: 1.55071
Value Function Loss: 3.15999

Mean KL Divergence: 0.07966
SB3 Clip Fraction: 0.43850
Policy Update Magnitude: 1.45940
Value Function Update Magnitude: 1.16198

Collected Steps per Second: 21,507.56864
Overall Steps per Second: 11,066.18910

Timestep Collection Time: 2.32486
Timestep Consumption Time: 2.19359
PPO Batch Consumption Time: 0.13788
Total Iteration Time: 4.51845

Cumulative Model Updates: 7,316
Cumulative Timesteps: 30,566,840

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 266.33193
Policy Entropy: 1.52628
Value Function Loss: 3.05526

Mean KL Divergence: 0.07925
SB3 Clip Fraction: 0.43642
Policy Update Magnitude: 1.45225
Value Function Update Magnitude: 1.20975

Collected Steps per Second: 22,929.47370
Overall Steps per Second: 11,208.29852

Timestep Collection Time: 2.18261
Timestep Consumption Time: 2.28248
PPO Batch Consumption Time: 0.13770
Total Iteration Time: 4.46508

Cumulative Model Updates: 7,328
Cumulative Timesteps: 30,616,886

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 261.84556
Policy Entropy: 1.49785
Value Function Loss: 3.03588

Mean KL Divergence: 0.07899
SB3 Clip Fraction: 0.43119
Policy Update Magnitude: 1.45720
Value Function Update Magnitude: 1.14241

Collected Steps per Second: 21,193.36498
Overall Steps per Second: 10,932.71159

Timestep Collection Time: 2.36149
Timestep Consumption Time: 2.21633
PPO Batch Consumption Time: 0.13798
Total Iteration Time: 4.57782

Cumulative Model Updates: 7,340
Cumulative Timesteps: 30,666,934

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 246.87422
Policy Entropy: 1.46413
Value Function Loss: 3.03983

Mean KL Divergence: 0.08002
SB3 Clip Fraction: 0.43215
Policy Update Magnitude: 1.44192
Value Function Update Magnitude: 1.15692

Collected Steps per Second: 22,666.14329
Overall Steps per Second: 11,240.54494

Timestep Collection Time: 2.20620
Timestep Consumption Time: 2.24252
PPO Batch Consumption Time: 0.13802
Total Iteration Time: 4.44872

Cumulative Model Updates: 7,352
Cumulative Timesteps: 30,716,940

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 195.49952
Policy Entropy: 1.43066
Value Function Loss: 3.09020

Mean KL Divergence: 0.08011
SB3 Clip Fraction: 0.42664
Policy Update Magnitude: 1.43050
Value Function Update Magnitude: 1.12076

Collected Steps per Second: 22,567.58898
Overall Steps per Second: 11,237.42388

Timestep Collection Time: 2.21690
Timestep Consumption Time: 2.23519
PPO Batch Consumption Time: 0.13785
Total Iteration Time: 4.45209

Cumulative Model Updates: 7,364
Cumulative Timesteps: 30,766,970

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 195.92111
Policy Entropy: 1.40925
Value Function Loss: 3.05539

Mean KL Divergence: 0.08087
SB3 Clip Fraction: 0.42385
Policy Update Magnitude: 1.43554
Value Function Update Magnitude: 1.18042

Collected Steps per Second: 23,785.41190
Overall Steps per Second: 11,599.32469

Timestep Collection Time: 2.10398
Timestep Consumption Time: 2.21041
PPO Batch Consumption Time: 0.13852
Total Iteration Time: 4.31439

Cumulative Model Updates: 7,376
Cumulative Timesteps: 30,817,014

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 256.11584
Policy Entropy: 1.38751
Value Function Loss: 3.13179

Mean KL Divergence: 0.08115
SB3 Clip Fraction: 0.42312
Policy Update Magnitude: 1.42507
Value Function Update Magnitude: 1.33295

Collected Steps per Second: 24,355.00805
Overall Steps per Second: 11,582.51393

Timestep Collection Time: 2.05403
Timestep Consumption Time: 2.26506
PPO Batch Consumption Time: 0.13889
Total Iteration Time: 4.31910

Cumulative Model Updates: 7,388
Cumulative Timesteps: 30,867,040

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 261.01605
Policy Entropy: 1.37521
Value Function Loss: 3.10284

Mean KL Divergence: 0.08212
SB3 Clip Fraction: 0.42270
Policy Update Magnitude: 1.42356
Value Function Update Magnitude: 1.21271

Collected Steps per Second: 23,238.07766
Overall Steps per Second: 11,364.20523

Timestep Collection Time: 2.15207
Timestep Consumption Time: 2.24859
PPO Batch Consumption Time: 0.13824
Total Iteration Time: 4.40066

Cumulative Model Updates: 7,400
Cumulative Timesteps: 30,917,050

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 222.89732
Policy Entropy: 1.34992
Value Function Loss: 3.17649

Mean KL Divergence: 0.08337
SB3 Clip Fraction: 0.42181
Policy Update Magnitude: 1.41935
Value Function Update Magnitude: 1.29235

Collected Steps per Second: 23,123.74152
Overall Steps per Second: 11,344.47526

Timestep Collection Time: 2.16263
Timestep Consumption Time: 2.24551
PPO Batch Consumption Time: 0.13888
Total Iteration Time: 4.40814

Cumulative Model Updates: 7,412
Cumulative Timesteps: 30,967,058

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 234.81035
Policy Entropy: 1.33372
Value Function Loss: 3.05675

Mean KL Divergence: 0.08361
SB3 Clip Fraction: 0.41923
Policy Update Magnitude: 1.40003
Value Function Update Magnitude: 1.25343

Collected Steps per Second: 22,155.75780
Overall Steps per Second: 11,030.37325

Timestep Collection Time: 2.25738
Timestep Consumption Time: 2.27683
PPO Batch Consumption Time: 0.14562
Total Iteration Time: 4.53421

Cumulative Model Updates: 7,424
Cumulative Timesteps: 31,017,072

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 270.65826
Policy Entropy: 1.31075
Value Function Loss: 3.01351

Mean KL Divergence: 0.08443
SB3 Clip Fraction: 0.41817
Policy Update Magnitude: 1.38916
Value Function Update Magnitude: 1.22653

Collected Steps per Second: 23,452.59735
Overall Steps per Second: 11,479.51779

Timestep Collection Time: 2.13324
Timestep Consumption Time: 2.22496
PPO Batch Consumption Time: 0.13762
Total Iteration Time: 4.35820

Cumulative Model Updates: 7,436
Cumulative Timesteps: 31,067,102

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 244.17249
Policy Entropy: 1.28923
Value Function Loss: 2.92526

Mean KL Divergence: 0.08455
SB3 Clip Fraction: 0.41685
Policy Update Magnitude: 1.35656
Value Function Update Magnitude: 1.22348

Collected Steps per Second: 22,845.43545
Overall Steps per Second: 11,161.34059

Timestep Collection Time: 2.19133
Timestep Consumption Time: 2.29397
PPO Batch Consumption Time: 0.14462
Total Iteration Time: 4.48530

Cumulative Model Updates: 7,448
Cumulative Timesteps: 31,117,164

Timesteps Collected: 50,062
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 242.67279
Policy Entropy: 1.26392
Value Function Loss: 2.98806

Mean KL Divergence: 0.08422
SB3 Clip Fraction: 0.41143
Policy Update Magnitude: 1.37461
Value Function Update Magnitude: 1.26008

Collected Steps per Second: 24,220.38021
Overall Steps per Second: 11,530.14567

Timestep Collection Time: 2.06727
Timestep Consumption Time: 2.27526
PPO Batch Consumption Time: 0.14575
Total Iteration Time: 4.34253

Cumulative Model Updates: 7,460
Cumulative Timesteps: 31,167,234

Timesteps Collected: 50,070
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 225.51285
Policy Entropy: 1.24172
Value Function Loss: 2.99414

Mean KL Divergence: 0.08485
SB3 Clip Fraction: 0.41001
Policy Update Magnitude: 1.36302
Value Function Update Magnitude: 1.37774

Collected Steps per Second: 24,261.05069
Overall Steps per Second: 11,639.20818

Timestep Collection Time: 2.06224
Timestep Consumption Time: 2.23634
PPO Batch Consumption Time: 0.13749
Total Iteration Time: 4.29857

Cumulative Model Updates: 7,472
Cumulative Timesteps: 31,217,266

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 260.98059
Policy Entropy: 1.21642
Value Function Loss: 3.14180

Mean KL Divergence: 0.08443
SB3 Clip Fraction: 0.40327
Policy Update Magnitude: 1.40370
Value Function Update Magnitude: 1.30073

Collected Steps per Second: 23,953.63744
Overall Steps per Second: 11,622.49803

Timestep Collection Time: 2.08970
Timestep Consumption Time: 2.21712
PPO Batch Consumption Time: 0.14303
Total Iteration Time: 4.30682

Cumulative Model Updates: 7,484
Cumulative Timesteps: 31,267,322

Timesteps Collected: 50,056
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 259.16018
Policy Entropy: 1.18818
Value Function Loss: 3.13154

Mean KL Divergence: 0.08808
SB3 Clip Fraction: 0.40758
Policy Update Magnitude: 1.35772
Value Function Update Magnitude: 1.23573

Collected Steps per Second: 24,245.78914
Overall Steps per Second: 11,329.92323

Timestep Collection Time: 2.06337
Timestep Consumption Time: 2.35220
PPO Batch Consumption Time: 0.14203
Total Iteration Time: 4.41556

Cumulative Model Updates: 7,496
Cumulative Timesteps: 31,317,350

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 262.08663
Policy Entropy: 1.16198
Value Function Loss: 3.11763

Mean KL Divergence: 0.08936
SB3 Clip Fraction: 0.40860
Policy Update Magnitude: 1.34903
Value Function Update Magnitude: 1.24842

Collected Steps per Second: 21,733.19280
Overall Steps per Second: 11,110.05867

Timestep Collection Time: 2.30182
Timestep Consumption Time: 2.20094
PPO Batch Consumption Time: 0.13758
Total Iteration Time: 4.50277

Cumulative Model Updates: 7,508
Cumulative Timesteps: 31,367,376

Timesteps Collected: 50,026
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 289.48390
Policy Entropy: 1.13396
Value Function Loss: 3.14400

Mean KL Divergence: 0.08823
SB3 Clip Fraction: 0.40153
Policy Update Magnitude: 1.34404
Value Function Update Magnitude: 1.18689

Collected Steps per Second: 22,614.76278
Overall Steps per Second: 11,176.59214

Timestep Collection Time: 2.21201
Timestep Consumption Time: 2.26378
PPO Batch Consumption Time: 0.14472
Total Iteration Time: 4.47578

Cumulative Model Updates: 7,520
Cumulative Timesteps: 31,417,400

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 229.81725
Policy Entropy: 1.11422
Value Function Loss: 3.18750

Mean KL Divergence: 0.09024
SB3 Clip Fraction: 0.39772
Policy Update Magnitude: 1.36279
Value Function Update Magnitude: 1.18039

Collected Steps per Second: 21,402.76505
Overall Steps per Second: 10,744.35825

Timestep Collection Time: 2.33727
Timestep Consumption Time: 2.31857
PPO Batch Consumption Time: 0.13961
Total Iteration Time: 4.65584

Cumulative Model Updates: 7,532
Cumulative Timesteps: 31,467,424

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 249.11169
Policy Entropy: 1.09473
Value Function Loss: 3.27973

Mean KL Divergence: 0.09125
SB3 Clip Fraction: 0.39699
Policy Update Magnitude: 1.36915
Value Function Update Magnitude: 1.20048

Collected Steps per Second: 20,799.33952
Overall Steps per Second: 10,943.28165

Timestep Collection Time: 2.40479
Timestep Consumption Time: 2.16587
PPO Batch Consumption Time: 0.13791
Total Iteration Time: 4.57066

Cumulative Model Updates: 7,544
Cumulative Timesteps: 31,517,442

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.35743
Policy Entropy: 1.07564
Value Function Loss: 3.18185

Mean KL Divergence: 0.09214
SB3 Clip Fraction: 0.39850
Policy Update Magnitude: 1.35002
Value Function Update Magnitude: 1.20171

Collected Steps per Second: 23,215.07371
Overall Steps per Second: 11,394.23853

Timestep Collection Time: 2.15420
Timestep Consumption Time: 2.23486
PPO Batch Consumption Time: 0.13874
Total Iteration Time: 4.38906

Cumulative Model Updates: 7,556
Cumulative Timesteps: 31,567,452

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.43354
Policy Entropy: 1.05753
Value Function Loss: 3.15161

Mean KL Divergence: 0.09436
SB3 Clip Fraction: 0.39676
Policy Update Magnitude: 1.34675
Value Function Update Magnitude: 1.17747

Collected Steps per Second: 23,052.12515
Overall Steps per Second: 11,261.93676

Timestep Collection Time: 2.16969
Timestep Consumption Time: 2.27146
PPO Batch Consumption Time: 0.14185
Total Iteration Time: 4.44115

Cumulative Model Updates: 7,568
Cumulative Timesteps: 31,617,468

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 240.59598
Policy Entropy: 1.04047
Value Function Loss: 3.03091

Mean KL Divergence: 0.09664
SB3 Clip Fraction: 0.39296
Policy Update Magnitude: 1.31040
Value Function Update Magnitude: 1.23471

Collected Steps per Second: 24,197.35231
Overall Steps per Second: 11,585.31369

Timestep Collection Time: 2.06684
Timestep Consumption Time: 2.25001
PPO Batch Consumption Time: 0.13969
Total Iteration Time: 4.31684

Cumulative Model Updates: 7,580
Cumulative Timesteps: 31,667,480

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 245.24578
Policy Entropy: 1.02543
Value Function Loss: 2.94376

Mean KL Divergence: 0.09590
SB3 Clip Fraction: 0.38843
Policy Update Magnitude: 1.33283
Value Function Update Magnitude: 1.22435

Collected Steps per Second: 22,516.55564
Overall Steps per Second: 11,115.90893

Timestep Collection Time: 2.22210
Timestep Consumption Time: 2.27902
PPO Batch Consumption Time: 0.14105
Total Iteration Time: 4.50112

Cumulative Model Updates: 7,592
Cumulative Timesteps: 31,717,514

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 243.70486
Policy Entropy: 1.00367
Value Function Loss: 2.97965

Mean KL Divergence: 0.09410
SB3 Clip Fraction: 0.38922
Policy Update Magnitude: 1.30056
Value Function Update Magnitude: 1.21086

Collected Steps per Second: 22,971.28865
Overall Steps per Second: 11,470.44920

Timestep Collection Time: 2.17707
Timestep Consumption Time: 2.18283
PPO Batch Consumption Time: 0.13892
Total Iteration Time: 4.35990

Cumulative Model Updates: 7,604
Cumulative Timesteps: 31,767,524

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 243.63349
Policy Entropy: 0.98262
Value Function Loss: 2.97994

Mean KL Divergence: 0.09561
SB3 Clip Fraction: 0.38480
Policy Update Magnitude: 1.31241
Value Function Update Magnitude: 1.23783

Collected Steps per Second: 22,387.95552
Overall Steps per Second: 11,086.84987

Timestep Collection Time: 2.23486
Timestep Consumption Time: 2.27805
PPO Batch Consumption Time: 0.14045
Total Iteration Time: 4.51291

Cumulative Model Updates: 7,616
Cumulative Timesteps: 31,817,558

Timesteps Collected: 50,034
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 223.20722
Policy Entropy: 0.96403
Value Function Loss: 3.01773

Mean KL Divergence: 0.09707
SB3 Clip Fraction: 0.38283
Policy Update Magnitude: 1.30626
Value Function Update Magnitude: 1.23308

Collected Steps per Second: 24,114.36161
Overall Steps per Second: 11,363.39479

Timestep Collection Time: 2.07478
Timestep Consumption Time: 2.32813
PPO Batch Consumption Time: 0.14874
Total Iteration Time: 4.40291

Cumulative Model Updates: 7,628
Cumulative Timesteps: 31,867,590

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 261.74719
Policy Entropy: 0.94450
Value Function Loss: 2.88978

Mean KL Divergence: 0.09960
SB3 Clip Fraction: 0.37968
Policy Update Magnitude: 1.29725
Value Function Update Magnitude: 1.22794

Collected Steps per Second: 23,693.36052
Overall Steps per Second: 11,441.92289

Timestep Collection Time: 2.11148
Timestep Consumption Time: 2.26086
PPO Batch Consumption Time: 0.14012
Total Iteration Time: 4.37234

Cumulative Model Updates: 7,640
Cumulative Timesteps: 31,917,618

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 267.92219
Policy Entropy: 0.92908
Value Function Loss: 2.97413

Mean KL Divergence: 0.09892
SB3 Clip Fraction: 0.37876
Policy Update Magnitude: 1.30763
Value Function Update Magnitude: 1.19826

Collected Steps per Second: 21,812.73635
Overall Steps per Second: 10,981.09113

Timestep Collection Time: 2.29288
Timestep Consumption Time: 2.26168
PPO Batch Consumption Time: 0.14029
Total Iteration Time: 4.55456

Cumulative Model Updates: 7,652
Cumulative Timesteps: 31,967,632

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 238.91879
Policy Entropy: 0.91720
Value Function Loss: 2.99761

Mean KL Divergence: 0.10135
SB3 Clip Fraction: 0.37680
Policy Update Magnitude: 1.30514
Value Function Update Magnitude: 1.22330

Collected Steps per Second: 23,847.31007
Overall Steps per Second: 11,480.01360

Timestep Collection Time: 2.09701
Timestep Consumption Time: 2.25908
PPO Batch Consumption Time: 0.13972
Total Iteration Time: 4.35609

Cumulative Model Updates: 7,664
Cumulative Timesteps: 32,017,640

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 247.14631
Policy Entropy: 0.89662
Value Function Loss: 3.12337

Mean KL Divergence: 0.10323
SB3 Clip Fraction: 0.37533
Policy Update Magnitude: 1.27853
Value Function Update Magnitude: 1.24273

Collected Steps per Second: 22,615.04587
Overall Steps per Second: 11,187.15965

Timestep Collection Time: 2.21171
Timestep Consumption Time: 2.25931
PPO Batch Consumption Time: 0.13932
Total Iteration Time: 4.47102

Cumulative Model Updates: 7,676
Cumulative Timesteps: 32,067,658

Timesteps Collected: 50,018
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.89821
Policy Entropy: 0.88199
Value Function Loss: 2.99006

Mean KL Divergence: 0.10315
SB3 Clip Fraction: 0.37415
Policy Update Magnitude: 1.26491
Value Function Update Magnitude: 1.37239

Collected Steps per Second: 22,815.21899
Overall Steps per Second: 11,294.32729

Timestep Collection Time: 2.19178
Timestep Consumption Time: 2.23575
PPO Batch Consumption Time: 0.14037
Total Iteration Time: 4.42753

Cumulative Model Updates: 7,688
Cumulative Timesteps: 32,117,664

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 247.33374
Policy Entropy: 0.87246
Value Function Loss: 3.00205

Mean KL Divergence: 0.10595
SB3 Clip Fraction: 0.37424
Policy Update Magnitude: 1.26467
Value Function Update Magnitude: 1.37727

Collected Steps per Second: 23,002.30492
Overall Steps per Second: 11,309.21562

Timestep Collection Time: 2.17404
Timestep Consumption Time: 2.24784
PPO Batch Consumption Time: 0.13959
Total Iteration Time: 4.42188

Cumulative Model Updates: 7,700
Cumulative Timesteps: 32,167,672

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 228.79290
Policy Entropy: 0.85931
Value Function Loss: 2.99634

Mean KL Divergence: 0.10491
SB3 Clip Fraction: 0.37012
Policy Update Magnitude: 1.27975
Value Function Update Magnitude: 1.34189

Collected Steps per Second: 19,560.75814
Overall Steps per Second: 10,490.28531

Timestep Collection Time: 2.55829
Timestep Consumption Time: 2.21203
PPO Batch Consumption Time: 0.14122
Total Iteration Time: 4.77032

Cumulative Model Updates: 7,712
Cumulative Timesteps: 32,217,714

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 234.07019
Policy Entropy: 0.84154
Value Function Loss: 3.15296

Mean KL Divergence: 0.10649
SB3 Clip Fraction: 0.36700
Policy Update Magnitude: 1.29127
Value Function Update Magnitude: 1.37122

Collected Steps per Second: 22,185.63809
Overall Steps per Second: 11,038.92483

Timestep Collection Time: 2.25380
Timestep Consumption Time: 2.27581
PPO Batch Consumption Time: 0.14141
Total Iteration Time: 4.52961

Cumulative Model Updates: 7,724
Cumulative Timesteps: 32,267,716

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 220.62371
Policy Entropy: 0.81908
Value Function Loss: 3.22708

Mean KL Divergence: 0.11084
SB3 Clip Fraction: 0.36695
Policy Update Magnitude: 1.27002
Value Function Update Magnitude: 1.39246

Collected Steps per Second: 22,367.17961
Overall Steps per Second: 11,156.68111

Timestep Collection Time: 2.23551
Timestep Consumption Time: 2.24629
PPO Batch Consumption Time: 0.14035
Total Iteration Time: 4.48180

Cumulative Model Updates: 7,736
Cumulative Timesteps: 32,317,718

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 241.58337
Policy Entropy: 0.80391
Value Function Loss: 3.19859

Mean KL Divergence: 0.11092
SB3 Clip Fraction: 0.36329
Policy Update Magnitude: 1.26959
Value Function Update Magnitude: 1.44851

Collected Steps per Second: 24,337.35431
Overall Steps per Second: 11,573.16478

Timestep Collection Time: 2.05495
Timestep Consumption Time: 2.26643
PPO Batch Consumption Time: 0.13983
Total Iteration Time: 4.32138

Cumulative Model Updates: 7,748
Cumulative Timesteps: 32,367,730

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 254.89261
Policy Entropy: 0.78912
Value Function Loss: 3.21290

Mean KL Divergence: 0.11317
SB3 Clip Fraction: 0.35982
Policy Update Magnitude: 1.26163
Value Function Update Magnitude: 1.50223

Collected Steps per Second: 23,412.43822
Overall Steps per Second: 11,418.85219

Timestep Collection Time: 2.13681
Timestep Consumption Time: 2.24436
PPO Batch Consumption Time: 0.13954
Total Iteration Time: 4.38118

Cumulative Model Updates: 7,760
Cumulative Timesteps: 32,417,758

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 259.03004
Policy Entropy: 0.78236
Value Function Loss: 3.07767

Mean KL Divergence: 0.11422
SB3 Clip Fraction: 0.35886
Policy Update Magnitude: 1.28051
Value Function Update Magnitude: 1.41103

Collected Steps per Second: 24,895.91563
Overall Steps per Second: 11,801.08907

Timestep Collection Time: 2.00868
Timestep Consumption Time: 2.22889
PPO Batch Consumption Time: 0.13897
Total Iteration Time: 4.23757

Cumulative Model Updates: 7,772
Cumulative Timesteps: 32,467,766

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 232.05967
Policy Entropy: 0.77189
Value Function Loss: 3.10983

Mean KL Divergence: 0.11670
SB3 Clip Fraction: 0.36182
Policy Update Magnitude: 1.28561
Value Function Update Magnitude: 1.38640

Collected Steps per Second: 23,524.10706
Overall Steps per Second: 11,307.47705

Timestep Collection Time: 2.12641
Timestep Consumption Time: 2.29738
PPO Batch Consumption Time: 0.14339
Total Iteration Time: 4.42380

Cumulative Model Updates: 7,784
Cumulative Timesteps: 32,517,788

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 213.45293
Policy Entropy: 0.76583
Value Function Loss: 3.10045

Mean KL Divergence: 0.11886
SB3 Clip Fraction: 0.36145
Policy Update Magnitude: 1.28385
Value Function Update Magnitude: 1.37724

Collected Steps per Second: 22,138.36411
Overall Steps per Second: 11,125.84620

Timestep Collection Time: 2.25897
Timestep Consumption Time: 2.23596
PPO Batch Consumption Time: 0.13899
Total Iteration Time: 4.49494

Cumulative Model Updates: 7,796
Cumulative Timesteps: 32,567,798

Timesteps Collected: 50,010
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 235.62220
Policy Entropy: 0.76008
Value Function Loss: 3.11647

Mean KL Divergence: 0.11695
SB3 Clip Fraction: 0.35974
Policy Update Magnitude: 1.28546
Value Function Update Magnitude: 1.39887

Collected Steps per Second: 22,501.17345
Overall Steps per Second: 11,332.66743

Timestep Collection Time: 2.22228
Timestep Consumption Time: 2.19009
PPO Batch Consumption Time: 0.13880
Total Iteration Time: 4.41238

Cumulative Model Updates: 7,808
Cumulative Timesteps: 32,617,802

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 259.93752
Policy Entropy: 0.74697
Value Function Loss: 3.06573

Mean KL Divergence: 0.11947
SB3 Clip Fraction: 0.35815
Policy Update Magnitude: 1.25078
Value Function Update Magnitude: 1.39374

Collected Steps per Second: 23,969.56273
Overall Steps per Second: 11,483.98206

Timestep Collection Time: 2.08723
Timestep Consumption Time: 2.26927
PPO Batch Consumption Time: 0.14043
Total Iteration Time: 4.35650

Cumulative Model Updates: 7,820
Cumulative Timesteps: 32,667,832

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 248.77642
Policy Entropy: 0.73539
Value Function Loss: 3.05667

Mean KL Divergence: 0.11844
SB3 Clip Fraction: 0.35388
Policy Update Magnitude: 1.26330
Value Function Update Magnitude: 1.36770

Collected Steps per Second: 25,108.75334
Overall Steps per Second: 11,830.43671

Timestep Collection Time: 1.99325
Timestep Consumption Time: 2.23719
PPO Batch Consumption Time: 0.13968
Total Iteration Time: 4.23044

Cumulative Model Updates: 7,832
Cumulative Timesteps: 32,717,880

Timesteps Collected: 50,048
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 218.98419
Policy Entropy: 0.72182
Value Function Loss: 2.97861

Mean KL Divergence: 0.11675
SB3 Clip Fraction: 0.35065
Policy Update Magnitude: 1.26183
Value Function Update Magnitude: 1.32306

Collected Steps per Second: 24,680.03255
Overall Steps per Second: 11,471.67359

Timestep Collection Time: 2.02755
Timestep Consumption Time: 2.33450
PPO Batch Consumption Time: 0.14570
Total Iteration Time: 4.36205

Cumulative Model Updates: 7,844
Cumulative Timesteps: 32,767,920

Timesteps Collected: 50,040
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 250.81789
Policy Entropy: 0.71763
Value Function Loss: 2.95618

Mean KL Divergence: 0.11694
SB3 Clip Fraction: 0.34967
Policy Update Magnitude: 1.25207
Value Function Update Magnitude: 1.25608

Collected Steps per Second: 21,982.77472
Overall Steps per Second: 10,900.01204

Timestep Collection Time: 2.27515
Timestep Consumption Time: 2.31329
PPO Batch Consumption Time: 0.14251
Total Iteration Time: 4.58844

Cumulative Model Updates: 7,856
Cumulative Timesteps: 32,817,934

Timesteps Collected: 50,014
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.64546
Policy Entropy: 0.70964
Value Function Loss: 2.83075

Mean KL Divergence: 0.11720
SB3 Clip Fraction: 0.34943
Policy Update Magnitude: 1.23411
Value Function Update Magnitude: 1.27927

Collected Steps per Second: 21,701.99699
Overall Steps per Second: 10,994.33731

Timestep Collection Time: 2.30467
Timestep Consumption Time: 2.24458
PPO Batch Consumption Time: 0.14193
Total Iteration Time: 4.54925

Cumulative Model Updates: 7,868
Cumulative Timesteps: 32,867,950

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 257.96656
Policy Entropy: 0.69828
Value Function Loss: 2.91578

Mean KL Divergence: 0.11846
SB3 Clip Fraction: 0.35034
Policy Update Magnitude: 1.18561
Value Function Update Magnitude: 1.26592

Collected Steps per Second: 23,508.49807
Overall Steps per Second: 11,417.39867

Timestep Collection Time: 2.12706
Timestep Consumption Time: 2.25257
PPO Batch Consumption Time: 0.13924
Total Iteration Time: 4.37963

Cumulative Model Updates: 7,880
Cumulative Timesteps: 32,917,954

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 239.54206
Policy Entropy: 0.68810
Value Function Loss: 2.77221

Mean KL Divergence: 0.11609
SB3 Clip Fraction: 0.34288
Policy Update Magnitude: 1.23443
Value Function Update Magnitude: 1.33708

Collected Steps per Second: 20,681.21950
Overall Steps per Second: 10,604.79013

Timestep Collection Time: 2.41920
Timestep Consumption Time: 2.29867
PPO Batch Consumption Time: 0.14499
Total Iteration Time: 4.71787

Cumulative Model Updates: 7,892
Cumulative Timesteps: 32,967,986

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 261.92879
Policy Entropy: 0.67858
Value Function Loss: 2.88640

Mean KL Divergence: 0.12086
SB3 Clip Fraction: 0.34602
Policy Update Magnitude: 1.21904
Value Function Update Magnitude: 1.31327

Collected Steps per Second: 23,117.50589
Overall Steps per Second: 11,315.82996

Timestep Collection Time: 2.16477
Timestep Consumption Time: 2.25771
PPO Batch Consumption Time: 0.13926
Total Iteration Time: 4.42248

Cumulative Model Updates: 7,904
Cumulative Timesteps: 33,018,030

Timesteps Collected: 50,044
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 211.66996
Policy Entropy: 0.67263
Value Function Loss: 2.92035

Mean KL Divergence: 0.12475
SB3 Clip Fraction: 0.34426
Policy Update Magnitude: 1.23413
Value Function Update Magnitude: 1.35187

Collected Steps per Second: 22,533.40873
Overall Steps per Second: 11,241.94381

Timestep Collection Time: 2.22035
Timestep Consumption Time: 2.23013
PPO Batch Consumption Time: 0.13861
Total Iteration Time: 4.45048

Cumulative Model Updates: 7,916
Cumulative Timesteps: 33,068,062

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 258.75999
Policy Entropy: 0.66428
Value Function Loss: 2.98866

Mean KL Divergence: 0.12833
SB3 Clip Fraction: 0.34489
Policy Update Magnitude: 1.22516
Value Function Update Magnitude: 1.42687

Collected Steps per Second: 25,443.60544
Overall Steps per Second: 11,885.19350

Timestep Collection Time: 1.96513
Timestep Consumption Time: 2.24178
PPO Batch Consumption Time: 0.13802
Total Iteration Time: 4.20692

Cumulative Model Updates: 7,928
Cumulative Timesteps: 33,118,062

Timesteps Collected: 50,000
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 214.13305
Policy Entropy: 0.65787
Value Function Loss: 3.03018

Mean KL Divergence: 0.12435
SB3 Clip Fraction: 0.34119
Policy Update Magnitude: 1.23220
Value Function Update Magnitude: 1.39011

Collected Steps per Second: 24,413.80834
Overall Steps per Second: 11,597.99978

Timestep Collection Time: 2.04991
Timestep Consumption Time: 2.26515
PPO Batch Consumption Time: 0.13767
Total Iteration Time: 4.31505

Cumulative Model Updates: 7,940
Cumulative Timesteps: 33,168,108

Timesteps Collected: 50,046
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 238.85280
Policy Entropy: 0.65233
Value Function Loss: 3.00486

Mean KL Divergence: 0.12589
SB3 Clip Fraction: 0.33848
Policy Update Magnitude: 1.23067
Value Function Update Magnitude: 1.32816

Collected Steps per Second: 22,556.37877
Overall Steps per Second: 11,275.91784

Timestep Collection Time: 2.21791
Timestep Consumption Time: 2.21880
PPO Batch Consumption Time: 0.14314
Total Iteration Time: 4.43671

Cumulative Model Updates: 7,952
Cumulative Timesteps: 33,218,136

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 225.68701
Policy Entropy: 0.64448
Value Function Loss: 3.08201

Mean KL Divergence: 0.12918
SB3 Clip Fraction: 0.33978
Policy Update Magnitude: 1.23646
Value Function Update Magnitude: 1.24988

Collected Steps per Second: 21,442.00896
Overall Steps per Second: 10,658.84932

Timestep Collection Time: 2.33318
Timestep Consumption Time: 2.36039
PPO Batch Consumption Time: 0.14546
Total Iteration Time: 4.69356

Cumulative Model Updates: 7,964
Cumulative Timesteps: 33,268,164

Timesteps Collected: 50,028
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 258.73153
Policy Entropy: 0.63929
Value Function Loss: 2.94600

Mean KL Divergence: 0.12921
SB3 Clip Fraction: 0.33698
Policy Update Magnitude: 1.21972
Value Function Update Magnitude: 1.22791

Collected Steps per Second: 23,586.88003
Overall Steps per Second: 11,564.45513

Timestep Collection Time: 2.12033
Timestep Consumption Time: 2.20430
PPO Batch Consumption Time: 0.14063
Total Iteration Time: 4.32463

Cumulative Model Updates: 7,976
Cumulative Timesteps: 33,318,176

Timesteps Collected: 50,012
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 253.86771
Policy Entropy: 0.62950
Value Function Loss: 2.90276

Mean KL Divergence: 0.13120
SB3 Clip Fraction: 0.33460
Policy Update Magnitude: 1.20335
Value Function Update Magnitude: 1.22802

Collected Steps per Second: 24,377.70989
Overall Steps per Second: 11,617.19467

Timestep Collection Time: 2.05237
Timestep Consumption Time: 2.25435
PPO Batch Consumption Time: 0.13970
Total Iteration Time: 4.30672

Cumulative Model Updates: 7,988
Cumulative Timesteps: 33,368,208

Timesteps Collected: 50,032
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 244.69580
Policy Entropy: 0.62079
Value Function Loss: 2.98812

Mean KL Divergence: 0.13226
SB3 Clip Fraction: 0.33183
Policy Update Magnitude: 1.21477
Value Function Update Magnitude: 1.25341

Collected Steps per Second: 25,953.68055
Overall Steps per Second: 11,889.59305

Timestep Collection Time: 1.92813
Timestep Consumption Time: 2.28076
PPO Batch Consumption Time: 0.14270
Total Iteration Time: 4.20889

Cumulative Model Updates: 8,000
Cumulative Timesteps: 33,418,250

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 237.12997
Policy Entropy: 0.61029
Value Function Loss: 3.13176

Mean KL Divergence: 0.12955
SB3 Clip Fraction: 0.33148
Policy Update Magnitude: 1.21102
Value Function Update Magnitude: 1.30216

Collected Steps per Second: 22,348.88937
Overall Steps per Second: 11,187.37668

Timestep Collection Time: 2.23761
Timestep Consumption Time: 2.23243
PPO Batch Consumption Time: 0.13950
Total Iteration Time: 4.47004

Cumulative Model Updates: 8,012
Cumulative Timesteps: 33,468,258

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 221.36367
Policy Entropy: 0.60317
Value Function Loss: 3.23458

Mean KL Divergence: 0.13238
SB3 Clip Fraction: 0.32778
Policy Update Magnitude: 1.20792
Value Function Update Magnitude: 1.39023

Collected Steps per Second: 20,368.86679
Overall Steps per Second: 10,321.37456

Timestep Collection Time: 2.45512
Timestep Consumption Time: 2.38997
PPO Batch Consumption Time: 0.14290
Total Iteration Time: 4.84509

Cumulative Model Updates: 8,024
Cumulative Timesteps: 33,518,266

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.32127
Policy Entropy: 0.59635
Value Function Loss: 3.08658

Mean KL Divergence: 0.14000
SB3 Clip Fraction: 0.32990
Policy Update Magnitude: 1.19866
Value Function Update Magnitude: 1.41135

Collected Steps per Second: 20,727.94109
Overall Steps per Second: 10,728.04774

Timestep Collection Time: 2.41317
Timestep Consumption Time: 2.24938
PPO Batch Consumption Time: 0.14211
Total Iteration Time: 4.66254

Cumulative Model Updates: 8,036
Cumulative Timesteps: 33,568,286

Timesteps Collected: 50,020
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 289.54990
Policy Entropy: 0.59042
Value Function Loss: 2.96051

Mean KL Divergence: 0.13848
SB3 Clip Fraction: 0.32490
Policy Update Magnitude: 1.19048
Value Function Update Magnitude: 1.39556

Collected Steps per Second: 19,212.07959
Overall Steps per Second: 10,310.81252

Timestep Collection Time: 2.60274
Timestep Consumption Time: 2.24693
PPO Batch Consumption Time: 0.13750
Total Iteration Time: 4.84967

Cumulative Model Updates: 8,048
Cumulative Timesteps: 33,618,290

Timesteps Collected: 50,004
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 227.22383
Policy Entropy: 0.58305
Value Function Loss: 2.90948

Mean KL Divergence: 0.12858
SB3 Clip Fraction: 0.32327
Policy Update Magnitude: 1.21062
Value Function Update Magnitude: 1.32892

Collected Steps per Second: 23,898.85418
Overall Steps per Second: 11,543.69315

Timestep Collection Time: 2.09282
Timestep Consumption Time: 2.23994
PPO Batch Consumption Time: 0.13918
Total Iteration Time: 4.33276

Cumulative Model Updates: 8,060
Cumulative Timesteps: 33,668,306

Timesteps Collected: 50,016
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 216.84020
Policy Entropy: 0.57547
Value Function Loss: 2.99730

Mean KL Divergence: 0.12879
SB3 Clip Fraction: 0.32154
Policy Update Magnitude: 1.19522
Value Function Update Magnitude: 1.31115

Collected Steps per Second: 24,257.18496
Overall Steps per Second: 11,602.37145

Timestep Collection Time: 2.06133
Timestep Consumption Time: 2.24831
PPO Batch Consumption Time: 0.13731
Total Iteration Time: 4.30964

Cumulative Model Updates: 8,072
Cumulative Timesteps: 33,718,308

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 245.26457
Policy Entropy: 0.57042
Value Function Loss: 2.99156

Mean KL Divergence: 0.13097
SB3 Clip Fraction: 0.32110
Policy Update Magnitude: 1.20622
Value Function Update Magnitude: 1.36131

Collected Steps per Second: 20,606.91997
Overall Steps per Second: 10,661.70749

Timestep Collection Time: 2.42744
Timestep Consumption Time: 2.26431
PPO Batch Consumption Time: 0.13982
Total Iteration Time: 4.69174

Cumulative Model Updates: 8,084
Cumulative Timesteps: 33,768,330

Timesteps Collected: 50,022
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 262.48444
Policy Entropy: 0.56334
Value Function Loss: 3.16176

Mean KL Divergence: 0.13323
SB3 Clip Fraction: 0.32243
Policy Update Magnitude: 1.19160
Value Function Update Magnitude: 1.45201

Collected Steps per Second: 22,413.55862
Overall Steps per Second: 10,955.46501

Timestep Collection Time: 2.23088
Timestep Consumption Time: 2.33323
PPO Batch Consumption Time: 0.15158
Total Iteration Time: 4.56411

Cumulative Model Updates: 8,096
Cumulative Timesteps: 33,818,332

Timesteps Collected: 50,002
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 246.90081
Policy Entropy: 0.55954
Value Function Loss: 3.09990

Mean KL Divergence: 0.13169
SB3 Clip Fraction: 0.31970
Policy Update Magnitude: 1.18364
Value Function Update Magnitude: 1.41586

Collected Steps per Second: 23,971.26132
Overall Steps per Second: 11,654.55227

Timestep Collection Time: 2.08708
Timestep Consumption Time: 2.20566
PPO Batch Consumption Time: 0.13724
Total Iteration Time: 4.29274

Cumulative Model Updates: 8,108
Cumulative Timesteps: 33,868,362

Timesteps Collected: 50,030
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 224.58848
Policy Entropy: 0.55215
Value Function Loss: 2.98984

Mean KL Divergence: 0.13514
SB3 Clip Fraction: 0.31868
Policy Update Magnitude: 1.18396
Value Function Update Magnitude: 1.41536

Collected Steps per Second: 23,412.38728
Overall Steps per Second: 11,651.88136

Timestep Collection Time: 2.13596
Timestep Consumption Time: 2.15588
PPO Batch Consumption Time: 0.13696
Total Iteration Time: 4.29184

Cumulative Model Updates: 8,120
Cumulative Timesteps: 33,918,370

Timesteps Collected: 50,008
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 222.86589
Policy Entropy: 0.54859
Value Function Loss: 2.86593

Mean KL Divergence: 0.13465
SB3 Clip Fraction: 0.31770
Policy Update Magnitude: 1.18417
Value Function Update Magnitude: 1.38816

Collected Steps per Second: 24,052.74127
Overall Steps per Second: 11,642.26878

Timestep Collection Time: 2.08051
Timestep Consumption Time: 2.21779
PPO Batch Consumption Time: 0.13730
Total Iteration Time: 4.29830

Cumulative Model Updates: 8,132
Cumulative Timesteps: 33,968,412

Timesteps Collected: 50,042
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 217.15003
Policy Entropy: 0.54814
Value Function Loss: 2.87215

Mean KL Divergence: 0.13733
SB3 Clip Fraction: 0.31960
Policy Update Magnitude: 1.17327
Value Function Update Magnitude: 1.43001

Collected Steps per Second: 21,498.98612
Overall Steps per Second: 10,913.37420

Timestep Collection Time: 2.32681
Timestep Consumption Time: 2.25693
PPO Batch Consumption Time: 0.13831
Total Iteration Time: 4.58373

Cumulative Model Updates: 8,144
Cumulative Timesteps: 34,018,436

Timesteps Collected: 50,024
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 265.01736
Policy Entropy: 0.54090
Value Function Loss: 2.90908

Mean KL Divergence: 0.13734
SB3 Clip Fraction: 0.31604
Policy Update Magnitude: 1.18659
Value Function Update Magnitude: 1.38698

Collected Steps per Second: 23,460.35167
Overall Steps per Second: 11,362.82221

Timestep Collection Time: 2.13151
Timestep Consumption Time: 2.26933
PPO Batch Consumption Time: 0.13959
Total Iteration Time: 4.40084

Cumulative Model Updates: 8,156
Cumulative Timesteps: 34,068,442

Timesteps Collected: 50,006
--------END ITERATION REPORT--------


--------BEGIN ITERATION REPORT--------
Policy Reward: 247.14415
Policy Entropy: 0.53642
Value Function Loss: 2.99542

Mean KL Divergence: 0.13628
SB3 Clip Fraction: 0.31497
Policy Update Magnitude: 1.20184
Value Function Update Magnitude: 1.40772

Collected Steps per Second: 21,850.07045
Overall Steps per Second: 11,091.29803

Timestep Collection Time: 2.28988
Timestep Consumption Time: 2.22123
PPO Batch Consumption Time: 0.13724
Total Iteration Time: 4.51110

Cumulative Model Updates: 8,168
Cumulative Timesteps: 34,118,476

Timesteps Collected: 50,034
--------END ITERATION REPORT--------
